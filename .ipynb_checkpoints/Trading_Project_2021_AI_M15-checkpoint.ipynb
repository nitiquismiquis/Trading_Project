{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "from scipy.stats import linregress\n",
    "from IPython.display import Image\n",
    "from statistics import mean\n",
    "import random\n",
    "\n",
    "# Plot Imports\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.dates as mpl_dates\n",
    "import plotly.graph_objects as go\n",
    "import datetime\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Closing(df):\n",
    "    '''Function that provides Close_Up/Close_Dw depending on how it closed in regards to the open.\n",
    "\n",
    "    INPUTS:\n",
    "\n",
    "     - df\n",
    "\n",
    "    OUTPUTS:\n",
    "\n",
    "     - ['Closing'] ==> output: col with 2 vbles: 1 if Close_Up & -1 if Close_Dw.\n",
    "\n",
    "    '''\n",
    "    df['Closing'] = df.apply(lambda x : 1 if x['Open'] <= x['Close'] else -1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Market_Structure(df,clean_sweep,swing_max,perc_95):\n",
    "    \n",
    "    '''Function that provides the Market Structure on the df adding the following cols outputs:\n",
    "\n",
    "    INPUTS:\n",
    "\n",
    "     - df\n",
    "\n",
    "    OUTPUTS:\n",
    "\n",
    "     - MS_H      ==> output:  value of Market structure High for each data point\n",
    "     - MS_L      ==> output:  value of Market structure Low  for each data point\n",
    "     - MS_Sit    ==> outputs are categorical vbles: 'MS', 'Settling','Up_Break', 'Dw_Break'\n",
    "    \n",
    "    perc_95 used for H1 is: 126.6 pips\n",
    "    perc_95 used for H4 is: 259.1 pips\n",
    "    perc_95 used for D1 is: 577.2 pips    \n",
    "    '''\n",
    "    \n",
    "# Initiates 1st row;\n",
    "\n",
    "    for i in range (0,10*swing_max):\n",
    "        \n",
    "        df.loc[df.index[i],'MS_L'] = df.loc[df.index[i],'Low']   \n",
    "        df.loc[df.index[i],'MS_H'] = df.loc[df.index[i],'High']\n",
    "        df.loc[df.index[i],'MS_Sit'] = 'MS'\n",
    " \n",
    "    for i in range(10*swing_max,len(df)):\n",
    "        \n",
    "        i = i + df.index[0]\n",
    "    \n",
    "    # From 'MS'/'Dw_break' => 'Up_break' situation\n",
    "        if (df.loc[i-1,'MS_Sit'] not in ('Up_Break')) & (df.loc[i,'Close'] > df.loc[i-1,'MS_H'] + 0.0001*clean_sweep):\n",
    "            \n",
    "            df.loc[i,'MS_Sit'] = 'Up_Break'\n",
    "            df.loc[i,'MS_H']   = df.loc[i,'High']\n",
    "            #df.loc[i,'MS_N']   = df.loc[i-1,'MS_N'] + 1\n",
    "\n",
    "    # Id 1st Low swing before the break\n",
    "            for j in range (0,1000):\n",
    "                \n",
    "                if (df.loc[(i-j),'Low'] >= df.loc[(i-j) -1 ,'Low']):\n",
    "                    continue\n",
    "                else:\n",
    "                    if df.loc[(i-j),'Low'] < df.loc[i-j-swing_max:i-j-1,'Low'].min():\n",
    "                        df.loc[i,'MS_L'] = df.loc[(i-j),'Low']\n",
    "                        \n",
    "    # Test that MS_range is under limit perc_95\n",
    "                        if (10000 * (df.loc[i,'MS_H'] - df.loc[i,'MS_L'])) >= perc_95:\n",
    "                            df.loc[i,'MS_L'] = df.loc[i,'MS_H'] - 0.0001 * perc_95\n",
    "                        else:\n",
    "                            pass\n",
    "                        break\n",
    "                    \n",
    "                    else:\n",
    "                        continue\n",
    "        \n",
    "    # From 'MS'/'Up_break' => 'Dw_break' situation\n",
    "        elif (df.loc[i-1,'MS_Sit'] not in ('Dw_Break')) & (df.loc[i,'Close'] < df.loc[i-1,'MS_L']- 0.0001*clean_sweep):\n",
    "            \n",
    "            df.loc[i,'MS_Sit'] = 'Dw_Break'\n",
    "            df.loc[i,'MS_L']   = df.loc[i,'Low']\n",
    "\n",
    "    # Id 1st High swing before the break\n",
    "            for j in range (0,1000):\n",
    "                if (df.loc[(i-j),'High'] <= df.loc[(i-j) -1 ,'High']):\n",
    "                    continue\n",
    "                else:\n",
    "                    if df.loc[(i-j),'High'] > df.loc[i-j-swing_max:i-j-1,'High'].max():\n",
    "                        df.loc[i,'MS_H'] = df.loc[(i-j),'High']\n",
    "                        \n",
    "    # Test that MS_range is under limit perc_95\n",
    "                        if (10000 * (df.loc[i,'MS_H'] - df.loc[i,'MS_L'])) >= perc_95:\n",
    "                            df.loc[i,'MS_H'] = df.loc[i,'MS_L'] + 0.0001 * perc_95\n",
    "                        else:\n",
    "                            pass\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "    # From Up_Break => Up_Break / Settling_Up       \n",
    "        elif (df.loc[i-1,'MS_Sit'] in ('Up_Break','Settling_Up')) & (df.loc[i,'High'] > df.loc[i-1,'High']):\n",
    "            \n",
    "            df.loc[i,'MS_Sit'] = 'Settling_Up'\n",
    "            df.loc[i,'MS_H']   = df.loc[i,'High']\n",
    "            \n",
    "    # Test that MS_range is under limit perc_95\n",
    "            if (10000 * (df.loc[i,'MS_H'] - df.loc[i-1,'MS_L'])) <= perc_95:\n",
    "                df.loc[i,'MS_L']   = df.loc[i-1,'MS_L']   \n",
    "            else:\n",
    "                df.loc[i,'MS_L'] = df.loc[i,'MS_H'] - 0.0001 * perc_95\n",
    "\n",
    "    # From Dw_Break => Dw_Break / Settling_Dw situation           \n",
    "        elif (df.loc[i-1,'MS_Sit'] in ('Dw_Break','Settling_Dw')) & (df.loc[i,'Low'] < df.loc[i-1,'Low']):\n",
    "            \n",
    "            df.loc[i,'MS_Sit'] = 'Settling_Dw'\n",
    "            df.loc[i,'MS_L']   = df.loc[i,'Low']\n",
    "            \n",
    "    # Test that MS_range is under limit perc_95\n",
    "            if (10000 * (df.loc[i-1,'MS_H'] - df.loc[i,'MS_L'])) <= perc_95:\n",
    "                df.loc[i,'MS_H']   = df.loc[i-1,'MS_H']   \n",
    "            else:\n",
    "                df.loc[i,'MS_H'] = df.loc[i,'MS_L'] + 0.0001 * perc_95\n",
    "\n",
    "    # From MS => MS situation             \n",
    "        else:\n",
    "            \n",
    "            df.loc[i,'MS_Sit'] = 'MS'\n",
    "            df.loc[i,'MS_H']   = df.loc[i-1,'MS_H']\n",
    "            df.loc[i,'MS_L']   = df.loc[i-1,'MS_L']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Trend(df):\n",
    "\n",
    "    '''Function that calculates the trend based on the previous MS break:\n",
    "\n",
    "    INPUTS:\n",
    "\n",
    "     - df\n",
    "\n",
    "    OUTPUTS:\n",
    "\n",
    "     - Trend  ==> outputs are categorical vbles: 'Up', 'Dw' (depending on the value of the previous Break) \n",
    "\n",
    "    '''\n",
    "    \n",
    "    df.loc[df['MS_Sit'] == 'Up_Break','Trend'] =  1\n",
    "    df.loc[df['MS_Sit'] == 'Dw_Break','Trend'] = -1\n",
    "    \n",
    "    for i in range(1,len(df)):\n",
    "        \n",
    "        i = i + df.index[0]\n",
    "        \n",
    "        if df.loc[i,'MS_Sit'] not in ('Dw_Break','Up_Break'):\n",
    "            df.loc[i,'Trend'] = df.loc[i-1,'Trend']\n",
    "            \n",
    "        df['Trend'] = df['Trend'].fillna('none')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N_breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_breaks(df):\n",
    "\n",
    "    '''Function that calculates the number of previous breaks in the same direction:\n",
    "\n",
    "    INPUTS:\n",
    "\n",
    "     - df\n",
    "\n",
    "    OUTPUTS:\n",
    "\n",
    "     - N_Breaks ==> value of the the number of previous breaks in the same direction\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Correction of ('Dw_Break','Up_Break')\n",
    "    \n",
    "    for i in range (1,len(df)):\n",
    "        \n",
    "        i = i + df.index[0]\n",
    "        \n",
    "        if (df.loc[i,'MS_Sit'] == 'Dw_Break') & (df.loc[i-1,'MS_Sit'] == 'Settling_Dw'):\n",
    "            df.loc[i,'MS_Sit'] = 'Settling_Dw'\n",
    "            \n",
    "        elif (df.loc[i,'MS_Sit'] == 'Up_Break') & (df.loc[i-1,'MS_Sit'] == 'Settling_Up'):\n",
    "            df.loc[i,'MS_Sit'] = 'Settling_Up'\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    \n",
    "    df['N_Breaks'] = 0\n",
    "    \n",
    "    for i in range (1,len(df)):\n",
    "        \n",
    "        i = i + df.index[0]\n",
    "        \n",
    "        if df.loc[i,'MS_Sit'] not in ('Dw_Break','Up_Break'):\n",
    "            df.loc[i,'N_Breaks'] = df.loc[i-1,'N_Breaks']\n",
    "            \n",
    "        elif (df.loc[i,'MS_Sit'] == 'Up_Break'):\n",
    "            if (df.loc[i-1,'Trend'] == -1):\n",
    "                df.loc[i,'N_Breaks'] = 1\n",
    "            else:\n",
    "                df.loc[i,'N_Breaks'] = df.loc[i-1,'N_Breaks'] + 1\n",
    "        \n",
    "        elif (df.loc[i,'MS_Sit'] == 'Dw_Break'):\n",
    "            if (df.loc[i-1,'Trend'] == 1):\n",
    "                df.loc[i,'N_Breaks'] = -1\n",
    "            else:\n",
    "                df.loc[i,'N_Breaks'] = df.loc[i-1,'N_Breaks'] - 1\n",
    "                \n",
    "    df['N_Breaks'] = df['N_Breaks'].abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MS_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MS_periods(df):\n",
    "    \n",
    "    '''Function that calculates the number of periods the price remains in MS:\n",
    "\n",
    "    INPUTS:\n",
    "\n",
    "     - df\n",
    "\n",
    "    OUTPUTS:\n",
    "\n",
    "     - N_periods_MS ==> value of the range within the same MS\n",
    "\n",
    "    '''\n",
    "    \n",
    "    df.loc[df.index[0],'MS_Pds'] = 0\n",
    "    \n",
    "    for i in range (1,len(df)):\n",
    "        \n",
    "        i = i + df.index[0]\n",
    "        \n",
    "        if (df.loc[i-1,'MS_Sit'] == 'MS') & (df.loc[i,'MS_Sit'] == 'MS'):\n",
    "            df.loc[i,'MS_Pds'] = df.loc[i-1,'MS_Pds'] + 1\n",
    "        \n",
    "        elif (df.loc[i-1,'MS_Sit'] != 'MS') & (df.loc[i,'MS_Sit'] == 'MS'):\n",
    "            df.loc[i,'MS_Pds'] = 1\n",
    "        \n",
    "        else:\n",
    "            df.loc[i,'MS_Pds'] = 0\n",
    "    \n",
    "    # Converts column into integers\n",
    "    df['MS_Pds'] = df['MS_Pds'].apply(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MS_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MS_range(df):\n",
    "\n",
    "    '''Function that calculates the number of pips between MS_High & MS_Low within MS:\n",
    "\n",
    "    INPUTS:\n",
    "\n",
    "     - df\n",
    "\n",
    "    OUTPUTS:\n",
    "\n",
    "     - MS_range ==> value of the range (MS_H - MS_L) within the same MS\n",
    "\n",
    "    '''\n",
    "    \n",
    "    df['MS_range'] = 10000 * (df['MS_H'] - df['MS_L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MS_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MS_N(df):\n",
    "    \n",
    "    df.loc[df.index[0],'MS_N'] = 0\n",
    "    for i in range(1,len(df)):\n",
    "\n",
    "        i = i + df.index[0]\n",
    "\n",
    "        if df.loc[i,'MS_Sit'] in ('Dw_Break','Up_Break'):\n",
    "            df.loc[i,'MS_N']   = df.loc[i-1,'MS_N'] + 1\n",
    "\n",
    "        else:\n",
    "            df.loc[i,'MS_N']   = df.loc[i-1,'MS_N']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Indicators(df):\n",
    "\n",
    "    ''' -----------------------------------\n",
    "    Calculates RSI (SMA) using RSI_SMA_1\n",
    "\n",
    "    RSI_SMA_1 = 14\n",
    "    Upper_SMA_RSI_lim = 70\n",
    "    Lower_SMA_RSI_lim = 30\n",
    "\n",
    "    '''\n",
    "\n",
    "    RSI_SMA_1 = 14\n",
    "    Upper_SMA_RSI_lim = 70\n",
    "    Lower_SMA_RSI_lim = 30\n",
    "    \n",
    "    df['Close_im1'] = df['Close'].shift()\n",
    "    df['U_Move'] = df.apply(lambda x : x['Close'] - x['Close_im1'] if x['Close'] > x['Close_im1'] else np.NaN, axis=1)\n",
    "    df['D_Move'] = df.apply(lambda x : x['Close_im1'] - x['Close'] if x['Close'] < x['Close_im1'] else np.NaN, axis=1)\n",
    "\n",
    "    df[str(RSI_SMA_1) + '_Avg_U'] = df['U_Move'].rolling(min_periods=1, window=RSI_SMA_1).mean()\n",
    "    df[str(RSI_SMA_1) + '_Avg_D'] = df['D_Move'].rolling(min_periods=1, window=RSI_SMA_1).mean()\n",
    "\n",
    "    #Calculates the RSI\n",
    "\n",
    "    df['RSI_' + str(RSI_SMA_1) + '_SMA'] = 100-(100/(1+(df[str(RSI_SMA_1) + '_Avg_U']/df[str(RSI_SMA_1) + '_Avg_D'])))\n",
    "    \n",
    "    ''' -----------------------------------\n",
    "    Calculates RSI (EMA) using RSI_EMA_1 for the specific period\n",
    "\n",
    "    RSI_EMA_1 = 14\n",
    "    Upper_EMA_RSI_lim = 70\n",
    "    Lower_EMA_RSI_lim = 30 \n",
    "\n",
    "    '''\n",
    "\n",
    "    RSI_EMA_1 = 14\n",
    "    Upper_EMA_RSI_lim = 70\n",
    "    Lower_EMA_RSI_lim = 30 \n",
    "\n",
    "    df[str(RSI_EMA_1) + '_Avg_U_EMA'] = df[str(RSI_SMA_1) + '_Avg_U'].ewm(span=RSI_EMA_1).mean()\n",
    "    df[str(RSI_EMA_1) + '_Avg_D_EMA'] = df[str(RSI_SMA_1) + '_Avg_D'].ewm(span=RSI_EMA_1).mean()\n",
    "\n",
    "    df['RSI_' + str(RSI_EMA_1) + '_EMA'] = 100-(100/(1+(df[str(RSI_EMA_1) + '_Avg_U_EMA']/df[str(RSI_EMA_1) + '_Avg_D_EMA'])))\n",
    "\n",
    "    ''' -----------------------------------\n",
    "    Calculates MACD using EMA_1_macd, EMA_2_macd\n",
    "\n",
    "    # MACD\n",
    "    EMA_1_macd = 12\n",
    "    EMA_2_macd = 26\n",
    "    MACD_EMA = 9\n",
    "\n",
    "    '''\n",
    "\n",
    "    EMA_1_macd = 12\n",
    "    EMA_2_macd = 26\n",
    "    MACD_EMA = 9\n",
    "\n",
    "    df['EMA_' + str(EMA_1_macd) + '_macd'] = df['Close'].ewm(span=EMA_1_macd).mean()\n",
    "    df['EMA_' + str(EMA_2_macd) + '_macd'] = df['Close'].ewm(span=EMA_2_macd).mean()\n",
    "    \n",
    "    # Calculates [MACD] = 10000 x ([EMA_1] - [EMA_2]) using ''''''MACD_EMA''''''\n",
    "    \n",
    "    df['MACD_' + str(EMA_1_macd) + '_' + str(EMA_2_macd)] = 10000 * (df['EMA_' + str(EMA_1_macd) + '_macd'] - df['EMA_' + str(EMA_2_macd) + '_macd'])\n",
    "\n",
    "    # Calculates [EMA_MACD] = EMA_3(MACD) using ''''''MACD_EMA''''''\n",
    "\n",
    "    df['EMA_MACD_'  + str(EMA_1_macd) + '_' + str(EMA_2_macd) + '_' + str(MACD_EMA)] = df['MACD_' + str(EMA_1_macd) + '_' + str(EMA_2_macd)].ewm(span=MACD_EMA).mean()\n",
    "\n",
    "    # Calculates MACD Histogram [MACD] - [EMA_MACD]\n",
    "\n",
    "    df['Hist_MACD_' + str(EMA_1_macd) + '_' + str(EMA_2_macd) + '_' + str(MACD_EMA)] = df['MACD_' + str(EMA_1_macd) + '_' + str(EMA_2_macd)] - df['EMA_MACD_'  + str(EMA_1_macd) + '_' + str(EMA_2_macd)  + '_' + str(MACD_EMA)]\n",
    "\n",
    "    # Calculates the signal(MACD)\n",
    "\n",
    "    df['MACD_signal'] = df['Hist_MACD_' + str(EMA_1_macd) + '_' + str(EMA_2_macd) + '_' + str(MACD_EMA)] * df['Hist_MACD_' + str(EMA_1_macd) + '_' + str(EMA_2_macd) + '_' + str(MACD_EMA)].shift()\n",
    "\n",
    "    ''' -----------------------------------\n",
    "    Calculates the Upper & lower BBands using Boll_SMA\n",
    "\n",
    "    # Bollinger Bands\n",
    "    Boll_SMA = 20\n",
    "    Boll_Var = 2\n",
    "\n",
    "    '''\n",
    "\n",
    "    Boll_SMA = 20\n",
    "    Boll_Var = 2\n",
    "\n",
    "    # Calculates the Upper & lower BBands using ''''''Boll_SMA''''''\n",
    "\n",
    "    df['Boll_SMA_' + str(Boll_SMA)] = df['Close'].rolling(min_periods=Boll_SMA,window=Boll_SMA).mean()\n",
    "    df['Boll_SMA_' + str(Boll_SMA) + '_Var_-' + str(Boll_Var)] = df['Boll_SMA_' + str(Boll_SMA)] - Boll_Var*df['Boll_SMA_' + str(Boll_SMA)].rolling(min_periods=Boll_SMA,window=Boll_SMA).std()\n",
    "    df['Boll_SMA_' + str(Boll_SMA) + '_Var_+' + str(Boll_Var)] = df['Boll_SMA_' + str(Boll_SMA)] + Boll_Var*df['Boll_SMA_' + str(Boll_SMA)].rolling(min_periods=Boll_SMA,window=Boll_SMA).std()\n",
    "\n",
    "    # Calculates the DELTA(P,Upper|Lower BB) using ''''''Boll_Var'''''' as the No. of Variances\n",
    "    \n",
    "    df['Dist_BollB_SMA_' + str(Boll_SMA) + '_Var_+' + str(Boll_Var)] = 10000*( df['Close'] - df['Boll_SMA_' + str(Boll_SMA) + '_Var_+' + str(Boll_Var)])\n",
    "    df['Dist_BollB_SMA_' + str(Boll_SMA) + '_Var_-' + str(Boll_Var)] = 10000*(-df['Close'] + df['Boll_SMA_' + str(Boll_SMA) + '_Var_-' + str(Boll_Var)])\n",
    "    \n",
    "    # Calculates the WIDENESS of the Bands\n",
    "    \n",
    "    df['BollB_Wideness'] = 10000 * 2*Boll_Var*df['Boll_SMA_' + str(Boll_SMA)].rolling(min_periods=Boll_SMA,window=Boll_SMA).std()\n",
    "    \n",
    "    '''\n",
    "    # Drops unnecessary columns:\n",
    "\n",
    "    df.drop(['Close_im1','U_Move','D_Move', \n",
    "             str(RSI_EMA_1) + '_Avg_U_EMA',\n",
    "             str(RSI_EMA_1) + '_Avg_D_EMA',\n",
    "             str(RSI_SMA_1) + '_Avg_U',\n",
    "             str(RSI_SMA_1) + '_Avg_D',\n",
    "             'Boll_SMA_' + str(Boll_SMA),\n",
    "             'Boll_SMA_' + str(Boll_SMA) + '_Var_-' + str(Boll_Var),\n",
    "             'Boll_SMA_' + str(Boll_SMA) + '_Var_+' + str(Boll_Var),axis=1,inplace=True)\n",
    "\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read_prep_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_prep_df(File, n, sd, ed, clean_sweep, swing_max, perc_95):\n",
    "    \n",
    "    df = pd.read_excel(File)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    if sd != None:\n",
    "        df = df.loc[df[df['Date'] == sd].index[0]:df[df['Date'] == ed].index[0]]\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    else:\n",
    "        pass   \n",
    "    \n",
    "    if n == 0:\n",
    "        \n",
    "        print(len(df))\n",
    "        \n",
    "        Closing(df) \n",
    "        Market_Structure(df, clean_sweep, swing_max, perc_95)\n",
    "        Trend(df)\n",
    "        N_breaks(df)\n",
    "        MS_periods(df)\n",
    "        MS_range(df)\n",
    "        #MS_retracement(df)\n",
    "        MS_N(df)\n",
    "        Indicators(df)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        print(len(df))\n",
    "        \n",
    "        for i in range(0,1 + int(len(df)/n)):\n",
    "        \n",
    "            if i == 0:\n",
    "            \n",
    "                dfn = df.loc[(i*n) : ((i+1)*n) + 499,:].copy()\n",
    "                \n",
    "                Closing(dfn) \n",
    "                Market_Structure(dfn, clean_sweep, swing_max, perc_95)\n",
    "                Trend(dfn)\n",
    "                N_breaks(dfn)\n",
    "                MS_periods(dfn)\n",
    "                MS_range(dfn)\n",
    "                #MS_retracement(dfn)\n",
    "            \n",
    "                df_out = dfn.copy()\n",
    "                print(i)\n",
    "                \n",
    "            elif (i > 0) & (i < (1 + int(len(df)/n))):\n",
    "\n",
    "                dfn = df.loc[i*n:((i+1)*n)+499,:].copy()\n",
    "\n",
    "                Closing(dfn) \n",
    "                Market_Structure(dfn, clean_sweep, swing_max, perc_95)\n",
    "                Trend(dfn)\n",
    "                N_breaks(dfn)\n",
    "                MS_periods(dfn)\n",
    "                MS_range(dfn)\n",
    "                #MS_retracement(dfn)\n",
    "\n",
    "                dfn = dfn.loc[(i*n) + 500 : ((i+1)*n) + 499,:].copy()\n",
    "\n",
    "                df_out = pd.concat([df_out,dfn])\n",
    "                print(i)                \n",
    "                \n",
    "            else:\n",
    "\n",
    "                dfn = df.loc[i*n:,:].copy()\n",
    "\n",
    "                Closing(dfn) \n",
    "                Market_Structure(dfn, clean_sweep, swing_max, perc_95)\n",
    "                Trend(dfn)\n",
    "                N_breaks(dfn)\n",
    "                MS_periods(dfn)\n",
    "                MS_range(dfn)\n",
    "                #MS_retracement(dfn)\n",
    "\n",
    "                dfn = dfn.loc[(i*n) + 500 :,:].copy()\n",
    "\n",
    "                df_out = pd.concat([df_out,dfn])\n",
    "                print(i)\n",
    "                \n",
    "        df = df_out.copy()\n",
    "        MS_N(df)\n",
    "        Indicators(df)\n",
    "        df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y %H:%M:%S').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rename_df(df,suffix):\n",
    "    \n",
    "    '''Function that renames all columns adding the suffix at the end\n",
    "\n",
    "    INPUTS:\n",
    "\n",
    "     - df     : Dataframe\n",
    "     - suffix : '1D', '4H' or '15M' to rename the columns\n",
    "\n",
    "    OUTPUTS:\n",
    "\n",
    "     - New df with renamed columns\n",
    "\n",
    "    '''\n",
    "\n",
    "    keys = df.columns\n",
    "    values = keys + '_' + suffix\n",
    "    dictionary = dict(zip(keys, values))\n",
    "    \n",
    "    df = df.rename(columns=dictionary)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merges D1 - H4 - H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge_shift_df_H4_D1(df1,df2):\n",
    "    \n",
    "    '''Function that merges de 4H & 1D dataframes (df1 & df2)\n",
    "\n",
    "    INPUTS:\n",
    "\n",
    "     - df1    : df4H\n",
    "     - df2    : df1D\n",
    "\n",
    "    OUTPUTS:\n",
    "\n",
    "     - New merged df\n",
    "\n",
    "    '''\n",
    "\n",
    "    df1.iloc[:, 0] = pd.to_datetime(df1.iloc[:, 0], dayfirst=True)\n",
    "    df2.iloc[:, 0] = pd.to_datetime(df2.iloc[:, 0], dayfirst=True)\n",
    "    \n",
    "    df1['Date_del'] = df1.iloc[:, 0] + dt.timedelta(hours=4) \n",
    "    df2['Date_del'] = df2.iloc[:, 0] + dt.timedelta(days=1)\n",
    "\n",
    "    df = pd.merge(df1, df2, how='left', on='Date_del')\n",
    "    \n",
    "    df1.drop(['Date_del'],axis=1,inplace=True)\n",
    "    df2.drop(['Date_del'],axis=1,inplace=True)\n",
    "    df. drop(['Date_del'],axis=1,inplace=True)\n",
    "    \n",
    "    df = df.ffill(axis=0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge_shift_df_H1_H4(df1,df2):\n",
    "    \n",
    "    '''Function that merges de 15M & 4H dataframes (df1 & df2)\n",
    "\n",
    "    INPUTS:\n",
    "\n",
    "     - df1    : df1H\n",
    "     - df2    : df4H or (dfH1_H4)\n",
    "\n",
    "    OUTPUTS:\n",
    "\n",
    "     - New merged df\n",
    "\n",
    "    '''\n",
    "    \n",
    "    df1.iloc[:, 0] = pd.to_datetime(df1.iloc[:, 0], dayfirst=True)\n",
    "    df2.iloc[:, 0] = pd.to_datetime(df2.iloc[:, 0], dayfirst=True)\n",
    "    \n",
    "    df1['Date_del'] = df1.iloc[:, 0] + dt.timedelta(hours=1) \n",
    "    df2['Date_del'] = df2.iloc[:, 0] + dt.timedelta(hours=4)\n",
    "   \n",
    "    df = pd.merge(df1, df2, how='left', on='Date_del')\n",
    "    \n",
    "    df['Date_H4'] = df['Date_H4'].ffill(axis=0)\n",
    "    \n",
    "    df1.drop(['Date_del'],axis=1,inplace=True)\n",
    "    df2.drop(['Date_del'],axis=1,inplace=True)\n",
    "    df. drop(['Date_del'],axis=1,inplace=True)\n",
    "    \n",
    "    df = df.ffill(axis=0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge_shift_df_M15_H1(df1,df2):\n",
    "    \n",
    "    '''Function that merges de 15M & 4H dataframes (df1 & df2)\n",
    "\n",
    "    INPUTS:\n",
    "\n",
    "     - df1    : dfM15\n",
    "     - df2    : dfH1 or (dfM15_H1)\n",
    "\n",
    "    OUTPUTS:\n",
    "\n",
    "     - New merged df\n",
    "\n",
    "    '''\n",
    "    \n",
    "    df1.iloc[:, 0] = pd.to_datetime(df1.iloc[:, 0], dayfirst=True)\n",
    "    df2.iloc[:, 0] = pd.to_datetime(df2.iloc[:, 0], dayfirst=True)\n",
    "    \n",
    "    df1['Date_del'] = df1.iloc[:, 0] + dt.timedelta(minutes=15) \n",
    "    df2['Date_del'] = df2.iloc[:, 0] + dt.timedelta(hours=1)\n",
    "   \n",
    "    df = pd.merge(df1, df2, how='left', on='Date_del')\n",
    "    \n",
    "    df['Date_H1'] = df['Date_H1'].ffill(axis=0)\n",
    "    \n",
    "    df1.drop(['Date_del'],axis=1,inplace=True)\n",
    "    df2.drop(['Date_del'],axis=1,inplace=True)\n",
    "    df. drop(['Date_del'],axis=1,inplace=True)\n",
    "    \n",
    "    df = df.ffill(axis=0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MS_retracement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MS_retracement(df):\n",
    "\n",
    "    '''Function that calculates that the price has retraced (0,100)% between MS_High & MS_Low depending on its trend:\n",
    "\n",
    "    INPUTS:\n",
    "\n",
    "     - df\n",
    "\n",
    "    OUTPUTS:\n",
    "\n",
    "     - MS_retracement ==> value of the range (MS_H - MS_L) within the same MS\n",
    "    '''\n",
    "    \n",
    "    df['MS_retracement_D1']  = df.apply(lambda x: 100 * (x['Close_M15'] - x['MS_L_D1'])  /(x['MS_H_D1']  - x['MS_L_D1'])  if x['Trend_D1']  == -1 else (100 * (x['MS_H_D1']  - x['Close_M15'])/(x['MS_H_D1']  - x['MS_L_D1'])),  axis=1).round(1)\n",
    "    df['MS_retracement_H4']  = df.apply(lambda x: 100 * (x['Close_M15'] - x['MS_L_H4'])  /(x['MS_H_H4']  - x['MS_L_H4'])  if x['Trend_H4']  == -1 else (100 * (x['MS_H_H4']  - x['Close_M15'])/(x['MS_H_H4']  - x['MS_L_H4'])),  axis=1).round(1)\n",
    "    df['MS_retracement_H1']  = df.apply(lambda x: 100 * (x['Close_M15'] - x['MS_L_H1'])  /(x['MS_H_H1']  - x['MS_L_H1'])  if x['Trend_H1']  == -1 else (100 * (x['MS_H_H1']  - x['Close_M15'])/(x['MS_H_H1']  - x['MS_L_H1'])),  axis=1).round(1)\n",
    "    df['MS_retracement_M15'] = df.apply(lambda x: 100 * (x['Close_M15'] - x['MS_L_M15']) /(x['MS_H_M15'] - x['MS_L_M15']) if x['Trend_M15'] == -1 else (100 * (x['MS_H_M15'] - x['Close_M15'])/(x['MS_H_M15'] - x['MS_L_M15'])), axis=1).round(1)  \n",
    "    \n",
    "    #df['MS_retracement'] = df.apply(lambda x: 100 * (x['Close'] - x['MS_L']) /(x['MS_H'] - x['MS_L']) if x['Trend'] == -1 else (100 * (x['MS_H'] - x['Close'])/(x['MS_H'] - x['MS_L'])), axis=1)\n",
    "    #df['MS_retracement'] = df['MS_retracement'].round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \n",
    "    from scipy import stats\n",
    "    \n",
    "# Drop non required columns\n",
    "    RSI_SMA_1, RSI_EMA_1 = 14, 14\n",
    "\n",
    "    non_req_cols = ['U_Move_M15','U_Move_H1','U_Move_H4','U_Move_D1','D_Move_M15','D_Move_H1','D_Move_H4','D_Move_D1',\n",
    "                    'Close_im1_M15','Close_im1_H1','Close_im1_H4','Close_im1_D1',\n",
    "                    str(RSI_SMA_1) + '_Avg_U_M15',str(RSI_SMA_1) + '_Avg_U_H1',str(RSI_SMA_1) + '_Avg_U_H4',str(RSI_SMA_1) + '_Avg_U_D1',\n",
    "                    str(RSI_SMA_1) + '_Avg_D_M15',str(RSI_SMA_1) + '_Avg_D_H1',str(RSI_SMA_1) + '_Avg_D_H4',str(RSI_SMA_1) + '_Avg_D_D1',\n",
    "                    str(RSI_EMA_1) + '_Avg_U_EMA_M15',str(RSI_EMA_1) + '_Avg_U_EMA_H1',str(RSI_EMA_1) + '_Avg_U_EMA_H4',str(RSI_EMA_1) + '_Avg_U_EMA_D1',\n",
    "                    str(RSI_EMA_1) + '_Avg_D_EMA_M15',str(RSI_EMA_1) + '_Avg_D_EMA_H1',str(RSI_EMA_1) + '_Avg_D_EMA_H4',str(RSI_EMA_1) + '_Avg_D_EMA_D1',\n",
    "                    #'Open_M15','High_M15','Low_M15','Close_M15','MS_L_M15','MS_H_M15','MS_Sit_M15',\n",
    "                    #'Open_H1' ,'High_H1' ,'Low_H1' ,'Close_H1', 'MS_L_H1' ,'MS_H_H1' ,'MS_Sit_H1',\n",
    "                    #'Open_H4' ,'High_H4' ,'Low_H4' ,'Close_H4', 'MS_L_H4' ,'MS_H_H4' ,'MS_Sit_H4',\n",
    "                    #'Open_D1' ,'High_D1' ,'Low_D1' ,'Close_D1', 'MS_L_D1' ,'MS_H_D1' ,'MS_Sit_D1',\n",
    "                    'B_Win_Idx_M15_H4','B_Lose_Idx_M15_H4','S_Win_Idx_M15_H4','S_Lose_Idx_M15_H4',\n",
    "                    'B_Win_Idx_M15_H1','B_Lose_Idx_M15_H1','S_Win_Idx_M15_H1','S_Lose_Idx_M15_H1']\n",
    "    \n",
    "    print('No. columns BEFORE dropping:',df.shape[1])\n",
    "    df.drop(non_req_cols,inplace = True, axis = 1)\n",
    "    print('No. columns AFTER dropping:',df.shape[1])\n",
    "     \n",
    "# Drop None´s\n",
    "    print('Length BEFORE removing None´s:',len(df))\n",
    "    last_none_idx = df[df.isin(['none']).any(axis=1)].index[-1]\n",
    "    print('Last row containing a None:',last_none_idx)\n",
    "    df.drop(df.index[:last_none_idx],inplace=True)\n",
    "    print('Length AFTER removing None´s:',len(df))\n",
    "    \n",
    "# Drop Nan´s\n",
    "    print('Length BEFORE removing NaNs:',len(df))\n",
    "    print('Number rows containing NaNs:',df.isnull().any(axis=1).sum())\n",
    "    df.dropna(inplace=True)\n",
    "    print('Length AFTER removing NaNs:',len(df))\n",
    "    \n",
    "# Remove Ouliars z = 3 (3 standard deviations)\n",
    "\n",
    "    gen_cols_outliars = ['RSI_14_SMA_M15','RSI_14_SMA_H1','RSI_14_SMA_H4','RSI_14_SMA_D1',\n",
    "                         'RSI_14_EMA_M15','RSI_14_EMA_H1','RSI_14_EMA_H4','RSI_14_EMA_D1',\n",
    "                         'EMA_12_macd_M15','EMA_12_macd_H1','EMA_12_macd_H4','EMA_12_macd_D1',\n",
    "                         'EMA_26_macd_M15','EMA_26_macd_H1','EMA_26_macd_H4','EMA_26_macd_D1',\n",
    "                         'MACD_12_26_M15','MACD_12_26_H1', 'MACD_12_26_H4', 'MACD_12_26_D1',\n",
    "                         'EMA_MACD_12_26_9_M15','EMA_MACD_12_26_9_H1', 'EMA_MACD_12_26_9_H4', 'EMA_MACD_12_26_9_D1',\n",
    "                         'Hist_MACD_12_26_9_M15','Hist_MACD_12_26_9_H1', 'Hist_MACD_12_26_9_H4', 'Hist_MACD_12_26_9_D1',\n",
    "                         'MACD_signal_M15','MACD_signal_H1', 'MACD_signal_H4', 'MACD_signal_D1',\n",
    "                         'Boll_SMA_20_M15','Boll_SMA_20_H1', 'Boll_SMA_20_H4', 'Boll_SMA_20_D1',\n",
    "                         'Boll_SMA_20_Var_-2_M15','Boll_SMA_20_Var_-2_H1', 'Boll_SMA_20_Var_-2_H4', 'Boll_SMA_20_Var_-2_D1',\n",
    "                         'Boll_SMA_20_Var_+2_M15','Boll_SMA_20_Var_+2_H1', 'Boll_SMA_20_Var_+2_H4', 'Boll_SMA_20_Var_+2_D1',\n",
    "                         'Dist_BollB_SMA_20_Var_+2_M15','Dist_BollB_SMA_20_Var_+2_H1', 'Dist_BollB_SMA_20_Var_+2_H4', 'Dist_BollB_SMA_20_Var_+2_D1',\n",
    "                         'Dist_BollB_SMA_20_Var_-2_M15','Dist_BollB_SMA_20_Var_-2_H1', 'Dist_BollB_SMA_20_Var_-2_H4', 'Dist_BollB_SMA_20_Var_-2_D1',\n",
    "                         'BollB_Wideness_M15','BollB_Wideness_H1', 'BollB_Wideness_H4', 'BollB_Wideness_D1',]\n",
    "\n",
    "    print('Length BEFORE removing Ouliars:',len(df))\n",
    "    df = df[(np.abs(stats.zscore(df[gen_cols_outliars])) < 3).all(axis=1)]\n",
    "    print('Length AFTER removing Ouliars:',len(df))\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots df_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_MS(df, date, rng):\n",
    "    \n",
    "    p = df[df['Date'] == date].index[0]\n",
    "    i = p\n",
    "    j = p + rng\n",
    "    \n",
    "    dfp = df.loc[i:j,['Date', 'Open', 'High', 'Low', 'Close']]\n",
    "    \n",
    "    fig = go.Figure(data=[go.Candlestick(x=dfp['Date'],\n",
    "                    open=dfp['Open'],\n",
    "                    high=dfp['High'],\n",
    "                    low=dfp['Low'],\n",
    "                    close=dfp['Close'])])\n",
    "\n",
    "    fig.update_layout(width=1000, height=1000,margin=dict(l=0, r=20, b=100, t=20, pad=4))\n",
    "    \n",
    "    for k in range(i,j):\n",
    "        \n",
    "        fig.add_shape(\n",
    "                # Line Horizontal - ['MS_H']\n",
    "                    type=\"line\",\n",
    "                    x0 = df.loc[k,'Date'],\n",
    "                    y0 = df.loc[k,'MS_H'],\n",
    "                    x1 = df.loc[k+1,'Date'],\n",
    "                    y1 = df.loc[k,'MS_H'],\n",
    "                    line=dict(\n",
    "                        color=\"Green\",\n",
    "                        width=4,\n",
    "                        dash=\"dashdot\",\n",
    "                    ))\n",
    "\n",
    "        fig.add_shape(\n",
    "                # Line Horizontal - ['MS_L']\n",
    "                    type=\"line\",\n",
    "                    x0 = df.loc[k,'Date'],\n",
    "                    y0 = df.loc[k,'MS_L'],\n",
    "                    x1 = df.loc[k+1,'Date'],\n",
    "                    y1 = df.loc[k,'MS_L'],\n",
    "                    line=dict(\n",
    "                        color=\"Blue\",\n",
    "                        width=4,\n",
    "                        dash=\"dashdot\",\n",
    "                    ))\n",
    "    \n",
    "    print(df.loc[p,'Date'])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots df (merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_MS_all(df, date, rng):\n",
    "    \n",
    "    p = df[df['Date_H1'] == date].index[0]\n",
    "    i = p - rng\n",
    "    j = p + rng\n",
    "    \n",
    "    dfp = df.loc[i:j,['Date_H1', 'Open_H1', 'High_H1', 'Low_H1', 'Close_H1']]\n",
    "    \n",
    "    fig = go.Figure(data=[go.Candlestick(x=dfp['Date_H1'],\n",
    "                    open=dfp['Open_H1'],\n",
    "                    high=dfp['High_H1'],\n",
    "                    low=dfp['Low_H1'],\n",
    "                    close=dfp['Close_H1'])])\n",
    "\n",
    "    fig.update_layout(width=1000, height=2000,margin=dict(l=0, r=20, b=100, t=20, pad=4))    \n",
    "    \n",
    "    for k in range(i,j):\n",
    "        \n",
    "        fig.add_shape(\n",
    "                # Line Horizontal - ['MS_H_H1']\n",
    "                    type=\"line\",\n",
    "                    x0 = df.loc[k,'Date_H1'],\n",
    "                    y0 = df.loc[k,'MS_H_H1'],\n",
    "                    x1 = df.loc[k+1,'Date_H1'],\n",
    "                    y1 = df.loc[k,'MS_H_H1'],\n",
    "                    line=dict(\n",
    "                        color=\"Blue\",\n",
    "                        width=4,\n",
    "                        dash=\"dashdot\",\n",
    "                    ))\n",
    "\n",
    "        fig.add_shape(\n",
    "                # Line Horizontal - ['MS_L_H1']\n",
    "                    type=\"line\",\n",
    "                    x0 = df.loc[k,'Date_H1'],\n",
    "                    y0 = df.loc[k,'MS_L_H1'],\n",
    "                    x1 = df.loc[k+1,'Date_H1'],\n",
    "                    y1 = df.loc[k,'MS_L_H1'],\n",
    "                    line=dict(\n",
    "                        color=\"Blue\",\n",
    "                        width=4,\n",
    "                        dash=\"dashdot\",\n",
    "                    ))\n",
    "        \n",
    "        fig.add_shape(\n",
    "                # Line Horizontal - ['MS_H_H4']\n",
    "                    type=\"line\",\n",
    "                    x0 = df.loc[k,'Date_H1'],\n",
    "                    y0 = df.loc[k,'MS_H_H4'],\n",
    "                    x1 = df.loc[k+1,'Date_H1'],\n",
    "                    y1 = df.loc[k,'MS_H_H4'],\n",
    "                    line=dict(\n",
    "                        color=\"Red\",\n",
    "                        width=4,\n",
    "                        dash=\"dashdot\",\n",
    "                    ))\n",
    "\n",
    "        fig.add_shape(\n",
    "                # Line Horizontal - ['MS_L_H4']\n",
    "                    type=\"line\",\n",
    "                    x0 = df.loc[k,'Date_H1'],\n",
    "                    y0 = df.loc[k,'MS_L_H4'],\n",
    "                    x1 = df.loc[k+1,'Date_H1'],\n",
    "                    y1 = df.loc[k,'MS_L_H4'],\n",
    "                    line=dict(\n",
    "                        color=\"Red\",\n",
    "                        width=4,\n",
    "                        dash=\"dashdot\",\n",
    "                    ))\n",
    "        \n",
    "        fig.add_shape(\n",
    "                # Line Horizontal - ['MS_H_D1']\n",
    "                    type=\"line\",\n",
    "                    x0 = df.loc[k,'Date_H1'],\n",
    "                    y0 = df.loc[k,'MS_H_D1'],\n",
    "                    x1 = df.loc[k+1,'Date_H1'],\n",
    "                    y1 = df.loc[k,'MS_H_D1'],\n",
    "                    line=dict(\n",
    "                        color=\"Yellow\",\n",
    "                        width=4,\n",
    "                        dash=\"dashdot\",\n",
    "                    ))\n",
    "\n",
    "        fig.add_shape(\n",
    "                # Line Horizontal - ['MS_L_D1']\n",
    "                    type=\"line\",\n",
    "                    x0 = df.loc[k,'Date_H1'],\n",
    "                    y0 = df.loc[k,'MS_L_D1'],\n",
    "                    x1 = df.loc[k+1,'Date_H1'],\n",
    "                    y1 = df.loc[k,'MS_L_D1'],\n",
    "                    line=dict(\n",
    "                        color=\"Yellow\",\n",
    "                        width=4,\n",
    "                        dash=\"dashdot\",\n",
    "                    ))\n",
    "    \n",
    "    print(df.loc[p,'Date_H1'])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price_M15_H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Price_M15_H1(df,k,ratio,pip_min,pip_max,pip_over):\n",
    "    \n",
    "    '''Function that prices all M15 candles based on [Close_M15].shift() value and calcs. its stop loss &\n",
    "                limits based on MS_H1.\n",
    "    INPUTS:\n",
    "\n",
    "     - df    : Dataframe\n",
    "     - ratio : limit:stop_loss ratio (normally 3)\n",
    "\n",
    "    OUTPUTS:\n",
    "\n",
    "     - Price_M15          ==> output: value of [Close_M15].shift()\n",
    "     - B_Stop_Loss_M15_H1 ==> output: # pips between ([Price_M15] - [MS_L_H1])\n",
    "     - B_Limit_M15_H1     ==> output: # pips between (ratio x [B_Stop_Loss_M15_H1])\n",
    "     - S_Stop_Loss_M15_H1 ==> output: # pips between ([MS_H_H1] - [Price_M15])\n",
    "     - S_Limit_M15_H1     ==> output: # pips between (ratio x [S_Stop_Loss_M15_H1])\n",
    "     - Labelb_M15         ==> output: 1 or 0 whether the action of buying  was succesful or not\n",
    "     - Labels_M15         ==> output: 1 or 0 whether the action of selling was succesful or not    \n",
    "    '''\n",
    "    \n",
    "# Price\n",
    "    df['Price_M15']          = df['Close_M15']\n",
    "    df['B_Stop_Loss_M15_H1'] = (10000 * (df['Price_M15'] - df['MS_L_H1'])) - pip_over\n",
    "    df['B_Stop_Loss_M15_H1'] = df.apply(lambda x: x['B_Stop_Loss_M15_H1'] if x['B_Stop_Loss_M15_H1'] < pip_max else pip_max ,axis = 1)\n",
    "    df['B_Stop_Loss_M15_H1'] = df.apply(lambda x: x['B_Stop_Loss_M15_H1'] if x['B_Stop_Loss_M15_H1'] > pip_min else pip_min ,axis = 1)\n",
    "    df['B_Limit_M15_H1']     = ratio * df['B_Stop_Loss_M15_H1']\n",
    "    df['S_Stop_Loss_M15_H1'] = 10000 * (df['MS_H_H1'] - df['Price_M15']) - pip_over\n",
    "    df['S_Stop_Loss_M15_H1'] = df.apply(lambda x: x['S_Stop_Loss_M15_H1'] if x['S_Stop_Loss_M15_H1'] < pip_max else pip_max ,axis = 1)\n",
    "    df['S_Stop_Loss_M15_H1'] = df.apply(lambda x: x['S_Stop_Loss_M15_H1'] if x['S_Stop_Loss_M15_H1'] > pip_min else pip_min ,axis = 1)\n",
    "    df['S_Limit_M15_H1']     = ratio * df['S_Stop_Loss_M15_H1']\n",
    "\n",
    "    m = 0\n",
    "    for i, row in df.iterrows():\n",
    "        \n",
    "        if i < len(df)-k:\n",
    "            m = k\n",
    "        else:\n",
    "            m = len(df) - i - 3\n",
    "        print('Price_M15_H1 - ',int(100*((i/20000)*20000/len(df))),'%') if ((i > 0) & (i%20000 == 0)) else ''\n",
    "\n",
    "# BUY - Losing Index\n",
    "        \n",
    "        if len(np.where((df.loc[i+1:i+m,'Low_M15'] <= row['Price_M15'] - (0.0001*(row['B_Stop_Loss_M15_H1']))))[0]) == 0:\n",
    "            df.loc[i, 'B_Lose_Idx_M15_H1'] = 1000000\n",
    "        else:\n",
    "            df.loc[i, 'B_Lose_Idx_M15_H1'] = np.where((df.loc[i+1:i+m,'Low_M15'] <= row['Price_M15'] - (0.0001*(row['B_Stop_Loss_M15_H1']))))[0][0]       \n",
    "                       \n",
    "# BUY - Winning Index\n",
    "        \n",
    "        if len(np.where((df.loc[i+1:i+m,'High_M15'] >= row['Price_M15'] + (0.0001*(row['B_Limit_M15_H1']))))[0]) == 0:\n",
    "            df.loc[i, 'B_Win_Idx_M15_H1'] = 1000000\n",
    "\n",
    "        else:\n",
    "            df.loc[i, 'B_Win_Idx_M15_H1'] = np.where((df.loc[i+1:i+m,'High_M15'] >= row['Price_M15'] + (0.0001*(row['B_Limit_M15_H1']))))[0][0]\n",
    "\n",
    "# SELL - Losing Index\n",
    "        \n",
    "        if len(np.where((df.loc[i+1:i+m,'High_M15'] >= row['Price_M15'] + (0.0001*(row['S_Stop_Loss_M15_H1']))))[0]) == 0:\n",
    "            df.loc[i, 'S_Lose_Idx_M15_H1'] = 1000000\n",
    "\n",
    "        else:\n",
    "            df.loc[i, 'S_Lose_Idx_M15_H1'] = np.where((df.loc[i+1:i+m,'High_M15'] >= row['Price_M15'] + (0.0001*(row['S_Stop_Loss_M15_H1']))))[0][0]\n",
    "\n",
    "# SELL - Winning Index\n",
    "        \n",
    "        if len(np.where((df.loc[i+1:i+m,'Low_M15'] <= row['Price_M15'] - (0.0001*(row['S_Limit_M15_H1']))))[0]) == 0:\n",
    "            df.loc[i, 'S_Win_Idx_M15_H1'] = 1000000\n",
    "\n",
    "        else:\n",
    "            df.loc[i, 'S_Win_Idx_M15_H1'] = np.where((df.loc[i+1:i+m,'Low_M15'] <= row['Price_M15'] - (0.0001*(row['S_Limit_M15_H1']))))[0][0]\n",
    "\n",
    "# Label\n",
    "# 1 if B_Win Index < B_Lose Index\n",
    "    df['Labelb_M15_H1'] = df.apply(lambda x: 1 if x['B_Win_Idx_M15_H1'] < x['B_Lose_Idx_M15_H1'] else 0, axis=1)\n",
    "\n",
    "# -1 if S_Win Index < S_Lose Index  \n",
    "    df['Labels_M15_H1'] = df.apply(lambda x: 1 if x['S_Win_Idx_M15_H1'] < x['S_Lose_Idx_M15_H1'] else 0, axis=1) \n",
    "    \n",
    "# Drops all index    \n",
    "    #df.drop(['B_Lose_Idx_M15_H1','B_Win_Idx_M15_H1','S_Lose_Idx_M15_H1','S_Win_Idx_M15_H1'],axis = 1, inplace =True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price_M15_H4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Price_M15_H4(df,k,ratio,pip_min,pip_max,pip_over):\n",
    "    \n",
    "    '''Function that prices all M15 candles based on [Close_M15].shift() value and calcs. its stop loss &\n",
    "                limits based on MS_H1.\n",
    "    INPUTS:\n",
    "\n",
    "     - df    : Dataframe\n",
    "     - ratio : limit:stop_loss ratio (normally 3)\n",
    "\n",
    "    OUTPUTS:\n",
    "\n",
    "     - Price_m15          ==> output: value of [Close_M15].shift()\n",
    "     - B_Stop_Loss_M15_H4 ==> output: # pips between ([Price_M15] - [MS_L_H4])\n",
    "     - B_Limit_M15_H4     ==> output: # pips between (ratio x [B_Stop_Loss_M15_H4])\n",
    "     - S_Stop_Loss_M15_H4 ==> output: # pips between ([MS_H_H4] - [Price_M15])\n",
    "     - S_Limit_M15_H4     ==> output: # pips between (ratio x [S_Stop_Loss_M15_H4])\n",
    "     - Labelb_M15         ==> output: 1 or 0 whether the action of buying  was succesful or not\n",
    "     - Labels_M15         ==> output: 1 or 0 whether the action of selling was succesful or not    \n",
    "    '''\n",
    "    \n",
    "# Price\n",
    "    df['Price_M15']          = df['Close_M15']\n",
    "    df['B_Stop_Loss_M15_H4'] = (10000 * (df['Price_M15'] - df['MS_L_H4'])) - pip_over\n",
    "    df['B_Stop_Loss_M15_H4'] = df.apply(lambda x: x['B_Stop_Loss_M15_H4'] if x['B_Stop_Loss_M15_H4'] < pip_max else pip_max ,axis = 1)\n",
    "    df['B_Stop_Loss_M15_H4'] = df.apply(lambda x: x['B_Stop_Loss_M15_H4'] if x['B_Stop_Loss_M15_H4'] > pip_min else pip_min ,axis = 1)\n",
    "    df['B_Limit_M15_H4']     = ratio * df['B_Stop_Loss_M15_H4']\n",
    "    df['S_Stop_Loss_M15_H4'] = 10000 * (df['MS_H_H4'] - df['Price_M15']) - pip_over\n",
    "    df['S_Stop_Loss_M15_H4'] = df.apply(lambda x: x['S_Stop_Loss_M15_H4'] if x['S_Stop_Loss_M15_H4'] < pip_max else pip_max ,axis = 1)\n",
    "    df['S_Stop_Loss_M15_H4'] = df.apply(lambda x: x['S_Stop_Loss_M15_H4'] if x['S_Stop_Loss_M15_H4'] > pip_min else pip_min ,axis = 1)\n",
    "    df['S_Limit_M15_H4']     = ratio * df['S_Stop_Loss_M15_H4']\n",
    "\n",
    "    m = 0\n",
    "    for i, row in df.iterrows():\n",
    "        \n",
    "        if i < len(df)-k:\n",
    "            m = k\n",
    "        else:\n",
    "            m = len(df) - i - 3\n",
    "        print('Price_M15_H4 - ',int(100*((i/10000)*10000/len(df))),'%') if ((i > 0) & (i%10000 == 0)) else ''\n",
    "\n",
    "# BUY - Losing Index\n",
    "        \n",
    "        if len(np.where((df.loc[i+1:i+m,'Low_M15'] <= row['Price_M15'] - (0.0001*(row['B_Stop_Loss_M15_H4']))))[0]) == 0:\n",
    "            df.loc[i, 'B_Lose_Idx_M15_H4'] = 1000000\n",
    "        else:\n",
    "            df.loc[i, 'B_Lose_Idx_M15_H4'] = np.where((df.loc[i+1:i+m,'Low_M15'] <= row['Price_M15'] - (0.0001*(row['B_Stop_Loss_M15_H4']))))[0][0]       \n",
    "                       \n",
    "# BUY - Winning Index\n",
    "        \n",
    "        if len(np.where((df.loc[i+1:i+m,'High_M15'] >= row['Price_M15'] + (0.0001*(row['B_Limit_M15_H4']))))[0]) == 0:\n",
    "            df.loc[i, 'B_Win_Idx_M15_H4'] = 1000000\n",
    "\n",
    "        else:\n",
    "            df.loc[i, 'B_Win_Idx_M15_H4'] = np.where((df.loc[i+1:i+m,'High_M15'] >= row['Price_M15'] + (0.0001*(row['B_Limit_M15_H4']))))[0][0]\n",
    "\n",
    "# SELL - Losing Index\n",
    "        \n",
    "        if len(np.where((df.loc[i+1:i+m,'High_M15'] >= row['Price_M15'] + (0.0001*(row['S_Stop_Loss_M15_H4']))))[0]) == 0:\n",
    "            df.loc[i, 'S_Lose_Idx_M15_H4'] = 1000000\n",
    "\n",
    "        else:\n",
    "            df.loc[i, 'S_Lose_Idx_M15_H4'] = np.where((df.loc[i+1:i+m,'High_M15'] >= row['Price_M15'] + (0.0001*(row['S_Stop_Loss_M15_H4']))))[0][0]\n",
    "\n",
    "# SELL - Winning Index\n",
    "        \n",
    "        if len(np.where((df.loc[i+1:i+m,'Low_M15'] <= row['Price_M15'] - (0.0001*(row['S_Limit_M15_H4']))))[0]) == 0:\n",
    "            df.loc[i, 'S_Win_Idx_M15_H4'] = 1000000\n",
    "\n",
    "        else:\n",
    "            df.loc[i, 'S_Win_Idx_M15_H4'] = np.where((df.loc[i+1:i+m,'Low_M15'] <= row['Price_M15'] - (0.0001*(row['S_Limit_M15_H4']))))[0][0]\n",
    "\n",
    "# Label\n",
    "# 1 if B_Win Index < B_Lose Index\n",
    "    df['Labelb_M15_H4'] = df.apply(lambda x: 1 if x['B_Win_Idx_M15_H4'] < x['B_Lose_Idx_M15_H4'] else 0, axis=1)\n",
    "\n",
    "# -1 if S_Win Index < S_Lose Index  \n",
    "    df['Labels_M15_H4'] = df.apply(lambda x: 1 if x['S_Win_Idx_M15_H4'] < x['S_Lose_Idx_M15_H4'] else 0, axis=1) \n",
    "    \n",
    "# Drops all index    \n",
    "    #df.drop(['B_Lose_Idx_M15_H4','B_Win_Idx_M15_H4','S_Lose_Idx_M15_H4','S_Win_Idx_M15_H4'],axis = 1, inplace =True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price *** Not Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Price_M15_H4_H1(df,k4,k1,ratio,pip_min,pip_max,pip_over):\n",
    "    \n",
    "    '''Function that prices all M15 candles based on [Close_M15].shift() value and calcs. its stop loss &\n",
    "                limits based on MS_H1.\n",
    "    INPUTS:\n",
    "\n",
    "     - df    : Dataframe\n",
    "     - ratio : limit:stop_loss ratio (normally 3)\n",
    "\n",
    "    OUTPUTS:\n",
    "\n",
    "     - Price_m15          ==> output: value of [Close_M15].shift()\n",
    "     - B_Stop_Loss_M15_H4 ==> output: # pips between ([Price_M15] - [MS_L_H4])\n",
    "     - B_Limit_M15_H4     ==> output: # pips between (ratio x [B_Stop_Loss_M15_H4])\n",
    "     - S_Stop_Loss_M15_H4 ==> output: # pips between ([MS_H_H4] - [Price_M15])\n",
    "     - S_Limit_M15_H4     ==> output: # pips between (ratio x [S_Stop_Loss_M15_H4])\n",
    "     - Labelb_M15         ==> output: 1 or 0 whether the action of buying  was succesful or not\n",
    "     - Labels_M15         ==> output: 1 or 0 whether the action of selling was succesful or not    \n",
    "    '''\n",
    "\n",
    "# Set Price Initiate\n",
    "\n",
    "    df['Price_M15']          = df['Close_M15']    \n",
    "\n",
    "# Price_M15_H4 Initiate\n",
    "\n",
    "    df['B_Stop_Loss_M15_H4'] = (10000 * (df['Price_M15'] - df['MS_L_H4'])) - pip_over\n",
    "    df['B_Stop_Loss_M15_H4'] = df.apply(lambda x: x['B_Stop_Loss_M15_H4'] if x['B_Stop_Loss_M15_H4'] < pip_max else pip_max ,axis = 1)\n",
    "    df['B_Stop_Loss_M15_H4'] = df.apply(lambda x: x['B_Stop_Loss_M15_H4'] if x['B_Stop_Loss_M15_H4'] > pip_min else pip_min ,axis = 1)\n",
    "    df['B_Limit_M15_H4']     = ratio * df['B_Stop_Loss_M15_H4']\n",
    "    df['S_Stop_Loss_M15_H4'] = 10000 * (df['MS_H_H4'] - df['Price_M15']) - pip_over\n",
    "    df['S_Stop_Loss_M15_H4'] = df.apply(lambda x: x['S_Stop_Loss_M15_H4'] if x['S_Stop_Loss_M15_H4'] < pip_max else pip_max ,axis = 1)\n",
    "    df['S_Stop_Loss_M15_H4'] = df.apply(lambda x: x['S_Stop_Loss_M15_H4'] if x['S_Stop_Loss_M15_H4'] > pip_min else pip_min ,axis = 1)\n",
    "    df['S_Limit_M15_H4']     = ratio * df['S_Stop_Loss_M15_H4']\n",
    "    \n",
    "# Price_M15_H1 Initiate    \n",
    "\n",
    "    df['B_Stop_Loss_M15_H1'] = (10000 * (df['Price_M15'] - df['MS_L_H1'])) - pip_over\n",
    "    df['B_Stop_Loss_M15_H1'] = df.apply(lambda x: x['B_Stop_Loss_M15_H1'] if x['B_Stop_Loss_M15_H1'] < pip_max else pip_max ,axis = 1)\n",
    "    df['B_Stop_Loss_M15_H1'] = df.apply(lambda x: x['B_Stop_Loss_M15_H1'] if x['B_Stop_Loss_M15_H1'] > pip_min else pip_min ,axis = 1)\n",
    "    df['B_Limit_M15_H1']     = ratio * df['B_Stop_Loss_M15_H1']\n",
    "    df['S_Stop_Loss_M15_H1'] = 10000 * (df['MS_H_H1'] - df['Price_M15']) - pip_over\n",
    "    df['S_Stop_Loss_M15_H1'] = df.apply(lambda x: x['S_Stop_Loss_M15_H1'] if x['S_Stop_Loss_M15_H1'] < pip_max else pip_max ,axis = 1)\n",
    "    df['S_Stop_Loss_M15_H1'] = df.apply(lambda x: x['S_Stop_Loss_M15_H1'] if x['S_Stop_Loss_M15_H1'] > pip_min else pip_min ,axis = 1)\n",
    "    df['S_Limit_M15_H1']     = ratio * df['S_Stop_Loss_M15_H1']\n",
    "\n",
    "    m = 0\n",
    "    for i, row in df.iterrows():\n",
    "        \n",
    "        if i < len(df)-k4:\n",
    "            m = k4\n",
    "        else:\n",
    "            m = len(df) - i - 3\n",
    "        \n",
    "        print('Price - ',int(100*((i/20000)*20000/len(df))),'%') if ((i > 0) & (i%20000 == 0)) else ''\n",
    "\n",
    "# BUY - Losing Index M15_H4\n",
    "        \n",
    "        if len(np.where((df.loc[i+1:i+m,'Low_M15'] <= row['Price_M15'] - (0.0001*(row['B_Stop_Loss_M15_H4']))))[0]) == 0:\n",
    "            df.loc[i, 'B_Lose_Idx_M15_H4'] = 1000000\n",
    "        else:\n",
    "            df.loc[i, 'B_Lose_Idx_M15_H4'] = np.where((df.loc[i+1:i+m,'Low_M15'] <= row['Price_M15'] - (0.0001*(row['B_Stop_Loss_M15_H4']))))[0][0]       \n",
    "                       \n",
    "# BUY - Winning Index M15_H4\n",
    "        \n",
    "        if len(np.where((df.loc[i+1:i+m,'High_M15'] >= row['Price_M15'] + (0.0001*(row['B_Limit_M15_H4']))))[0]) == 0:\n",
    "            df.loc[i, 'B_Win_Idx_M15_H4'] = 1000000\n",
    "\n",
    "        else:\n",
    "            df.loc[i, 'B_Win_Idx_M15_H4'] = np.where((df.loc[i+1:i+m,'High_M15'] >= row['Price_M15'] + (0.0001*(row['B_Limit_M15_H4']))))[0][0]\n",
    "\n",
    "# SELL - Losing Index M15_H4\n",
    "        \n",
    "        if len(np.where((df.loc[i+1:i+m,'High_M15'] >= row['Price_M15'] + (0.0001*(row['S_Stop_Loss_M15_H4']))))[0]) == 0:\n",
    "            df.loc[i, 'S_Lose_Idx_M15_H4'] = 1000000\n",
    "\n",
    "        else:\n",
    "            df.loc[i, 'S_Lose_Idx_M15_H4'] = np.where((df.loc[i+1:i+m,'High_M15'] >= row['Price_M15'] + (0.0001*(row['S_Stop_Loss_M15_H4']))))[0][0]\n",
    "\n",
    "# SELL - Winning Index M15_H4\n",
    "        \n",
    "        if len(np.where((df.loc[i+1:i+m,'Low_M15'] <= row['Price_M15'] - (0.0001*(row['S_Limit_M15_H4']))))[0]) == 0:\n",
    "            df.loc[i, 'S_Win_Idx_M15_H4'] = 1000000\n",
    "\n",
    "        else:\n",
    "            df.loc[i, 'S_Win_Idx_M15_H4'] = np.where((df.loc[i+1:i+m,'Low_M15'] <= row['Price_M15'] - (0.0001*(row['S_Limit_M15_H4']))))[0][0]\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "        if i < len(df)-k1:\n",
    "            n = k1\n",
    "        else:\n",
    "            n = len(df) - i - 3\n",
    "\n",
    "# BUY - Losing Index M15_H1\n",
    "        \n",
    "        if len(np.where((df.loc[i+1:i+n,'Low_M15'] <= row['Price_M15'] - (0.0001*(row['B_Stop_Loss_M15_H1']))))[0]) == 0:\n",
    "            df.loc[i, 'B_Lose_Idx_M15_H1'] = 1000000\n",
    "        else:\n",
    "            df.loc[i, 'B_Lose_Idx_M15_H1'] = np.where((df.loc[i+1:i+n,'Low_M15'] <= row['Price_M15'] - (0.0001*(row['B_Stop_Loss_M15_H1']))))[0][0]       \n",
    "                       \n",
    "# BUY - Winning Index M15_H1\n",
    "        \n",
    "        if len(np.where((df.loc[i+1:i+n,'High_M15'] >= row['Price_M15'] + (0.0001*(row['B_Limit_M15_H1']))))[0]) == 0:\n",
    "            df.loc[i, 'B_Win_Idx_M15_H1'] = 1000000\n",
    "\n",
    "        else:\n",
    "            df.loc[i, 'B_Win_Idx_M15_H1'] = np.where((df.loc[i+1:i+n,'High_M15'] >= row['Price_M15'] + (0.0001*(row['B_Limit_M15_H1']))))[0][0]\n",
    "\n",
    "# SELL - Losing Index M15_H1\n",
    "        \n",
    "        if len(np.where((df.loc[i+1:i+n,'High_M15'] >= row['Price_M15'] + (0.0001*(row['S_Stop_Loss_M15_H1']))))[0]) == 0:\n",
    "            df.loc[i, 'S_Lose_Idx_M15_H1'] = 1000000\n",
    "\n",
    "        else:\n",
    "            df.loc[i, 'S_Lose_Idx_M15_H1'] = np.where((df.loc[i+1:i+n,'High_M15'] >= row['Price_M15'] + (0.0001*(row['S_Stop_Loss_M15_H1']))))[0][0]\n",
    "\n",
    "# SELL - Winning Index M15_H1\n",
    "        \n",
    "        if len(np.where((df.loc[i+1:i+n,'Low_M15'] <= row['Price_M15'] - (0.0001*(row['S_Limit_M15_H1']))))[0]) == 0:\n",
    "            df.loc[i, 'S_Win_Idx_M15_H1'] = 1000000\n",
    "\n",
    "        else:\n",
    "            df.loc[i, 'S_Win_Idx_M15_H1'] = np.where((df.loc[i+1:i+n,'Low_M15'] <= row['Price_M15'] - (0.0001*(row['S_Limit_M15_H1']))))[0][0]\n",
    "            \n",
    "#Label M15_H4\n",
    "# 1 if B_Win Index < B_Lose Index\n",
    "    df['Labelb_M15_H4'] = df.apply(lambda x: 1 if x['B_Win_Idx_M15_H4'] < x['B_Lose_Idx_M15_H4'] else 0, axis=1)\n",
    "\n",
    "# -1 if S_Win Index < S_Lose Index  \n",
    "    df['Labels_M15_H4'] = df.apply(lambda x: 1 if x['S_Win_Idx_M15_H4'] < x['S_Lose_Idx_M15_H4'] else 0, axis=1) \n",
    "\n",
    "# Label M15_H1\n",
    "# 1 if B_Win Index < B_Lose Index\n",
    "    df['Labelb_M15_H1'] = df.apply(lambda x: 1 if x['B_Win_Idx_M15_H1'] < x['B_Lose_Idx_M15_H1'] else 0, axis=1)\n",
    "\n",
    "# -1 if S_Win Index < S_Lose Index  \n",
    "    df['Labels_M15_H1'] = df.apply(lambda x: 1 if x['S_Win_Idx_M15_H1'] < x['S_Lose_Idx_M15_H1'] else 0, axis=1) \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "### Read Files and apply functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sd = '2007-01-02 00:00:00'\n",
    "#ed = '2021-06-01 00:00:00'\n",
    "\n",
    "#dff_D1  = Read_prep_df( File = '../0. Data/EURUSD_D1.xlsx' , n = 0,     sd = sd, ed = ed, clean_sweep = 1, swing_max = 3, perc_95 = 10000) # per_75 = 577.2; per_95 = 990\n",
    "#dff_H4  = Read_prep_df( File = '../0. Data/EURUSD_H4.xlsx' , n = 0,     sd = sd, ed = ed, clean_sweep = 1, swing_max = 3, perc_95 = 480  ) # per_75 = 259.1; per_95 = 480\n",
    "#dff_H1  = Read_prep_df( File = '../0. Data/EURUSD_H1.xlsx' , n = 15000, sd = sd, ed = ed, clean_sweep = 1, swing_max = 3, perc_95 = 220  ) # per_75 = 126.6! per_95 = 220\n",
    "#dff_M15 = Read_prep_df( File = '../0. Data/EURUSD_M15.xlsx', n = 15000, sd = sd, ed = ed, clean_sweep = 1, swing_max = 3, perc_95 = 120  ) # per_75 = 81.8! per_95 = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge dataframes - df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_D1_r   = Rename_df(dff_D1,'D1')\n",
    "#df_H4_r   = Rename_df(dff_H4,'H4')\n",
    "#df_H1_r   = Rename_df(dff_H1,'H1')\n",
    "#df_M15_r  = Rename_df(dff_M15,'M15')\n",
    "\n",
    "#df_H4_D1  = Merge_shift_df_H4_D1(df_H4_r,df_D1_r)\n",
    "#df_H1_H4  = Merge_shift_df_H1_H4(df_H1_r,df_H4_D1)\n",
    "#dff_M15_H1 = Merge_shift_df_M15_H1(df_M15_r,df_H1_H4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MS_retracement- df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MS_retracement(dff_M15_H1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dffv = Price_M15_H4(dff_M15_H1 , k = 5000, ratio = 3, pip_min = 15, pip_max = 125, pip_over = -3)\n",
    "#dffv = Price_M15_H1(dffv       , k = 1500, ratio = 3, pip_min = 15, pip_max = 60 , pip_over = -3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_D1    .to_pickle('df_D1_EURUSD.pkl')\n",
    "#df_H4    .to_pickle('df_H4_EURUSD.pkl')\n",
    "#df_H1    .to_pickle('df_H1_EURUSD.pkl')\n",
    "#df_M15   .to_pickle('df_M15_EURUSD.pkl')\n",
    "#df_M15_H1.to_pickle('df_M15_H1.pkl')\n",
    "#dfv      .to_pickle('dfv_EURUSD.pkl')\n",
    "\n",
    "#dff_D1    .to_pickle('dff_D1_EURUSD.pkl')\n",
    "#dff_H4    .to_pickle('dff_H4_EURUSD.pkl')\n",
    "#dff_H1    .to_pickle('dff_H1_EURUSD.pkl')\n",
    "#dff_M15   .to_pickle('dff_M15_EURUSD.pkl')\n",
    "#dff_M15_H1.to_pickle('dff_M15_H1.pkl')\n",
    "#dffv      .to_pickle('dffv_EURUSD.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_D1     = pd.read_pickle('dff_D1_EURUSD.pkl')\n",
    "df_H4     = pd.read_pickle('dff_H4_EURUSD.pkl')\n",
    "df_H1     = pd.read_pickle('dff_H1_EURUSD.pkl')\n",
    "df_M15    = pd.read_pickle('dff_M15_EURUSD.pkl')\n",
    "#df_M15_H1 = pd.read_pickle('df_M15_H1.pkl')\n",
    "dfv       = pd.read_pickle('dffv_EURUSD.pkl')\n",
    "\n",
    "#df_D1     = pd.read_pickle('df_D1_EURUSD.pkl')\n",
    "#df_H4     = pd.read_pickle('df_H4_EURUSD.pkl')\n",
    "#df_H1     = pd.read_pickle('df_H1_EURUSD.pkl')\n",
    "#df_M15    = pd.read_pickle('df_M15_EURUSD.pkl')\n",
    "#df_M15_H1 = pd.read_pickle('df_M15_H1.pkl')\n",
    "#dfv       = pd.read_pickle('dfv_EURUSD.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test merge function\n",
    "r, rge = random.randint(0, len(dfv)), 100\n",
    "dfv.loc[r:r+rge,['Date_M15','Date_H1','Date_H4','Date_D1','Close_M15','Close_H1','Close_H4','Close_D1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test Closing Function\n",
    "r, rge = random.randint(0, len(dfv)), 20\n",
    "dfv.loc[r:r+rge,['Open_M15','Close_M15','Closing_M15','Open_H1','Close_H1','Closing_H1','Open_H4','Close_H4','Closing_H4','Open_D1','Close_D1','Closing_D1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test Market Structure\n",
    "r, rge = random.randint(0, len(df_M15)), 20\n",
    "df_M15.loc[r:r+rge,['Open','High','Low','Close','MS_Sit','MS_H','MS_L']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test Trend & N_Breaks & MS_N\n",
    "r, rge = random.randint(0, len(df_M15)), 200\n",
    "df_M15[df_M15['MS_Sit'] != 'MS'].loc[r:r+rge,['MS_Sit','Trend','N_Breaks','MS_N']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test MS_periods\n",
    "r, rge = random.randint(0, len(df_M15)), 20\n",
    "df_M15.loc[r:r+rge,['MS_Sit','MS_Pds']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test MS_range\n",
    "r, rge = random.randint(0, len(df_M15)), 20\n",
    "df_M15.loc[r:r+rge,['MS_H','MS_L','MS_Sit','MS_range']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test MS_retracement\n",
    "r, rge = random.randint(0, len(dfv)), 20\n",
    "dfv.loc[r:r+rge,['Date_M15','MS_L_M15','MS_H_M15','Close_M15','MS_Sit_M15','Trend_M15','MS_retracement_M15']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test Price\n",
    "r, rge = random.randint(0, len(dfv)), 50\n",
    "\n",
    "dfv['Price_Lose'] = dfv['Price_M15'] + 0.0001 * dfv['S_Stop_Loss_M15_H1']\n",
    "dfv['Price_Win'] = dfv['Price_M15'] - 0.0001 * dfv['S_Limit_M15_H1']\n",
    "dfv.loc[r:r+rge,['High_M15','Low_M15','Close_M15','Price_M15','S_Stop_Loss_M15_H1','Price_Lose','S_Lose_Idx_M15_H1','S_Limit_M15_H1','Price_Win','S_Win_Idx_M15_H1','Labels_M15_H1']]\n",
    "\n",
    "#dfv.drop(['Price_Lose','Price_Win'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Indicator - SMA\n",
    "r, rge = random.randint(0, len(df_M15)), 20\n",
    "df_M15.loc[r:r+rge,['Close','Close_im1','U_Move','D_Move','14_Avg_U','14_Avg_D','RSI_14_SMA']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Indicator - EMA\n",
    "r, rge = random.randint(0, len(df_M15)), 20\n",
    "df_M15.loc[r:r+rge,['Close','Close_im1','U_Move','D_Move','14_Avg_U_EMA','14_Avg_D_EMA','RSI_14_EMA']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Indicator - MACD\n",
    "r, rge = random.randint(0, len(df_M15)), 20\n",
    "df_M15.loc[r:r+rge,['EMA_12_macd','EMA_26_macd','MACD_12_26','EMA_MACD_12_26_9','Hist_MACD_12_26_9','MACD_signal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Indicator - Bollinger Bands\n",
    "r, rge = random.randint(0, len(df_M15)), 20\n",
    "df_M15.loc[r:r+rge,['Close','Boll_SMA_20','Boll_SMA_20_Var_+2','Boll_SMA_20_Var_+2','Dist_BollB_SMA_20_Var_-2','Dist_BollB_SMA_20_Var_+2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfv = pd.read_pickle('dffv_EURUSD.pkl')\n",
    "df  = clean_data(dfv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Auto_Analysis(df, itr, M15_H1, prt_ret, a1b, a2b, b1b, b2b, c1b, c2b, d1b, d2b, r1, r2, s1, s2, t1, t2, u1, u2, n1, n2, o1, o2, p1, p2, q1, q2): \n",
    "    \n",
    "    a1s, a2s, b1s, b2s, c1s, c2s, d1s, d2s = -a1b, -a2b, -b1b, -b2b, -c1b, -c2b, -d1b, -d2b   \n",
    "    df3, df2, df1 = split3_dfs(df)    \n",
    "\n",
    "    list_idx_1 = df1.index.to_list()\n",
    "    list_idx_2 = df2.index.to_list()\n",
    "    list_idx_3 = df3.index.to_list()\n",
    "\n",
    "    # Initialising lists\n",
    "\n",
    "    xpa1, xca1, xpb1, xcb1 = [], [], [], []\n",
    "    xpa2, xca2, xpb2, xcb2 = [], [], [], []\n",
    "    xpa3, xca3, xpb3, xcb3 = [], [], [], []\n",
    "\n",
    "    ypa1, yca1, ypb1, ycb1 = [], [], [], []\n",
    "    ypa2, yca2, ypb2, ycb2 = [], [], [], []\n",
    "    ypa3, yca3, ypb3, ycb3 = [], [], [], []\n",
    "\n",
    "    def analysis_b(df):\n",
    "\n",
    "        x = df[(df['MS_retracement_M15'].between(-1000000, 1000000)) & \n",
    "           ((df['Trend_D1']  == a1b) | (df['Trend_D1']  == a2b)) & \n",
    "           ((df['Trend_H4']  == b1b) | (df['Trend_H4']  == b2b)) &\n",
    "           ((df['Trend_H1']  == c1b) | (df['Trend_H1']  == c2b)) &\n",
    "           ((df['Trend_M15'] == d1b) | (df['Trend_M15'] == d2b)) &\n",
    "           #((df['Closing_D1'] == e1b) | (df['Closing_D1'] == e2b)) &\n",
    "           #((df['Closing_H4'] == f1b) | (df['Closing_H4'] == f2b)) &\n",
    "           #((df['Closing_H1'] == g1b) | (df['Closing_H1'] == g2b)) &\n",
    "           #((df['Closing_H1'] == h1b) | (df['Closing_H1'] == h2b)) &\n",
    "           (df['MS_retracement_D1'] .between(r1, r2)) &\n",
    "           (df['MS_retracement_H4'] .between(s1, s2)) & \n",
    "           (df['MS_retracement_H1'] .between(t1, t2)) &\n",
    "           (df['MS_retracement_M15'].between(u1, u2)) &\n",
    "           #(df['N_Breaks_D1'] .between(k1, k2)) &\n",
    "           #(df['N_Breaks_H4'] .between(l1, l2)) & \n",
    "           #(df['N_Breaks_H1'] .between(m1, m2)) &\n",
    "           (df['MS_range_D1'] .between(n1, n2)) & \n",
    "           (df['MS_range_H4'] .between(o1, o2)) & \n",
    "           (df['MS_range_H1'] .between(p1, p2)) &\n",
    "           (df['MS_range_M15'].between(q1, q2)) &\n",
    "           (df['MS_retracement_M15'].between(-1000000, 1000000))]['Labelb_'+str(M15_H1)].describe()[:2]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def analysis_s(df):\n",
    "        \n",
    "        y = df[(df['MS_retracement_M15'].between(-1000000, 1000000)) & \n",
    "           ((df['Trend_D1']  == a1s) | (df['Trend_D1']  == a2s)) & \n",
    "           ((df['Trend_H4']  == b1s) | (df['Trend_H4']  == b2s)) &\n",
    "           ((df['Trend_H1']  == c1s) | (df['Trend_H1']  == c2s)) &\n",
    "           ((df['Trend_M15'] == d1s) | (df['Trend_M15'] == d2s)) &\n",
    "           #((df['Closing_D4'] == e1s) | (df['Closing_D1'] == e2s)) &\n",
    "           #((df['Closing_H4'] == f1s) | (df['Closing_H4'] == f2s)) &\n",
    "           #((df['Closing_H1'] == g1s) | (df['Closing_H1'] == g2s)) &\n",
    "           #((df['Closing_H1'] == h1s) | (df['Closing_H1'] == h2s)) &\n",
    "           (df['MS_retracement_D1'] .between(r1, r2)) &\n",
    "           (df['MS_retracement_H4'] .between(s1, s2)) & \n",
    "           (df['MS_retracement_H1'] .between(t1, t2)) &\n",
    "           (df['MS_retracement_M15'].between(u1, u2)) &\n",
    "           #(df['N_Breaks_D1'] .between(k1, k2)) &\n",
    "           #(df['N_Breaks_H4'] .between(l1, l2)) & \n",
    "           #(df['N_Breaks_H1'] .between(m1, m2)) &\n",
    "           (df['MS_range_D1'] .between(n1, n2)) & \n",
    "           (df['MS_range_H4'] .between(o1, o2)) & \n",
    "           (df['MS_range_H1'] .between(p1, p2)) &\n",
    "           (df['MS_range_M15'].between(q1, q2)) &\n",
    "           (df['MS_retracement_M15'].between(-1000000, 1000000))]['Labels_'+str(M15_H1)].describe()[:2]\n",
    "\n",
    "        return y\n",
    "    \n",
    "    if prt_ret == 'print':\n",
    "\n",
    "        for i in range(0,itr):\n",
    "\n",
    "            df1a = df1.loc[Space_dfa(df1,10),:]\n",
    "            df1b = df1.loc[Space_dfb(list_idx_1,int(len(df1)/10)),:]\n",
    "            df2a = df2.loc[Space_dfa(df2,10),:]\n",
    "            df2b = df2.loc[Space_dfb(list_idx_2,int(len(df2)/10)),:]\n",
    "            df3a = df3.loc[Space_dfa(df3,10),:]\n",
    "            df3b = df3.loc[Space_dfb(list_idx_3,int(len(df3)/10)),:]\n",
    "\n",
    "            xa1, xb1 = analysis_b(df1a), analysis_b(df1b)\n",
    "            xa2, xb2 = analysis_b(df2a), analysis_b(df2b)\n",
    "            xa3, xb3 = analysis_b(df3a), analysis_b(df3b)\n",
    "\n",
    "            ya1, yb1 = analysis_s(df1a), analysis_s(df1b)\n",
    "            ya2, yb2 = analysis_s(df2a), analysis_s(df2b)\n",
    "            ya3, yb3 = analysis_s(df3a), analysis_s(df3b)\n",
    "\n",
    "            xpa1.append(xa1[1])\n",
    "            xca1.append(xa1[0])\n",
    "            xpb1.append(xb1[1])\n",
    "            xcb1.append(xb1[0])\n",
    "\n",
    "            xpa2.append(xa2[1])\n",
    "            xca2.append(xa2[0])\n",
    "            xpb2.append(xb2[1])\n",
    "            xcb2.append(xb2[0])\n",
    "\n",
    "            xpa3.append(xa3[1])\n",
    "            xca3.append(xa3[0])\n",
    "            xpb3.append(xb3[1])\n",
    "            xcb3.append(xb3[0])\n",
    "\n",
    "            ypa1.append(ya1[1])\n",
    "            yca1.append(ya1[0])\n",
    "            ypb1.append(yb1[1])\n",
    "            ycb1.append(yb1[0])\n",
    "\n",
    "            ypa2.append(ya2[1])\n",
    "            yca2.append(ya2[0])\n",
    "            ypb2.append(yb2[1])\n",
    "            ycb2.append(yb2[0])\n",
    "\n",
    "            ypa3.append(ya3[1])\n",
    "            yca3.append(ya3[0])\n",
    "            ypb3.append(yb3[1])\n",
    "            ycb3.append(yb3[0])\n",
    "\n",
    "        count_b = round((mean(xca1)+mean(xca2)+mean(xca3)+mean(xcb1)+mean(xcb2)+mean(xcb3))/6,2)\n",
    "        count_s = round((mean(yca1)+mean(yca2)+mean(yca3)+mean(ycb1)+mean(ycb2)+mean(ycb3))/6,2)\n",
    "\n",
    "        avg_b  = round((mean(xpa1)*mean(xca1) + mean(xpa2)*mean(xca2) + mean(xpa3)*mean(xca3)+mean(xpb1)*mean(xcb1) + mean(xpb2)*mean(xcb2) + mean(xpb3)*mean(xcb3))/(6*count_b),2)   \n",
    "        avg_s  = round((mean(ypa1)*mean(yca1) + mean(ypa2)*mean(yca2) + mean(ypa3)*mean(yca3)+mean(ypb1)*mean(ycb1) + mean(ypb2)*mean(ycb2) + mean(ypb3)*mean(ycb3))/(6*count_s),2)   \n",
    "\n",
    "        print('TOTAL SCORE BUY  --- MEAN:',avg_b,' --- COUNT:',count_b)\n",
    "        print('TOTAL SCORE SELL --- MEAN:',avg_s,' --- COUNT:',count_s)\n",
    "        print('BUY VBLES   :','(',a1b,a2b,')(',b1b,b2b,')(',c1b,c2b,')(',d1b,d2b,')')\n",
    "        print('SELL VBLES  :','(',a1s,a2s,')(',b1s,b2s,')(',c1s,c2s,')(',d1s,d2s,')')\n",
    "        print('COMMON VBLES:','(',r1,r2,')(',s1,s2,')(',t1,t2,')(',u1,u2,')n(',n1,n2,')o(',o1,o2,')p(',p1,p2,')q(',q1,q2,')')\n",
    "        print('BUY DFRAMES:')\n",
    "        print('df1a',round(mean(xpa1),2),round(mean(xca1),2),' - df1b',round(mean(xpb1),2),round(mean(xcb1),2))\n",
    "        print('df2a',round(mean(xpa2),2),round(mean(xca2),2),' - df2b',round(mean(xpb2),2),round(mean(xcb2),2))\n",
    "        print('df3a',round(mean(xpa3),2),round(mean(xca3),2),' - df3b',round(mean(xpb3),2),round(mean(xcb3),2))\n",
    "        print('Tot score:')\n",
    "        print('mean:',avg_b,'   count:',count_b)\n",
    "        print('SELL DFRAMES:')\n",
    "        print('df1a',round(mean(ypa1),2),round(mean(yca1),2),' - df1b',round(mean(ypb1),2),round(mean(ycb1),2))\n",
    "        print('df2a',round(mean(ypa2),2),round(mean(yca2),2),' - df2b',round(mean(ypb2),2),round(mean(ycb2),2))\n",
    "        print('df3a',round(mean(ypa3),2),round(mean(yca3),2),' - df3b',round(mean(ypb3),2),round(mean(ycb3),2))\n",
    "        print('Tot score:')\n",
    "        print('mean:',avg_s,'   count:',count_s)\n",
    "        \n",
    "    else:\n",
    "        x = df[(df['MS_retracement_M15'].between(-1000000, 1000000)) & \n",
    "           ((df['Trend_D1']  == a1b) | (df['Trend_D1']  == a2b)) & \n",
    "           ((df['Trend_H4']  == b1b) | (df['Trend_H4']  == b2b)) &\n",
    "           ((df['Trend_H1']  == c1b) | (df['Trend_H1']  == c2b)) &\n",
    "           ((df['Trend_M15'] == d1b) | (df['Trend_M15'] == d2b)) &\n",
    "           #((df['Closing_D4'] == e1b) | (df['Closing_D1'] == e2b)) &\n",
    "           #((df['Closing_H4'] == f1b) | (df['Closing_H4'] == f2b)) &\n",
    "           #((df['Closing_H1'] == g1b) | (df['Closing_H1'] == g2b)) &\n",
    "           #((df['Closing_H1'] == h1b) | (df['Closing_H1'] == h2b)) &\n",
    "           (df['MS_retracement_D1'] .between(r1, r2)) &\n",
    "           (df['MS_retracement_H4'] .between(s1, s2)) & \n",
    "           (df['MS_retracement_H1'] .between(t1, t2)) &\n",
    "           (df['MS_retracement_M15'].between(u1, u2)) &\n",
    "           #(df['N_Breaks_D1'] .between(k1, k2)) &\n",
    "           #(df['N_Breaks_H4'] .between(l1, l2)) & \n",
    "           #(df['N_Breaks_H1'] .between(m1, m2)) &\n",
    "           (df['MS_range_D1'] .between(n1, n2)) & \n",
    "           (df['MS_range_H4'] .between(o1, o2)) & \n",
    "           (df['MS_range_H1'] .between(p1, p2)) &\n",
    "           (df['MS_range_M15'].between(q1, q2)) &\n",
    "           (df['MS_retracement_M15'].between(-1000000, 1000000))]\n",
    "\n",
    "        y = df[(df['MS_retracement_M15'].between(-1000000, 1000000)) & \n",
    "           ((df['Trend_D1']  == a1s) | (df['Trend_D1']  == a2s)) & \n",
    "           ((df['Trend_H4']  == b1s) | (df['Trend_H4']  == b2s)) &\n",
    "           ((df['Trend_H1']  == c1s) | (df['Trend_H1']  == c2s)) &\n",
    "           ((df['Trend_M15'] == d1s) | (df['Trend_M15'] == d2s)) &\n",
    "           #((df['Closing_D4'] == e1s) | (df['Closing_D1'] == e2s)) &\n",
    "           #((df['Closing_H4'] == f1s) | (df['Closing_H4'] == f2s)) &\n",
    "           #((df['Closing_H1'] == g1s) | (df['Closing_H1'] == g2s)) &\n",
    "           #((df['Closing_H1'] == h1s) | (df['Closing_H1'] == h2s)) &\n",
    "           (df['MS_retracement_D1'] .between(r1, r2)) &\n",
    "           (df['MS_retracement_H4'] .between(s1, s2)) & \n",
    "           (df['MS_retracement_H1'] .between(t1, t2)) &\n",
    "           (df['MS_retracement_M15'].between(u1, u2)) &\n",
    "           #(df['N_Breaks_D1'] .between(k1, k2)) &\n",
    "           #(df['N_Breaks_H4'] .between(l1, l2)) & \n",
    "           #(df['N_Breaks_H1'] .between(m1, m2)) &\n",
    "           (df['MS_range_D1'] .between(n1, n2)) & \n",
    "           (df['MS_range_H4'] .between(o1, o2)) & \n",
    "           (df['MS_range_H1'] .between(p1, p2)) &\n",
    "           (df['MS_range_M15'].between(q1, q2)) &\n",
    "           (df['MS_retracement_M15'].between(-1000000, 1000000))]\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split3_dfs(df):\n",
    "    \n",
    "    df1 = df.loc[0:int(len(df)/3),:].copy()\n",
    "    df2 = df.loc[int(  len(df)/3) +1:int(2*len(df)/3),:].copy()\n",
    "    df3 = df.loc[int(2*len(df)/3) +1:,:].copy()\n",
    "    \n",
    "    df1 = df1.reset_index(drop=True)\n",
    "    df2 = df2.reset_index(drop=True)\n",
    "    df3 = df3.reset_index(drop=True)\n",
    "    \n",
    "    return df1, df2, df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space_dfa - Function that spaces dataframe rows by a distance k\n",
    "\n",
    "import random\n",
    "\n",
    "def Space_dfa(df,k):\n",
    "    \n",
    "    l1 = df.index.to_list()\n",
    "    r  = random.randint(0, k)\n",
    "    l2 = [r]\n",
    "    \n",
    "    for i in range(r+1,len(df),k):\n",
    "        l2.append(i)\n",
    "    \n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space_dfb - Function that picks 'size' number of random rows from dataframe\n",
    "\n",
    "import random\n",
    "\n",
    "def Space_dfb(l1,size):\n",
    "\n",
    "    l2 = random.sample(l1, size)\n",
    "    l2.sort()\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify when these point happen\n",
    "\n",
    "def N_months(df):\n",
    "    \n",
    "    return pd.to_datetime(pd.Series(df['Date_M15']), format = '%Y%m%d').apply(lambda x: x.strftime('%Y-%m')).nunique()\n",
    "    \n",
    "def diff_month(df):\n",
    "    \n",
    "    return ((df.reset_index().loc[len(df.reset_index())-1,'Date_M15'].year - df.reset_index().loc[0,'Date_M15'].year) * 12) + (df.reset_index().loc[len(df.reset_index())-1,'Date_M15'].month - df.reset_index().loc[0,'Date_M15'].month)    \n",
    "    \n",
    "def N_months_concat(df1, df2):\n",
    "    \n",
    "    s1 = pd.to_datetime(pd.Series(df1['Date_M15']), format = '%Y%m%d').apply(lambda x: x.strftime('%Y-%m'))\n",
    "    s2 = pd.to_datetime(pd.Series(df2['Date_M15']), format = '%Y%m%d').apply(lambda x: x.strftime('%Y-%m'))\n",
    "    \n",
    "    return s1.append(s2).nunique()\n",
    "\n",
    "def N_months_concat4(df1, df2, df3, df4):\n",
    "    \n",
    "    s1 = pd.to_datetime(pd.Series(df1['Date_M15']), format = '%Y%m%d').apply(lambda x: x.strftime('%Y-%m'))\n",
    "    s2 = pd.to_datetime(pd.Series(df2['Date_M15']), format = '%Y%m%d').apply(lambda x: x.strftime('%Y-%m'))\n",
    "    s3 = pd.to_datetime(pd.Series(df3['Date_M15']), format = '%Y%m%d').apply(lambda x: x.strftime('%Y-%m'))\n",
    "    s4 = pd.to_datetime(pd.Series(df4['Date_M15']), format = '%Y%m%d').apply(lambda x: x.strftime('%Y-%m'))\n",
    "    \n",
    "    return s1.append(s2).append(s3).append(s4).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PARAMETERS USED:\n",
    "\n",
    "Market_Structure(df,clean_sweep,x,perc_95)\n",
    "\n",
    "    D1  --> clean_sweep = 1; x irrelevant; p_100 = 100000\n",
    "    H4  --> clean_sweep = 1; x irrelevant; p_95  = 480\n",
    "    H1  --> clean_sweep = 1; x irrelevant; p_95  = 220\n",
    "    M15 --> clean_sweep = 1; x irrelevant; p_x   = 220\n",
    "\n",
    "Price_M15_H4(df_M15_H1, k = 5000, ratio = 3, pip_min = 15, pip_max = 125, pip_over = -3)\n",
    "Price_M15_H1(df_M15_H1, k = 1500, ratio = 3, pip_min = 15, pip_max = 90 , pip_over = -3)\n",
    "'''\n",
    "\n",
    "def MS_index_M15(df):\n",
    "    \n",
    "# Show data analitics based on individual Market structures (without taking into account repetitions)\n",
    "    idx_list_break = []\n",
    "    for i in range (1,len(df)-1):\n",
    "    \n",
    "        if (df.loc[i,'MS_Sit_M15'] == 'MS') & ((df.loc[i+1,'MS_Sit_M15'] == 'Up_Break') | (df.loc[i+1,'MS_Sit_M15'] == 'Dw_Break')):\n",
    "            idx_list_break.append(i)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return idx_list_break\n",
    "\n",
    "def MS_index_H1(df):\n",
    "    \n",
    "# Show data analitics based on individual Market structures (without taking into account repetitions)\n",
    "    idx_list_break = []\n",
    "    for i in range (1,len(df)-1):\n",
    "    \n",
    "        if (df.loc[i,'MS_Sit_H1'] == 'MS') & ((df.loc[i+1,'MS_Sit_H1'] == 'Up_Break') | (df.loc[i+1,'MS_Sit_H1'] == 'Dw_Break')):\n",
    "            idx_list_break.append(i)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return idx_list_break\n",
    "\n",
    "def MS_index_H4(df):\n",
    "    \n",
    "# Show data analitics based on individual Market structures (without taking into account repetitions)\n",
    "    idx_list_break = []\n",
    "    for i in range (1,len(df)-1):\n",
    "    \n",
    "        if (df.loc[i,'MS_Sit_H4'] == 'MS') & ((df.loc[i+1,'MS_Sit_H4'] == 'Up_Break') | (df.loc[i+1,'MS_Sit_H4'] == 'Dw_Break')):\n",
    "            idx_list_break.append(i)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return idx_list_break\n",
    "\n",
    "def MS_index_D1(df):\n",
    "    \n",
    "# Show data analitics based on individual Market structures (without taking into account repetitions)\n",
    "    idx_list_break = []\n",
    "    for i in range (1,len(df)-1):\n",
    "    \n",
    "        if (df.loc[i,'MS_Sit_D1'] == 'MS') & ((df.loc[i+1,'MS_Sit_D1'] == 'Up_Break') | (df.loc[i+1,'MS_Sit_D1'] == 'Dw_Break')):\n",
    "            idx_list_break.append(i)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return idx_list_break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explores df_D1, df_H4, df_H1 or df_M15 data\n",
    "\n",
    "r, rge = str(df_M15.loc[random.randint(0, len(df_M15)),'Date']), 100\n",
    "plot_MS(df_M15, r, rge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Explores df data -- NEEEEDS CHANGING\n",
    "\n",
    "#r, rge = str(df.loc[random.randint(0, len(df)),'Date_H1']), 25\n",
    "#plot_MS_all(df, r, rge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MS analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read, clean and split the df in 3 for H1_b, H1_s, H1_H4_b, H1_H4_s\n",
    "\n",
    "dfv = pd.read_pickle('dffv_EURUSD.pkl')\n",
    "df = clean_data(dfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculations\n",
    "df_idx_M15 = MS_index_M15(df)\n",
    "df_idx_H1  = MS_index_H1 (df)\n",
    "df_idx_H4  = MS_index_H4 (df)\n",
    "df_idx_D1  = MS_index_D1 (df)\n",
    "\n",
    "N_df_M15_T_Up = sum(df.loc[df_idx_M15,'Trend_M15'] ==  1)\n",
    "N_df_H1_T_Up  = sum(df.loc[df_idx_H1 ,'Trend_H1' ] ==  1)\n",
    "N_df_H4_T_Up  = sum(df.loc[df_idx_H4 ,'Trend_H4' ] ==  1)\n",
    "N_df_D1_T_Up  = sum(df.loc[df_idx_D1 ,'Trend_D1' ] ==  1)\n",
    "N_df_M15_T_Dw = sum(df.loc[df_idx_M15,'Trend_M15'] == -1)\n",
    "N_df_H1_T_Dw  = sum(df.loc[df_idx_H1 ,'Trend_H1' ] == -1)\n",
    "N_df_H4_T_Dw  = sum(df.loc[df_idx_H4 ,'Trend_H4' ] == -1)\n",
    "N_df_D1_T_Dw  = sum(df.loc[df_idx_D1 ,'Trend_D1' ] == -1)\n",
    "\n",
    "N_df_B_pts_M15_H4 = df[df['Labelb_M15_H4'] == 1]['Date_M15'].count()\n",
    "N_df_S_pts_M15_H4 = df[df['Labels_M15_H4'] == 1]['Date_M15'].count()\n",
    "N_df_B_pts_M15_H1 = df[df['Labelb_M15_H1'] == 1]['Date_M15'].count()\n",
    "N_df_S_pts_M15_H1 = df[df['Labels_M15_H1'] == 1]['Date_M15'].count()\n",
    "\n",
    "l0 = df['MS_range_M15'].describe().to_list()\n",
    "l0 = [round(x,1) for x in l0]\n",
    "l1 = df['MS_range_H1'].describe().to_list()\n",
    "l1 = [round(x,1) for x in l1]\n",
    "l2 = df['MS_range_H4'].describe().to_list()\n",
    "l2 = [round(x,1) for x in l2]\n",
    "l3 = df['MS_range_D1'].describe().to_list()\n",
    "l3 = [round(x,1) for x in l3]\n",
    "\n",
    "# Prints\n",
    "print('---- NUMBER OF BUYING AND SELLING POINTS for M15_H4 -----')\n",
    "print('df : No. Buying  points (No. 1´s in [Labelb_M15_H4])',N_df_B_pts_M15_H4,'(',100*round(N_df_B_pts_M15_H4/len(df),3),'%)')\n",
    "print('   : No. Selling points (No. 1´s in [Labels_M15_H4])',N_df_S_pts_M15_H4,'(',100*round(N_df_S_pts_M15_H4/len(df),3),'%)')\n",
    "\n",
    "print('---- NUMBER OF BUYING AND SELLING POINTS for M15_H1 -----')\n",
    "print('df : No. Buying  points (No. 1´s in [Labelb_M15_H1])',N_df_B_pts_M15_H1,'(',100*round(N_df_B_pts_M15_H1/len(df),3),'%)')\n",
    "print('   : No. Selling points (No. 1´s in [Labels_M15_H1])',N_df_S_pts_M15_H1,'(',100*round(N_df_S_pts_M15_H1/len(df),3),'%)')\n",
    "\n",
    "print('---- NUMBER OF Up & Dw TRENDS in MS STRUCTURES -----')\n",
    "print('dfs: No. M15 MS´s trending (Up,Dw): (',N_df_M15_T_Up,N_df_M15_T_Dw,') of',len(df_idx_M15),'H1 MS´s')\n",
    "print('dfs: No. H1  MS´s trending (Up,Dw): (',N_df_H1_T_Up,N_df_H1_T_Dw  ,') of',len(df_idx_H1),'H1 MS´s')\n",
    "print('dfs: No. H4  MS´s trending (Up,Dw): (',N_df_H4_T_Up,N_df_H4_T_Dw  ,') of',len(df_idx_H4),'H4 MS´s')\n",
    "print('dfs: No. D1  MS´s trending (Up,Dw): (',N_df_D1_T_Up,N_df_D1_T_Dw  ,') of',len(df_idx_D1),'D1 MS´s')\n",
    "\n",
    "print('---- NUMBER OF MS STRUCTURES AFFECTED BY CORRECTIONS -----')\n",
    "print(sum(df.loc[df_idx_H1,'MS_range_M15'] >= df['MS_range_M15'].max()),'MS´s of',len(df_idx_M15),'are affected from the MS function in M15')\n",
    "print(sum(df.loc[df_idx_H1,'MS_range_H1']  >= df['MS_range_H1'].max()) ,'MS´s of',len(df_idx_H1) ,'are affected from the MS function in H1')\n",
    "print(sum(df.loc[df_idx_H4,'MS_range_H4']  >= df['MS_range_H4'].max()) ,'MS´s of',len(df_idx_H4) ,'are affected from the MS function in H4')\n",
    "print(sum(df.loc[df_idx_D1,'MS_range_D1']  >= df['MS_range_D1'].max()) ,'MS´s of',len(df_idx_D1) ,'are affected from the MS function in D1')\n",
    "\n",
    "print('----- BUY - STOP LOSS LIMITATIONS M15_H4 -----')\n",
    "print('df :   ',df  [df  ['B_Stop_Loss_M15_H4'] <= df['B_Stop_Loss_M15_H4'].min()].count()['Date_M15'],'are limited to',df['B_Stop_Loss_M15_H4'].min(),'pips out of',len(df))\n",
    "print('df :   ',df  [df  ['B_Stop_Loss_M15_H4'] >= df['B_Stop_Loss_M15_H4'].max()].count()['Date_M15'],'are limited to',df['B_Stop_Loss_M15_H4'].max(),'pips out of',len(df))\n",
    "\n",
    "print('----- SELL - STOP LOSS LIMITATIONS M15_H4 -----')\n",
    "print('df :   ',df  [df  ['S_Stop_Loss_M15_H4'] <= df['S_Stop_Loss_M15_H4'].min()].count()['Date_M15'],'are limited to',df['S_Stop_Loss_M15_H4'].min(),'pips out of',len(df))\n",
    "print('df :   ',df  [df  ['S_Stop_Loss_M15_H4'] >= df['S_Stop_Loss_M15_H4'].max()].count()['Date_M15'],'are limited to',df['S_Stop_Loss_M15_H4'].max(),'pips out of',len(df))\n",
    "\n",
    "print('----- BUY - STOP LOSS LIMITATIONS M15_H1 -----')\n",
    "print('df :   ',df  [df  ['B_Stop_Loss_M15_H1'] <= df['B_Stop_Loss_M15_H1'].min()].count()['Date_M15'],'are limited to',df['B_Stop_Loss_M15_H1'].min(),'pips out of',len(df))\n",
    "print('df :   ',df  [df  ['B_Stop_Loss_M15_H1'] >= df['B_Stop_Loss_M15_H1'].max()].count()['Date_M15'],'are limited to',df['B_Stop_Loss_M15_H1'].max(),'pips out of',len(df))\n",
    "\n",
    "print('----- SELL - STOP LOSS LIMITATIONS M15_H1 -----')\n",
    "print('df :   ',df  [df  ['S_Stop_Loss_M15_H1'] <= df['S_Stop_Loss_M15_H1'].min()].count()['Date_M15'],'are limited to',df['S_Stop_Loss_M15_H1'].min(),'pips out of',len(df))\n",
    "print('df :   ',df  [df  ['S_Stop_Loss_M15_H1'] >= df['S_Stop_Loss_M15_H1'].max()].count()['Date_M15'],'are limited to',df['S_Stop_Loss_M15_H1'].max(),'pips out of',len(df),'\\n')\n",
    "\n",
    "print('----- MS Range Stats -----')\n",
    "print('MS_range_M15 mean:',l0[1],l0[3:])\n",
    "print('MS_range_H1  mean:',l1[1],l1[3:])\n",
    "print('MS_range_H4  mean:',l2[1],l2[3:])\n",
    "print('MS_range_D1  mean:',l3[1],l3[3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label M15_H4\n",
    "#### Full Manual Filtering Anaylisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df Exploration\n",
    "\n",
    "#                          min   25%    50%    75%    max\n",
    "#MS_range_D1  mean: 472.4 [39.4, 283.9, 407.9, 551.3, 2143.5]\n",
    "#MS_range_H4  mean: 205.7 [19.1, 120.9, 179.5, 260.7, 480.0]\n",
    "#MS_range_H1  mean: 111.9 [10.1, 68.8,  100.9, 146.3, 220.0]\n",
    "#MS_range_M15 mean: 59.3  [3.1, 35.5, 54.0, 79.4, 120.0]\n",
    "\n",
    "#Auto_Analysis(df, 100, 'H1_H4', 'print',        1, 1, 1, 1,-1, 1,           23.6, 78.6, 23.6, 78.6, 0, 99,           0,   700, 270, 10000, 0, 10000)\n",
    "#Auto_Analysis(df, 100, 'H1_H4', 'print', 1, -1, 1, 1, 1, 1,-1, 1, 0, 10000, 23.6, 78.6, 23.6, 78.6, 0, 99, 0, 10000, 0,   400, 150, 10000, 0, 10000)\n",
    "\n",
    "#dfr = pd.read_pickle('dfp_M15_H1_EURUSD.pkl')\n",
    "\n",
    "df3, df2, df1 = split3_dfs(df)\n",
    "\n",
    "Fib_listm1 = [0, 23.6, 38.2, 50, 61.8, 78.6, 90]\n",
    "Fib_list =   [0, 23.6, 38.2, 50, 61.8, 78.6, 90, 99]\n",
    "list_idx_1 = df1.index.to_list()\n",
    "list_idx_2 = df2.index.to_list()\n",
    "list_idx_3 = df3.index.to_list()\n",
    "\n",
    "# Buy Vbles\n",
    "\n",
    "a1b, a2b = 1, 1\n",
    "b1b, b2b = 1, 1\n",
    "c1b, c2b =     -1, 1\n",
    "d1b, d2b =     -1, 1\n",
    "\n",
    "# Sell Vbles\n",
    "\n",
    "a1s, a2s, b1s, b2s, c1s, c2s, d1s, d2s = -a1b, -a2b, -b1b, -b2b, -c1b, -c2b, -d1b, -d2b\n",
    "\n",
    "# Common Vbles\n",
    "\n",
    "r1, r2 = 23.6, 78.6 \n",
    "s1, s2 = 23.6, 78.6 \n",
    "t1, t2 = 0, 99\n",
    "u1, u2 = 0, 99\n",
    "\n",
    "n1, n2 = 0, 700\n",
    "o1, o2 = 270, 10000\n",
    "p1, p2 = 0, 10000 #80, 190\n",
    "q1, q2 = 0, 10000\n",
    "\n",
    "for i in range(0,100000):\n",
    "    \n",
    "    df1a = df1.loc[Space_dfa(df1,10),:]\n",
    "    df1b = df1.loc[Space_dfb(list_idx_1,int(len(df1)/10)),:]\n",
    "    df2a = df2.loc[Space_dfa(df2,10),:]\n",
    "    df2b = df2.loc[Space_dfb(list_idx_2,int(len(df2)/10)),:]\n",
    "    df3a = df3.loc[Space_dfa(df3,10),:]\n",
    "    df3b = df3.loc[Space_dfb(list_idx_3,int(len(df3)/10)),:]\n",
    "\n",
    "# Random vbles assignments\n",
    "\n",
    "#    a1b = random.choice([-1, 1,-1, 1, 1, 1])\n",
    "#    a1s = -a1b\n",
    "#    a2b = random.choice([-1, 1,-1, 1,-1, 1])\n",
    "#    a2s = -a2b\n",
    "    \n",
    "#    b1b = random.choice([-1, 1,-1, 1, 1, 1])\n",
    "#    b1s = -b1b\n",
    "#    b2b = random.choice([-1, 1,-1, 1,-1, 1])\n",
    "#    b2s = -b2b\n",
    "    \n",
    "#    c1b = random.choice([-1, 1,-1, 1, 1, 1])\n",
    "#    c1s = -c1b\n",
    "#    c2b = random.choice([-1, 1,-1, 1,-1, 1])\n",
    "#    c2s = -c2b\n",
    "    \n",
    "#    d1b = random.choice([-1, 1,-1, 1, 1, 1])\n",
    "#    d1s = -d1b\n",
    "#    d2b = random.choice([-1, 1,-1, 1,-1, 1])\n",
    "#    d2s = -d2b\n",
    "\n",
    "#    r1 = random.choice([38.2])\n",
    "#    r2 = random.choice([78.6, 90])#[i for i in Fib_list if i > r1])\n",
    "\n",
    "#    s1 = random.choice([38.2, 50]) #[-10000, 23.6])\n",
    "#    s2 = random.choice([61.8, 78.6, 90, 99])\n",
    "    \n",
    "#    t1 = random.choice([38.2, 50])\n",
    "#    t2 = random.choice([61.8, 78.6, 90])#[i for i in Fib_list if i > t1]) #[78.6, 90, 99, 10000])\n",
    "    \n",
    "#    u1 = random.choice([23.6, 38.2])\n",
    "#    u2 = random.choice([78.6, 90, 99])#[i for i in Fib_list if i > t1]) #[78.6, 90, 99, 10000])\n",
    "    \n",
    "#    n1 = random.randrange(1000,1700,300)\n",
    "#    n2 = random.randrange(700,1700,300)\n",
    "    \n",
    "#    o1 = random.choice([0,50, 100])\n",
    "#    o2 =  random.choice([300,350, 400, 450])\n",
    "    \n",
    "#    p1 = random.randrange(0,101,25)\n",
    "#    p2 = random.choice([219,220])\n",
    "\n",
    "#    q1 = random.randrange(0,101,25)\n",
    "#    q2 = random.choice([219,220])\n",
    "    \n",
    "    if (i % 1000) == 0: print('-->',i)\n",
    "    \n",
    "    def analysis_b(df):\n",
    "    \n",
    "        x = df[(df['MS_retracement_M15'].between(-1000000, 1000000)) & \n",
    "           ((df['Trend_D1']  == a1b) | (df['Trend_D1']  == a2b)) & \n",
    "           ((df['Trend_H4']  == b1b) | (df['Trend_H4']  == b2b)) &\n",
    "           ((df['Trend_H1']  == c1b) | (df['Trend_H1']  == c2b)) &\n",
    "           ((df['Trend_M15'] == d1b) | (df['Trend_M15'] == d2b)) &\n",
    "           #((df['Closing_D4'] == e1b) | (df['Closing_D1'] == e2b)) &\n",
    "           #((df['Closing_H4'] == f1b) | (df['Closing_H4'] == f2b)) &\n",
    "           #((df['Closing_H1'] == g1b) | (df['Closing_H1'] == g2b)) &\n",
    "           #((df['Closing_M15'] == h1b) | (df['Closing_M15'] == h2b)) &\n",
    "           (df['MS_retracement_D1'] .between(r1, r2)) &\n",
    "           (df['MS_retracement_H4'] .between(s1, s2)) & \n",
    "           (df['MS_retracement_H1'] .between(t1, t2)) &\n",
    "           (df['MS_retracement_M15'].between(u1, u2)) &\n",
    "           #(df['N_Breaks_D1'] .between(k1, k2)) &\n",
    "           #(df['N_Breaks_H4'] .between(l1, l2)) & \n",
    "           #(df['N_Breaks_H1'] .between(m1, m2)) &\n",
    "           (df['MS_range_D1'] .between(n1, n2)) & \n",
    "           (df['MS_range_H4'] .between(o1, o2)) & \n",
    "           (df['MS_range_H1'] .between(p1, p2)) &\n",
    "           (df['MS_range_M15'].between(q1, q2)) &\n",
    "           (df['MS_retracement_M15'].between(-1000000, 1000000))]['Labelb_M15_H4'].describe()[:2]\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def analysis_s(df):\n",
    "    \n",
    "        y = df[(df['MS_retracement_M15'].between(-1000000, 1000000)) & \n",
    "           ((df['Trend_D1']  == a1s) | (df['Trend_D1']  == a2s)) & \n",
    "           ((df['Trend_H4']  == b1s) | (df['Trend_H4']  == b2s)) &\n",
    "           ((df['Trend_H1']  == c1s) | (df['Trend_H1']  == c2s)) &\n",
    "           ((df['Trend_M15'] == d1s) | (df['Trend_M15'] == d2s)) &\n",
    "           #((df['Closing_D4'] == e1s) | (df['Closing_D1'] == e2s)) &\n",
    "           #((df['Closing_H4'] == f1s) | (df['Closing_H4'] == f2s)) &\n",
    "           #((df['Closing_H1'] == g1s) | (df['Closing_H1'] == g2s)) &\n",
    "           #((df['Closing_H1'] == h1s) | (df['Closing_H1'] == h2s)) &\n",
    "           (df['MS_retracement_D1'] .between(r1, r2)) &\n",
    "           (df['MS_retracement_H4'] .between(s1, s2)) & \n",
    "           (df['MS_retracement_H1'] .between(t1, t2)) &\n",
    "           (df['MS_retracement_M15'].between(u1, u2)) &\n",
    "           #(df['N_Breaks_D1'] .between(k1, k2)) &\n",
    "           #(df['N_Breaks_H4'] .between(l1, l2)) & \n",
    "           #(df['N_Breaks_H1'] .between(m1, m2)) &\n",
    "           (df['MS_range_D1'] .between(n1, n2)) & \n",
    "           (df['MS_range_H4'] .between(o1, o2)) & \n",
    "           (df['MS_range_H1'] .between(p1, p2)) &\n",
    "           (df['MS_range_M15'].between(q1, q2)) &\n",
    "           (df['MS_retracement_M15'].between(-1000000, 1000000))]['Labels_M15_H4'].describe()[:2]\n",
    "\n",
    "        return y\n",
    "    \n",
    "    xa1 = analysis_b(df1a)\n",
    "    xb1 = analysis_b(df1b)\n",
    "    xa2 = analysis_b(df2a)\n",
    "    xb2 = analysis_b(df2b)\n",
    "    xa3 = analysis_b(df3a)\n",
    "    xb3 = analysis_b(df3b)\n",
    "    \n",
    "    ya1 = analysis_s(df1a)\n",
    "    yb1 = analysis_s(df1b)\n",
    "    ya2 = analysis_s(df2a)\n",
    "    yb2 = analysis_s(df2b)\n",
    "    ya3 = analysis_s(df3a)\n",
    "    yb3 = analysis_s(df3b)\n",
    "    \n",
    "    par_a = 0.3\n",
    "    par_b = 40\n",
    "    par_c = 0.25\n",
    "    par_d = 20\n",
    "    \n",
    "    count_b = (xa1[0]+xb1[0]+xa2[0]+xb2[0]+xa3[0]+xb3[0])/6\n",
    "    #count_b = (xb1[0]+xb2[0]+xb3[0])/3\n",
    "    mean_b  = (xa1[0]*xa1[1]+xb1[0]*xb1[1]+xa2[0]*xa2[1]+xb2[0]*xb2[1]+xa3[0]*xa3[1]+xb3[0]*xb3[1])/(6*count_b)\n",
    "    #mean_b  = (xb1[0]*xb1[1]+xb2[0]*xb2[1]+xb3[0]*xb3[1])/(3*count_b)\n",
    "    \n",
    "    count_s = (ya1[0]+yb1[0]+ya2[0]+yb2[0]+ya3[0]+yb3[0])/6\n",
    "    #count_s = (yb1[0]+yb2[0]+yb3[0])/3\n",
    "    mean_s  = (ya1[0]*ya1[1]+yb1[0]*yb1[1]+ya2[0]*ya2[1]+yb2[0]*yb2[1]+ya3[0]*ya3[1]+yb3[0]*yb3[1])/(6*count_s) \n",
    "    #mean_s  = (yb1[0]*yb1[1]+yb2[0]*yb2[1]+yb3[0]*yb3[1])/(3*count_s)\n",
    "    \n",
    "    if ((mean_b > par_a) & (mean_s > par_a) & (count_b > par_b) & (count_s > par_b) & (xa1[1] > par_c) & (xa1[0] > par_d) & (xb1[1] > par_c) & (xb1[0] > par_d) & (xa2[1] > par_c) & (xa2[0] > par_d) & (xb2[1] > par_c) & (xb2[0] > par_d) & (xa3[1] > par_c) & (xa3[0] > par_d) & (xb3[1] > par_c) & (xb3[0] > par_d) & (ya1[1] > par_c) & (ya1[0] > par_d) & (yb1[1] > par_c) & (yb1[0] > par_d) & (ya2[1] > par_c) & (ya2[0] > par_d) & (yb2[1] > par_c) & (yb2[0] > par_d) & (ya3[1] > par_c) & (ya3[0] > par_d) & (yb3[1] > par_c) & (yb3[0] > par_d)):\n",
    "    #if ((mean_b > par_a) & (mean_s > par_a) & (count_b > par_b) & (count_s > par_b) & (xb1[1] > par_c) & (xb1[0] > par_d) & (xb2[1] > par_c) & (xb2[0] > par_d) & (xb3[1] > par_c) & (xb3[0] > par_d) & (yb1[1] > par_c) & (yb1[0] > par_d) & (yb2[1] > par_c) & (yb2[0] > par_d) & (yb3[1] > par_c) & (yb3[0] > par_d)):\n",
    "        \n",
    "        print(i)\n",
    "        print('TOTAL SCORE BUY  --- MEAN:',mean_b,' --- COUNT:',count_b)\n",
    "        print('TOTAL SCORE SELL --- MEAN:',mean_s,' --- COUNT:',count_s)\n",
    "        print('BUY VBLES   :','(',a1b,a2b,')(',b1b,b2b,')(',c1b,c2b,')(',d1b,d2b,')')\n",
    "        print('SELL VBLES  :','(',a1s,a2s,')(',b1s,b2s,')(',c1s,c2s,')(',d1s,d2s,')')\n",
    "        print('COMMON VBLES:','(',r1,r2,')(',s1,s2,')(',t1,t2,')(',u1,u2,')n(',n1,n2,')o(',o1,o2,')p(',p1,p2,')q(',q1,q2,')')\n",
    "        print('BUY DFRAMES:')\n",
    "        print('1a',i, xa1[0],xa1[1])\n",
    "        print('1b',i, xb1[0],xb1[1])\n",
    "        print('2a',i, xa2[0],xa2[1])\n",
    "        print('2b',i, xb2[0],xb2[1])\n",
    "        print('3a',i, xa3[0],xa3[1])\n",
    "        print('3b',i, xb3[0],xb3[1])\n",
    "        print('SELL DFRAMES:')\n",
    "        print('1a',i, ya1[0],ya1[1])\n",
    "        print('1b',i, yb1[0],yb1[1])\n",
    "        print('2a',i, ya2[0],ya2[1])\n",
    "        print('2b',i, yb2[0],yb2[1])\n",
    "        print('3a',i, ya3[0],ya3[1])\n",
    "        print('3b',i, yb3[0],yb3[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions M15_H4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H4', 'print', 1, 1, 1, 1, -1, 1, -1, 1,0, 78.6, 23.6, 78.6, 0, 90, 0, 99, 0, 1200, 280, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H4', 'ret', 1, 1, 1, 1, -1, 1, -1, 1,0, 78.6, 23.6, 78.6, 0, 90, 0, 99, 0, 1200, 280, 10000, 0, 10000, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H4', 'print', 1, 1, 1, 1, -1, 1, -1, 1, 23.6, 78.6, 23.6, 78.6, 0, 99, 0, 99, 0, 700, 270, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H4', 'ret', 1, 1, 1, 1, -1, 1, -1, 1, 23.6, 78.6, 23.6, 78.6, 0, 99, 0, 99, 0, 700, 270, 10000, 0, 10000, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H4', 'print', 1, 1, 1, 1, -1, 1, -1, 1, 23.6, 90, 23.6, 78.6, 0, 90, 0, 99, 0, 700, 270, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H4', 'ret', 1, 1, 1, 1, -1, 1, -1, 1, 23.6, 90, 23.6, 78.6, 0, 90, 0, 99, 0, 700, 270, 10000, 0, 10000, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H4', 'print', 1, 1, 1, 1, -1, 1, -1, 1, 23.6, 90, 23.6, 90, 23.6, 99, 0, 99, 0, 700, 270, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H4', 'ret', 1, 1, 1, 1, -1, 1, -1, 1, 23.6, 90, 23.6, 90, 23.6, 99, 0, 99, 0, 700, 270, 10000, 0, 10000, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H4', 'print', 1, 1, 1, 1, -1, 1, -1, 1, 23.6, 78.6, 0, 78.6, 0, 99, 0, 99, 0, 1400, 280, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H4', 'ret', 1, 1, 1, 1, -1, 1, -1, 1, 23.6, 78.6, 0, 78.6, 0, 99, 0, 99, 0, 1400, 280, 10000, 0, 10000, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H4', 'print', 1, 1, 1, 1, -1, 1, -1, 1, 23.6, 90, 0, 78.6, 0, 90, 0, 99, 0, 800, 280, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H4', 'ret', 1, 1, 1, 1, -1, 1, -1, 1, 23.6, 90, 0, 78.6, 0, 90, 0, 99, 0, 800, 280, 10000, 0, 10000, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label_M15_H1\n",
    "#### Full Manual Filtering Anaylisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfv = pd.read_pickle('dffv_EURUSD.pkl')\n",
    "df = clean_data(dfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df Exploration\n",
    "\n",
    "#                          min   25%    50%    75%    max\n",
    "#MS_range_D1  mean: 472.4 [39.4, 283.9, 407.9, 551.3, 2143.5]\n",
    "#MS_range_H4  mean: 205.7 [19.1, 120.9, 179.5, 260.7, 480.0]\n",
    "#MS_range_H1  mean: 111.9 [10.1, 68.8,  100.9, 146.3, 220.0]\n",
    "#MS_range_M15 mean: 59.3  [3.1, 35.5, 54.0, 79.4, 120.0]\n",
    "\n",
    "#Auto_Analysis(df, 100, 'H1_H4', 'print',        1, 1, 1, 1,-1, 1,           23.6, 78.6, 23.6, 78.6, 0, 99,           0,   700, 270, 10000, 0, 10000)\n",
    "#Auto_Analysis(df, 100, 'H1_H4', 'print', 1, -1, 1, 1, 1, 1,-1, 1, 0, 10000, 23.6, 78.6, 23.6, 78.6, 0, 99, 0, 10000, 0,   400, 150, 10000, 0, 10000)\n",
    "\n",
    "#dfr = pd.read_pickle('dfp_M15_H1_EURUSD.pkl')\n",
    "\n",
    "df3, df2, df1 = split3_dfs(df)\n",
    "\n",
    "Fib_listm1 = [0, 23.6, 38.2, 50, 61.8]\n",
    "Fib_list =   [0, 23.6, 38.2, 50, 61.8, 78.6, 90, 99]\n",
    "list_idx_1 = df1.index.to_list()\n",
    "list_idx_2 = df2.index.to_list()\n",
    "list_idx_3 = df3.index.to_list()\n",
    "\n",
    "# Buy Vbles\n",
    "\n",
    "a1b, a2b =     -1, 1\n",
    "b1b, b2b =     -1, 1\n",
    "c1b, c2b =     1, 1\n",
    "d1b, d2b =     -1, 1\n",
    "\n",
    "# Sell Vbles\n",
    "\n",
    "a1s, a2s, b1s, b2s, c1s, c2s, d1s, d2s = -a1b, -a2b, -b1b, -b2b, -c1b, -c2b, -d1b, -d2b\n",
    "\n",
    "# Common Vbles\n",
    "\n",
    "r1, r2 = 0, 99\n",
    "s1, s2 = 0, 99\n",
    "t1, t2 = 0, 99\n",
    "u1, u2 = 0, 99\n",
    "\n",
    "n1, n2 = 0, 10000#700\n",
    "o1, o2 = 0, 10000\n",
    "p1, p2 = 0, 10000\n",
    "q1, q2 = 0, 10000\n",
    "\n",
    "for i in range(0,1000000):\n",
    "    \n",
    "    df1a = df1.loc[Space_dfa(df1,10),:]\n",
    "    df1b = df1.loc[Space_dfb(list_idx_1,int(len(df1)/10)),:]\n",
    "    df2a = df2.loc[Space_dfa(df2,10),:]\n",
    "    df2b = df2.loc[Space_dfb(list_idx_2,int(len(df2)/10)),:]\n",
    "    df3a = df3.loc[Space_dfa(df3,10),:]\n",
    "    df3b = df3.loc[Space_dfb(list_idx_3,int(len(df3)/10)),:]\n",
    "\n",
    "# Random vbles assignments\n",
    "#-1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 78.6,  38.6, 61.8, 0, 10000, 0, 400, 0, 180, 0, 10000)\n",
    "#    a1b = random.choice([-1, 1,-1, 1, 1, 1])\n",
    "#    a1s = -a1b\n",
    "#    a2b = random.choice([-1, 1,-1, 1,-1, 1])\n",
    "#    a2s = -a2b\n",
    "    \n",
    "#    b1b = random.choice([-1, 1,-1, 1, 1, 1])\n",
    "#    b1s = -b1b\n",
    "#    b2b = random.choice([-1, 1,-1, 1,-1, 1])\n",
    "#    b2s = -b2b\n",
    "    \n",
    "#    c1b = random.choice([-1, 1,-1, 1, 1, 1])\n",
    "#    c1s = -c1b\n",
    "#    c2b = random.choice([-1, 1,-1, 1,-1, 1])\n",
    "#    c2s = -c2b\n",
    "    \n",
    "#    d1b = random.choice([-1, 1,-1, 1, 1, 1])\n",
    "#    d1s = -d1b\n",
    "#    d2b = random.choice([-1, 1,-1, 1,-1, 1])\n",
    "#    d2s = -d2b\n",
    "\n",
    "    r1 = random.choice([23.6, 38.6])\n",
    "    r2 = random.choice([50, 61.8])\n",
    "\n",
    "    s1 = random.choice([23.6, 38.6])\n",
    "    s2 = random.choice([50, 61.8])#([50, 61.8, 78.6, 90, 99])\n",
    "    \n",
    "    t1 = random.choice([23.6, 38.6])\n",
    "    t2 = random.choice([50, 61.8])\n",
    "    \n",
    "    u1 = random.choice([23.6, 38.6])#([0, 23.6, 38.6, 50])\n",
    "    u2 = random.choice([50, 61.8])\n",
    "\n",
    "#    -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 78.6,  38.6, 61.8, 0, 10000, 0, 400, 0, 180, 0, 10000)\n",
    "#    n1 = random.randrange(700,3000,300)\n",
    "#    n2 = random.randrange(1000,3000,250)\n",
    "    \n",
    "#    o1 = random.randrange(0,51,15)\n",
    "#    o2 = random.randrange(350,501,25)\n",
    "    \n",
    "#    p1 = random.randrange(0,21,5)\n",
    "#    p2 = random.randrange(170,221,10)\n",
    "\n",
    "#    q1 = random.randrange(0,21,5)\n",
    "#    q2 = random.randrange(70,110,5)\n",
    "    \n",
    "    if (i % 1000) == 0: print('-->',i)\n",
    "    \n",
    "    def analysis_b(df):\n",
    "    \n",
    "        x = df[(df['MS_retracement_M15'].between(-1000000, 1000000)) & \n",
    "           ((df['Trend_D1']  == a1b) | (df['Trend_D1']  == a2b)) & \n",
    "           ((df['Trend_H4']  == b1b) | (df['Trend_H4']  == b2b)) &\n",
    "           ((df['Trend_H1']  == c1b) | (df['Trend_H1']  == c2b)) &\n",
    "           ((df['Trend_M15'] == d1b) | (df['Trend_M15'] == d2b)) &\n",
    "           #((df['Closing_D4']  == e1b)  | (df['Closing_D1'] == e2b)) &\n",
    "           #((df['Closing_H4']  == f1b)  | (df['Closing_H4'] == f2b)) &\n",
    "           #((df['Closing_H1']  == g1b)  | (df['Closing_H1'] == g2b)) &\n",
    "           #((df['Closing_M15'] == h1b) | (df['Closing_M15'] == h2b)) &\n",
    "           (df['MS_retracement_D1'] .between(r1, r2)) &\n",
    "           (df['MS_retracement_H4'] .between(s1, s2)) & \n",
    "           (df['MS_retracement_H1'] .between(t1, t2)) &\n",
    "           (df['MS_retracement_M15'].between(u1, u2)) &\n",
    "           (df['MS_range_D1'] .between(n1, n2)) & \n",
    "           (df['MS_range_H4'] .between(o1, o2)) & \n",
    "           (df['MS_range_H1'] .between(p1, p2)) &\n",
    "           (df['MS_range_M15'].between(q1, q2)) &\n",
    "           (df['MS_retracement_M15'].between(-1000000, 1000000))]['Labelb_M15_H1'].describe()[:2]\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def analysis_s(df):\n",
    "    \n",
    "        y = df[(df['MS_retracement_M15'].between(-1000000, 1000000)) & \n",
    "           ((df['Trend_D1']  == a1s) | (df['Trend_D1']  == a2s)) & \n",
    "           ((df['Trend_H4']  == b1s) | (df['Trend_H4']  == b2s)) &\n",
    "           ((df['Trend_H1']  == c1s) | (df['Trend_H1']  == c2s)) &\n",
    "           ((df['Trend_M15'] == d1s) | (df['Trend_M15'] == d2s)) &\n",
    "           #((df['Closing_D4']  == e1b)  | (df['Closing_D1'] == e2b)) &\n",
    "           #((df['Closing_H4']  == f1b)  | (df['Closing_H4'] == f2b)) &\n",
    "           #((df['Closing_H1']  == g1b)  | (df['Closing_H1'] == g2b)) &\n",
    "           #((df['Closing_M15'] == h1b) | (df['Closing_M15'] == h2b)) &\n",
    "           (df['MS_retracement_D1'] .between(r1, r2)) &\n",
    "           (df['MS_retracement_H4'] .between(s1, s2)) & \n",
    "           (df['MS_retracement_H1'] .between(t1, t2)) &\n",
    "           (df['MS_retracement_M15'].between(u1, u2)) &\n",
    "           (df['MS_range_D1'] .between(n1, n2)) & \n",
    "           (df['MS_range_H4'] .between(o1, o2)) & \n",
    "           (df['MS_range_H1'] .between(p1, p2)) &\n",
    "           (df['MS_range_M15'].between(q1, q2)) &\n",
    "           (df['MS_retracement_M15'].between(-1000000, 1000000))]['Labels_M15_H1'].describe()[:2]\n",
    "\n",
    "        return y\n",
    "    \n",
    "    xa1 = analysis_b(df1a)\n",
    "    xb1 = analysis_b(df1b)\n",
    "    xa2 = analysis_b(df2a)\n",
    "    xb2 = analysis_b(df2b)\n",
    "    xa3 = analysis_b(df3a)\n",
    "    xb3 = analysis_b(df3b)\n",
    "    \n",
    "    ya1 = analysis_s(df1a)\n",
    "    yb1 = analysis_s(df1b)\n",
    "    ya2 = analysis_s(df2a)\n",
    "    yb2 = analysis_s(df2b)\n",
    "    ya3 = analysis_s(df3a)\n",
    "    yb3 = analysis_s(df3b)\n",
    "    \n",
    "    par_a = 0.30\n",
    "    par_b = 1\n",
    "    par_c = 0.27\n",
    "    par_d = 1\n",
    "    \n",
    "    count_b = (xa1[0]+xb1[0]+xa2[0]+xb2[0]+xa3[0]+xb3[0])/6\n",
    "    count_b = (xb1[0]+xb2[0]+xb3[0])/3\n",
    "    mean_b  = (xa1[0]*xa1[1]+xb1[0]*xb1[1]+xa2[0]*xa2[1]+xb2[0]*xb2[1]+xa3[0]*xa3[1]+xb3[0]*xb3[1])/(6*count_b)\n",
    "    mean_b  = (xb1[0]*xb1[1]+xb2[0]*xb2[1]+xb3[0]*xb3[1])/(3*count_b)\n",
    "    \n",
    "    count_s = (ya1[0]+yb1[0]+ya2[0]+yb2[0]+ya3[0]+yb3[0])/6\n",
    "    count_s = (yb1[0]+yb2[0]+yb3[0])/3\n",
    "    mean_s  = (ya1[0]*ya1[1]+yb1[0]*yb1[1]+ya2[0]*ya2[1]+yb2[0]*yb2[1]+ya3[0]*ya3[1]+yb3[0]*yb3[1])/(6*count_s) \n",
    "    mean_s  = (yb1[0]*yb1[1]+yb2[0]*yb2[1]+yb3[0]*yb3[1])/(3*count_s)\n",
    "    \n",
    "    if ((mean_b > par_a) & (mean_s > par_a) & (count_b > par_b) & (count_s > par_b) & (xa1[1] > par_c) & (xa1[0] > par_d) & (xb1[1] > par_c) & (xb1[0] > par_d) & (xa2[1] > par_c) & (xa2[0] > par_d) & (xb2[1] > par_c) & (xb2[0] > par_d) & (xa3[1] > par_c) & (xa3[0] > par_d) & (xb3[1] > par_c) & (xb3[0] > par_d) & (ya1[1] > par_c) & (ya1[0] > par_d) & (yb1[1] > par_c) & (yb1[0] > par_d) & (ya2[1] > par_c) & (ya2[0] > par_d) & (yb2[1] > par_c) & (yb2[0] > par_d) & (ya3[1] > par_c) & (ya3[0] > par_d) & (yb3[1] > par_c) & (yb3[0] > par_d)):\n",
    "    #if ((mean_b > par_a) & (mean_s > par_a) & (count_b > par_b) & (count_s > par_b) & (xb1[1] > par_c) & (xb1[0] > par_d) & (xb2[1] > par_c) & (xb2[0] > par_d) & (xb3[1] > par_c) & (xb3[0] > par_d) & (yb1[1] > par_c) & (yb1[0] > par_d) & (yb2[1] > par_c) & (yb2[0] > par_d) & (yb3[1] > par_c) & (yb3[0] > par_d)):\n",
    "        \n",
    "        print(i)\n",
    "        print('TOTAL SCORE BUY  --- MEAN:',mean_b,' --- COUNT:',count_b)\n",
    "        print('TOTAL SCORE SELL --- MEAN:',mean_s,' --- COUNT:',count_s)\n",
    "        print('BUY VBLES   :','(',a1b,a2b,')(',b1b,b2b,')(',c1b,c2b,')(',d1b,d2b,')(',h1b,h2b,')')\n",
    "        print('SELL VBLES  :','(',a1s,a2s,')(',b1s,b2s,')(',c1s,c2s,')(',d1s,d2s,')')\n",
    "        print('COMMON VBLES:','(',r1,r2,')(',s1,s2,')(',t1,t2,')(',u1,u2,')n(',n1,n2,')o(',o1,o2,')p(',p1,p2,')q(',q1,q2,')')\n",
    "        print('BUY DFRAMES:')\n",
    "        print('1a',i, xa1[0],xa1[1])\n",
    "        print('1b',i, xb1[0],xb1[1])\n",
    "        print('2a',i, xa2[0],xa2[1])\n",
    "        print('2b',i, xb2[0],xb2[1])\n",
    "        print('3a',i, xa3[0],xa3[1])\n",
    "        print('3b',i, xb3[0],xb3[1])\n",
    "        print('SELL DFRAMES:')\n",
    "        print('1a',i, ya1[0],ya1[1])\n",
    "        print('1b',i, yb1[0],yb1[1])\n",
    "        print('2a',i, ya2[0],ya2[1])\n",
    "        print('2b',i, yb2[0],yb2[1])\n",
    "        print('3a',i, ya3[0],ya3[1])\n",
    "        print('3b',i, yb3[0],yb3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BRUTAL - DOES NOT WORK\n",
    "Auto_Analysis(df, 50, 'M15_H1', 'print',            -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 61.8, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "#df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, -1, 1, -1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "#print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BRUTAL 2 - DOES NOT WORK\n",
    "Auto_Analysis(df, 50, 'M15_H1', 'print',            -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 61.8,  38.6, 61.8, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "#df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, -1, 1, -1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "#print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BRUTAL 3 - RCF WORKS ON BUY - NOT ON SELL df2\n",
    "Auto_Analysis(df, 50, 'M15_H1', 'print',            -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 78.6,  38.6, 61.8, 0, 10000, 0, 400, 0, 180, 0, 10000)\n",
    "#df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, -1, 1, -1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "#print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RCF WORKS ON BUY - ALMOST ON SELL df2\n",
    "Auto_Analysis(df, 50, 'M15_H1', 'print', -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 78.6,  38.6, 61.8, 0, 1900, 0, 400, 0, 180, 0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 50, 'M15_H1', 'print', -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 78.6,  38.6, 61.8, 0, 2200, 0, 400, 0, 180, 0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 50, 'M15_H1', 'print', -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 78.6,  38.6, 61.8, 0, 1700, 0, 400, 0, 180, 0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 50, 'M15_H1', 'print', -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 78.6,  38.6, 61.8, 0, 1900, 0, 380, 0, 180, 0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 50, 'M15_H1', 'print', -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 78.6,  38.6, 61.8, 0, 1900, 0, 420, 0, 180, 0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RCF NOT WORKING\n",
    "Auto_Analysis(df, 50, 'M15_H1', 'print', -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 78.6,  38.6, 61.8, 0, 1900, 0, 400, 0, 160, 0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Auto_Analysis(df, 50, 'M15_H1', 'print', -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 78.6,  38.6, 61.8, 0, 1900, 0, 400, 0, 200, 0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 50, 'M15_H1', 'print', -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 78.6,  38.6, 61.8, 0, 1900, 0, 400, 10, 180, 0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 50, 'M15_H1', 'print', -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 78.6,  38.6, 61.8, 0, 1900, 20, 400, 0, 180, 0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 50, 'M15_H1', 'print',            -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 78.6, 0, 78.6,  38.6, 61.8, 0, 1900, 0, 400, 0, 180, 0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOES NOT WORK\n",
    "Auto_Analysis(df, 50, 'M15_H1', 'print',            -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 90,  38.6, 61.8, 0, 1900, 0, 400, 0, 180, 0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOES NOT WORK\n",
    "Auto_Analysis(df, 50, 'M15_H1', 'print',            -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 78.6,  23.6, 61.8, 0, 1900, 0, 400, 0, 180, 0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOES NOT WORK\n",
    "Auto_Analysis(df, 50, 'M15_H1', 'print',            -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 78.6,  38.6, 78.6, 0, 1900, 0, 400, 0, 180, 0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOES NOT WORK\n",
    "Auto_Analysis(df, 50, 'M15_H1', 'print',            -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 70, 0, 78.6,  38.6, 61.8, 0, 10000, 0, 400, 0, 180, 0, 10000)\n",
    "#df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, -1, 1, -1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "#print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brutal\n",
    "Auto_Analysis(df, 50, 'M15_H1', 'print',            -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 70, 0, 78.6,  38.6, 61.8, 0, 10000, 0, 400, 0, 180, 0, 10000)\n",
    "#df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, -1, 1, -1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "#print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brutal\n",
    "Auto_Analysis(df, 50, 'M15_H1', 'print',            -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 78.6, 0, 78.6,  38.6, 61.8, 0, 10000, 0, 400, 0, 180, 0, 10000)\n",
    "#df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, -1, 1, -1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "#print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BRUTAL 4 # DOES NOT WORK\n",
    "Auto_Analysis(df, 50, 'M15_H1', 'print',            -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 90,  38.6, 61.8, 0, 10000, 0, 400, 0, 180, 0, 10000)\n",
    "#df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, -1, 1, -1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "#print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOES NOT WORK\n",
    "Auto_Analysis(df, 50, 'M15_H1', 'print',            -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 78.6,  23.6, 61.8, 0, 10000, 0, 400, 0, 180, 0, 10000)\n",
    "#df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, -1, 1, -1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "#print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 78.6,  38.6, 61.8, 0, 10000, 0, 400, 0, 180, 0, 10000)\n",
    "\n",
    "-1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 78.6, 0, 78.6,  38.6, 61.8, 0, 10000, 0, 400, 0, 180, 0, 10000)\n",
    "-1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 90,  38.6, 61.8, 0, 10000, 0, 400, 0, 180, 0, 10000)\n",
    "-1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 78.6,  23.6, 61.8, 0, 10000, 0, 400, 0, 180, 0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 50, 'M15_H1', 'print', -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 78.6, 0, 90,  38.6, 61.8, 0, 10000, 0, 400, 0, 180, 0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFC DIDN T WORK\n",
    "Auto_Analysis(df, 50, 'M15_H1', 'print', -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 90,  23.6, 61.8, 0, 10000, 0, 400, 0, 180, 0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, -1, 1, -1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, -1, 1, -1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL SCORE BUY  --- MEAN: 0.34763948497854075  --- COUNT: 77.66666666666667\n",
    "TOTAL SCORE SELL --- MEAN: 0.3562231759656652  --- COUNT: 77.66666666666667\n",
    "BUY VBLES   : ( -1 1 )( -1 1 )( -1 1 )( -1 1 )( -1 1 )\n",
    "SELL VBLES  : ( 1 -1 )( 1 -1 )( 1 -1 )( 1 -1 )\n",
    "COMMON VBLES: ( 38.6 50 )( 23.6 61.8 )( 23.6 61.8 )( 38.6 50 )n( 0 10000 )o( 0 10000 )p( 0 10000 )q( 0 10000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, -1, 1, -1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, -1, 1, -1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, -1, 1, -1, 1, 1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, -1, 1, -1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, -1, 1, -1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, 1, 1, -1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, -1, 1, -1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            1, 1, -1, 1, -1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, -1, 1, -1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, -1, 1, -1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, -1, 1, -1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 23.6, 61.8,  38.6, 50, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions M15_H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, 1, 1, 1, 1, 1, 1, 23.6, 90, 0, 99, 38.6, 61.8,  38.6, 61.8, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 23.6, 90, 0, 99, 38.6, 61.8,  38.6, 61.8, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 50, 38.6, 61.8,  23.6, 61.8, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 50, 38.6, 61.8,  23.6, 61.8, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOOOD\n",
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 50, 38.6, 61.8,  38.6, 61.8, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 50, 38.6, 61.8,  38.6, 61.8, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 61.8, 38.6, 61.8,  0, 61.8, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 61.8, 38.6, 61.8,  0, 61.8, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',             -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 61.8, 38.6, 61.8,  23.6, 50, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 100, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 61.8, 38.6, 61.8,  23.6, 50, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',             -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 61.8, 38.6, 61.8,  23.6, 61.8, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 100, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 61.8, 38.6, 61.8,  23.6, 61.8, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 61.8, 38.6, 61.8,  38.6, 61.8, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 61.8, 38.6, 61.8,  38.6, 61.8, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, -1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 61.8, 38.6, 61.8,  38.6, 61.8, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 61.8, 38.6, 61.8,  38.6, 61.8, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, 1, 1, -1, 1, 1, 1, 38.6, 61.8, 0, 61.8, 38.6, 61.8,  38.6, 61.8, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 61.8, 38.6, 61.8,  38.6, 61.8, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, 1, 1, 1, 1, -1, 1, 38.6, 61.8, 0, 61.8, 38.6, 61.8,  38.6, 61.8, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 61.8, 38.6, 61.8,  38.6, 61.8, 0, 1900, 0, 400, 0, 180, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',             -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 61.8, 38.6, 61.8,  38.6, 61.8, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 100, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 61.8, 38.6, 61.8,  38.6, 61.8, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',             -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 78.6, 38.6, 61.8,  0, 61.8, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 100, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 78.6, 38.6, 61.8,  0, 61.8, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOOOD ++++ # DOES NOT WORK\n",
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 78.6, 38.6, 61.8,  23.6, 61.8, 0, 2100, 0, 460, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 78.6, 38.6, 61.8,  23.6, 61.8, 0, 2100, 0, 460, 0, 10000, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, 1, 1, -1, 1, 1, 1, 38.6, 61.8, 0, 78.6, 50, 61.8,  23.6, 61.8, 0, 2100, 0, 460, 0, 10000, 0, 10000)\n",
    "#df_b, df_s = Auto_Analysis(df, 30, 'M15_H1', 'ret', -1, 1, 1, 1, -1, 1, 1, 1, 38.6, 61.8, 0, 78.6, 50, 61.8,  23.6, 61.8, 0, 2100, 0, 460, 0, 10000, 0, 10000)\n",
    "#print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',             -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 78.6, 38.6, 61.8,  23.6, 61.8, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "#df_b, df_s = Auto_Analysis(df, 100, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 78.6, 38.6, 61.8,  23.6, 61.8, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "#print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',             -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 78.6, 38.6, 61.8,  38.6, 61.8, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "#df_b, df_s = Auto_Analysis(df, 100, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 78.6, 38.6, 61.8,  38.6, 61.8, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "#print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',             -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 90, 38.6, 61.8,  38.6, 61.8, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "#df_b, df_s = Auto_Analysis(df, 100, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 90, 38.6, 61.8,  38.6, 61.8, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "#print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOD\n",
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 99, 38.6, 61.8,  23.6, 61.8, 0, 1900, 0, 450, 0, 10000, 0, 10000)\n",
    "#df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 99, 38.6, 61.8,  23.6, 61.8, 0, 1900, 0, 450, 0, 10000, 0, 10000)\n",
    "#print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 99, 38.6, 61.8,  0, 61.8, 0, 2500, 50, 350, 25, 220, 15, 119)\n",
    "#df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 99, 38.6, 61.8,  0, 61.8, 0, 2500, 50, 350, 25, 220, 15, 119)\n",
    "#print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 99, 38.6, 61.8,  0, 78.6, 0, 2800, 0, 350, 50, 220, 20, 120)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 99, 38.6, 61.8,  0, 78.6, 0, 2800, 0, 350, 50, 220, 20, 120)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 99, 38.6, 61.8,  23.6, 99, 0, 1600, 50, 450, 0, 210, 20, 119)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 99, 38.6, 61.8,  23.6, 99, 0, 1600, 50, 450, 0, 210, 20, 119)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 99, 38.6, 61.8,  38.6, 61.8, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 38.6, 61.8, 0, 99, 38.6, 61.8,  38.6, 61.8, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 100, 'M15_H1', 'print',            -1, 1, 1, 1, -1, 1, 1, 1, 38.2, 90, 23.6, 78.6, 50, 61.8, 23.6, 78.6, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, 1, 1, -1, 1, 1, 1, 38.2, 90, 23.6, 78.6, 50, 61.8, 23.6, 78.6, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, 1, 1, -1, 1, 1, 1, 38.2, 90, 23.6, 78.6, 50, 61.8, 38.2, 99, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, 1, 1, -1, 1, 1, 1, 38.2, 90, 23.6, 78.6, 50, 61.8, 38.2, 99, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, 1, 1, -1, 1, 1, 1, 38.2, 90, 23.6, 78.6, 50, 78.6, 38.2, 90, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, 1, 1, -1, 1, 1, 1, 38.2, 90, 23.6, 78.6, 50, 78.6, 38.2, 90, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, 1, 1, -1, 1, 1, 1, 38.2, 90, 23.6, 90, 50, 61.8, 38.2, 99, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, 1, 1, -1, 1, 1, 1, 38.2, 90, 23.6, 90, 50, 61.8, 38.2, 99, 0, 10000, 0, 10000, 0, 10000, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, 1, 1, -1, 1, 1, 1, 38.2, 90, 23.6, 99, 50, 61.8, 38.2, 78.6, 0, 1300, 0, 10000, 25, 10000, 0, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, 1, 1, -1, 1, 1, 1, 38.2, 90, 23.6, 99, 50, 61.8, 38.2, 78.6, 0, 1300, 0, 10000, 25, 10000, 0, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto_Analysis(df, 30, 'M15_H1', 'print',            -1, 1, 1, 1, 1, 1, 1, 1, 38.2, 90, 23.6, 99, 50, 61.8, 38.2, 78.6, 0, 1000, 0, 10000, 0, 10000, 25, 10000)\n",
    "df_b, df_s = Auto_Analysis(df, 50, 'M15_H1', 'ret', -1, 1, 1, 1, 1, 1, 1, 1, 38.2, 90, 23.6, 99, 50, 61.8, 38.2, 78.6, 0, 1000, 0, 10000, 0, 10000, 25, 10000)\n",
    "print('Total months:',diff_month(df),',months w/ points:',N_months_concat(df_b, df_s),'(',round(100*(N_months_concat(df_b, df_s)/diff_month(df)),1),'%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd Part of the Strategy - AI\n",
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(df, n):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc(model, X_test, y_test):\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import roc_curve, roc_auc_score\n",
    "    from matplotlib import pyplot\n",
    "\n",
    "    ## generate a no skill prediction (majority class)\n",
    "    ns_probs = [0 for _ in range(len(y_test))]\n",
    "\n",
    "    ## predict probabilities\n",
    "    lr_probs = model.predict_proba(X_test)\n",
    "    ## keep probabilities for the positive outcome only\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    ## calculate scores\n",
    "    ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "    lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "    ## summarize scores\n",
    "    print('No Skill   : ROC AUC=%.3f' % (ns_auc))\n",
    "    print('Rand Forest: ROC AUC=%.3f' % (lr_auc))\n",
    "    ## calculate roc curves\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "    ## plot the roc curve for the model\n",
    "    pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "    pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Rand Forest')\n",
    "    ## axis labels\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    ## show the legend\n",
    "    pyplot.legend()\n",
    "    ## show the plot\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df1,df2,df3,Label):\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "\n",
    "    df_frames = [df1, df2]\n",
    "    frames = pd.concat(df_frames)\n",
    "\n",
    "    X = frames.drop([Label],axis=1)[:]\n",
    "    y = frames[Label][:]\n",
    "\n",
    "    X2 = df3.drop([Label],axis=1)\n",
    "    y2 = df3[Label]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.99, shuffle = True, random_state = 101)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, X2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_rfc_random(X_train,y_train):\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "    from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "    from sklearn.datasets import make_classification\n",
    "\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 4)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 4)]\n",
    "    max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "\n",
    "    print(random_grid)\n",
    "\n",
    "    # Use the random grid to search for best hyperparameters\n",
    "    # First create the base model to tune\n",
    "    rfc = RandomForestClassifier()\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    rfc_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "    \n",
    "    # Fit the random search model\n",
    "    rfc_random.fit(X_train,y_train)\n",
    "    \n",
    "    print('Best Parameters:', rfc_random.best_params_)\n",
    "    \n",
    "    return rfc_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(model, X_test, y_test, thres):\n",
    "\n",
    "    from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "    print('Predictions - Threshold >=',thres)\n",
    "    predicted_proba = model.predict_proba(X_test)\n",
    "    predictions = (predicted_proba [:,1] >= thres).astype('int')\n",
    "    print(classification_report(y_test,predictions))\n",
    "    print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_cols(model, X_train, threshold):\n",
    "\n",
    "    feature_names = [f'{col}' for col in X_train.columns]\n",
    "    importances = model.feature_importances_\n",
    "    forest_importances = pd.Series(importances, index=feature_names)\n",
    "    \n",
    "    return list(forest_importances[forest_importances > threshold].sort_values(ascending = False).index[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_corr(df, ind_cols, th):\n",
    "\n",
    "    corr_list_top   = get_top_abs_correlations(df[ind_cols], 150)\n",
    "    corr_list_top_f = corr_list_top[corr_list_top > th].index[:]\n",
    "    \n",
    "    corr_list =[]\n",
    "    \n",
    "    for i in range(0,len(corr_list_top_f)):\n",
    "        corr_list.append(corr_list_top_f[i][1])\n",
    "    \n",
    "    corr_list = list(set(corr_list))\n",
    "    ret_list  = [x for x in ind_cols if x in corr_list]\n",
    "    \n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pre_rec_curve(model, X_test, y_test, thres):\n",
    "    \n",
    "    from sklearn.metrics import precision_recall_curve, accuracy_score, precision_score, average_precision_score\n",
    "    from sklearn.metrics import plot_precision_recall_curve\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    predicted_proba = model.predict_proba(X_test)\n",
    "    predicted = (predicted_proba [:,1] >= thres).astype('int')\n",
    "\n",
    "    accuracy  = accuracy_score(y_test, predicted)\n",
    "    precision = precision_score(y_test, predicted)\n",
    "\n",
    "    disp = plot_precision_recall_curve(model, X_test, y_test)\n",
    "    disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                       'AP={0:0.2f}'.format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_thres(model, X_test1, y_test1, X_test2, y_test2, X_test3, y_test3, pr):\n",
    "    \n",
    "    predicted_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    for i in np.arange(0,1,0.02):\n",
    "        \n",
    "        predicted_proba1 = model.predict_proba(X_test1)\n",
    "        predicted1 = (predicted_proba1 [:,1] >= i).astype('int')\n",
    "        if precision_score(y_test1, predicted1) > pr:\n",
    "            break\n",
    "            \n",
    "    print(i, precision_score(y_test1, predicted1))\n",
    "\n",
    "    for j in np.arange(0,1,0.02):\n",
    "        \n",
    "        predicted_proba2 = model.predict_proba(X_test2)\n",
    "        predicted2 = (predicted_proba2 [:,1] >= j).astype('int')\n",
    "        if precision_score(y_test2, predicted2) > pr:\n",
    "            break\n",
    "            \n",
    "    print(j, precision_score(y_test2, predicted2))\n",
    "\n",
    "    for k in np.arange(0,1,0.02):\n",
    "        \n",
    "        predicted_proba3 = model.predict_proba(X_test3)\n",
    "        predicted3 = (predicted_proba3 [:,1] >= k).astype('int')\n",
    "        if precision_score(y_test3, predicted3) > pr:\n",
    "            break\n",
    "            \n",
    "    print(k, precision_score(y_test3, predicted3))\n",
    "    \n",
    "    t = max(i,j,k)\n",
    "    print ('Threshold:',t) \n",
    "            \n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "\n",
    "# Read, clean and split the df in 3 for H1_b, H1_s, H1_H4_b, H1_H4_s\n",
    "\n",
    "dfv = pd.read_pickle('dffv_EURUSD.pkl')\n",
    "df = clean_data(dfv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore and drop corralated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_cols =  ['RSI_14_SMA_H1', 'RSI_14_EMA_H1', 'EMA_12_macd_H1', 'EMA_26_macd_H1',\n",
    "             'MACD_12_26_H1', 'EMA_MACD_12_26_9_H1', 'Hist_MACD_12_26_9_H1', 'MACD_signal_H1', \n",
    "             'Boll_SMA_20_H1', 'Boll_SMA_20_Var_-2_H1', 'Boll_SMA_20_Var_+2_H1', 'Dist_BollB_SMA_20_Var_+2_H1', \n",
    "             'Dist_BollB_SMA_20_Var_-2_H1', 'BollB_Wideness_H1',\n",
    "             'RSI_14_SMA_H4', 'RSI_14_EMA_H4', 'EMA_12_macd_H4', 'EMA_26_macd_H4',\n",
    "             'MACD_12_26_H4', 'EMA_MACD_12_26_9_H4', 'Hist_MACD_12_26_9_H4', 'MACD_signal_H4', \n",
    "             'Boll_SMA_20_H4', 'Boll_SMA_20_Var_-2_H4', 'Boll_SMA_20_Var_+2_H4', 'Dist_BollB_SMA_20_Var_+2_H4', \n",
    "             'Dist_BollB_SMA_20_Var_-2_H4', 'BollB_Wideness_H4',\n",
    "             'RSI_14_SMA_D1', 'RSI_14_EMA_D1', 'EMA_12_macd_D1', 'EMA_26_macd_D1',\n",
    "             'MACD_12_26_D1', 'EMA_MACD_12_26_9_D1', 'Hist_MACD_12_26_9_D1', 'MACD_signal_D1', \n",
    "             'Boll_SMA_20_D1', 'Boll_SMA_20_Var_-2_D1', 'Boll_SMA_20_Var_+2_D1', 'Dist_BollB_SMA_20_Var_+2_D1', \n",
    "             'Dist_BollB_SMA_20_Var_-2_D1', 'BollB_Wideness_D1']\n",
    "\n",
    "print(\"Top Absolute Correlations\")\n",
    "print(get_top_abs_correlations(df[ind_cols], 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corrolated columns that need dropping\n",
    "\n",
    "corr_cols = remove_corr(df, ind_cols, 0.98)\n",
    "\n",
    "print(\"Top Absolute Correlations\")\n",
    "print(get_top_abs_correlations(df[list(set(ind_cols)-set(corr_cols))], 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlations between columns\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ind_cols = list(set(ind_cols)-set(corr_cols))\n",
    "\n",
    "df_corr = df[ind_cols].corr().round(2)\n",
    "\n",
    "kot = df_corr[df_corr>=.0]\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "ax = sns.heatmap(kot, vmin=-1, vmax=1, square=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop correlated columns\n",
    "\n",
    "#df.drop(corr_cols, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtration process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data in 3 dfs\n",
    "\n",
    "df3, df2, df1 = split3_dfs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filtered dataframes for M15_H1\n",
    "df1_filt_b_M15_H1, df1_filt_s_M15_H1 = Auto_Analysis(df1, 50, 'M15_H1', 'ret', -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 78.6,  38.6, 61.8, 0, 1900, 0, 400, 0, 200, 0, 10000)\n",
    "df2_filt_b_M15_H1, df2_filt_s_M15_H1 = Auto_Analysis(df2, 50, 'M15_H1', 'ret', -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 78.6,  38.6, 61.8, 0, 1900, 0, 400, 0, 200, 0, 10000)\n",
    "df3_filt_b_M15_H1, df3_filt_s_M15_H1 = Auto_Analysis(df3, 50, 'M15_H1', 'ret', -1, 1, -1, 1, 1, 1, -1, 1, 38.6, 50, 23.6, 61.8, 0, 78.6,  38.6, 61.8, 0, 1900, 0, 400, 0, 200, 0, 10000)\n",
    "\n",
    "b_tot  = df1_filt_b_M15_H1.shape[0] + df2_filt_b_M15_H1.shape[0] + df3_filt_b_M15_H1.shape[0]\n",
    "s_tot  = df1_filt_s_M15_H1.shape[0] + df2_filt_s_M15_H1.shape[0] + df3_filt_s_M15_H1.shape[0]\n",
    "b_perc = (df1_filt_b_M15_H1[df1_filt_b_M15_H1['Labelb_M15_H1'] == 1].count()[0] + df2_filt_b_M15_H1[df2_filt_b_M15_H1['Labelb_M15_H1'] == 1].count()[0] + df3_filt_b_M15_H1[df3_filt_b_M15_H1['Labelb_M15_H1'] == 1].count()[0])/b_tot\n",
    "s_perc = (df1_filt_s_M15_H1[df1_filt_s_M15_H1['Labelb_M15_H1'] == 1].count()[0] + df2_filt_s_M15_H1[df2_filt_s_M15_H1['Labels_M15_H1'] == 1].count()[0] + df3_filt_s_M15_H1[df3_filt_s_M15_H1['Labels_M15_H1'] == 1].count()[0])/s_tot\n",
    "\n",
    "print('M15_H1 BUY   Shapes df1, df2, df3: ',df1_filt_b_M15_H1.shape,df2_filt_b_M15_H1.shape,df3_filt_b_M15_H1.shape)\n",
    "print('M15_H1 BUY   Total:',b_tot,'points')\n",
    "print('M15_H1 BUY   Overall performance:',round(100 * b_perc,2),'%\\n')\n",
    "print('M15_H1 SELL  Shapes df1, df2, df3: ',df1_filt_s_M15_H1.shape,df2_filt_s_M15_H1.shape,df3_filt_s_M15_H1.shape)\n",
    "print('M15_H1 SELL  Total:',s_tot,'points')\n",
    "print('M15_H1 SELL  Overall performance:',round(100 * s_perc,2),'%\\n')\n",
    "print('TOTAL POINTS:',b_tot + s_tot,'of',len(df),'(',round(100*(b_tot + s_tot)/len(df),2),'%)')\n",
    "print('TOTAL PERFORMANCE:',round((100 * (((b_perc*b_tot) + (s_perc*s_tot))/(b_tot+s_tot))),2),'%\\n\\n')\n",
    "\n",
    "# Create filtered dataframes for M15_H4\n",
    "df1_filt_b_M15_H4, df1_filt_s_M15_H4 = Auto_Analysis(df1, 50, 'M15_H4', 'ret', 1, 1, 1, 1, -1, 1, -1, 1,0, 78.6, 23.6, 78.6, 0, 90, 0, 99, 0, 1200, 280, 10000, 0, 10000, 0, 10000)\n",
    "df2_filt_b_M15_H4, df2_filt_s_M15_H4 = Auto_Analysis(df2, 50, 'M15_H4', 'ret', 1, 1, 1, 1, -1, 1, -1, 1,0, 78.6, 23.6, 78.6, 0, 90, 0, 99, 0, 1200, 280, 10000, 0, 10000, 0, 10000)\n",
    "df3_filt_b_M15_H4, df3_filt_s_M15_H4 = Auto_Analysis(df3, 50, 'M15_H4', 'ret', 1, 1, 1, 1, -1, 1, -1, 1,0, 78.6, 23.6, 78.6, 0, 90, 0, 99, 0, 1200, 280, 10000, 0, 10000, 0, 10000)\n",
    "\n",
    "b_tot  = df1_filt_b_M15_H4.shape[0] + df2_filt_b_M15_H4.shape[0] + df3_filt_b_M15_H4.shape[0]\n",
    "s_tot  = df1_filt_s_M15_H4.shape[0] + df2_filt_s_M15_H4.shape[0] + df3_filt_s_M15_H4.shape[0]\n",
    "b_perc = (df1_filt_b_M15_H4[df1_filt_b_M15_H4['Labelb_M15_H4'] == 1].count()[0] + df2_filt_b_M15_H4[df2_filt_b_M15_H4['Labelb_M15_H4'] == 1].count()[0] + df3_filt_b_M15_H4[df3_filt_b_M15_H4['Labelb_M15_H4'] == 1].count()[0])/b_tot\n",
    "s_perc = (df1_filt_s_M15_H4[df1_filt_s_M15_H4['Labels_M15_H4'] == 1].count()[0] + df2_filt_s_M15_H4[df2_filt_s_M15_H4['Labels_M15_H4'] == 1].count()[0] + df3_filt_s_M15_H4[df3_filt_s_M15_H4['Labels_M15_H4'] == 1].count()[0])/s_tot\n",
    "\n",
    "print('M15_H4 BUY   Shapes df1, df2, df3: ',df1_filt_b_M15_H4.shape,df2_filt_b_M15_H4.shape,df3_filt_b_M15_H4.shape)\n",
    "print('M15_H4 BUY   Total:',b_tot,'points')\n",
    "print('M15_H4 BUY   Overall performance:',round(100 * b_perc,2),'%\\n')\n",
    "print('M15_H4 SELL  Shapes df1, df2, df3: ',df1_filt_s_M15_H4.shape,df2_filt_s_M15_H4.shape,df3_filt_s_M15_H4.shape)\n",
    "print('M15_H4 SELL  Total:',s_tot,'points')\n",
    "print('M15_H4 SELL  Overall performance:',round(100 * s_perc,2),'%\\n')\n",
    "print('TOTAL POINTS:',b_tot + s_tot,'of',len(df),'(',round(100*(b_tot + s_tot)/len(df),2),'%)')\n",
    "print('TOTAL PERFORMANCE:',round((100 * (((b_perc*b_tot) + (s_perc*s_tot))/(b_tot+s_tot))),2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Buy  df1_M15_H1',N_months(df1_filt_b_M15_H1),'months out of', diff_month(df1))\n",
    "print('Buy  df2_M15_H1',N_months(df2_filt_b_M15_H1),'months out of', diff_month(df2))\n",
    "print('Buy  df3_M15_H1',N_months(df3_filt_b_M15_H1),'months out of', diff_month(df3),'\\n')\n",
    "\n",
    "print('Sell df1_M15_H1',N_months(df1_filt_s_M15_H1),'months out of', diff_month(df1))\n",
    "print('Sell df2_M15_H1',N_months(df2_filt_s_M15_H1),'months out of', diff_month(df2))\n",
    "print('Sell df3_M15_H1',N_months(df3_filt_s_M15_H1),'months out of', diff_month(df3),'\\n')\n",
    "\n",
    "print('Buy  df1_M15_H4',N_months(df1_filt_b_M15_H4),'months out of', diff_month(df1))\n",
    "print('Buy  df2_M15_H4',N_months(df2_filt_b_M15_H4),'months out of', diff_month(df2))\n",
    "print('Buy  df3_M15_H4',N_months(df3_filt_b_M15_H4),'months out of', diff_month(df3),'\\n')\n",
    "\n",
    "print('Sell df1_M15_H4',N_months(df1_filt_s_M15_H4),'months out of', diff_month(df1))\n",
    "print('Sell df2_M15_H4',N_months(df2_filt_s_M15_H4),'months out of', diff_month(df2))\n",
    "print('Sell df3_M15_H4',N_months(df3_filt_s_M15_H4),'months out of', diff_month(df3),'\\n')\n",
    "\n",
    "print('Total df1:',N_months_concat4(df1_filt_b_M15_H1, df1_filt_s_M15_H1, df1_filt_b_M15_H4, df1_filt_s_M15_H4),'months out of',diff_month(df1))\n",
    "print('Total df2:',N_months_concat4(df2_filt_b_M15_H1, df2_filt_s_M15_H1, df2_filt_b_M15_H4, df2_filt_s_M15_H4),'months out of',diff_month(df2))\n",
    "print('Total df3:',N_months_concat4(df3_filt_b_M15_H1, df3_filt_s_M15_H1, df3_filt_b_M15_H4, df3_filt_s_M15_H4),'months out of',diff_month(df3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index M15_H1\n",
    "df1_filt_b_M15_H1, df1_filt_s_M15_H1 = df1_filt_b_M15_H1.reset_index(drop=True), df1_filt_s_M15_H1.reset_index(drop=True)\n",
    "df2_filt_b_M15_H1, df2_filt_s_M15_H1 = df2_filt_b_M15_H1.reset_index(drop=True), df2_filt_s_M15_H1.reset_index(drop=True)\n",
    "df3_filt_b_M15_H1, df3_filt_s_M15_H1 = df3_filt_b_M15_H1.reset_index(drop=True), df3_filt_s_M15_H1.reset_index(drop=True)\n",
    "\n",
    "# Reset index M15_H4\n",
    "df1_filt_b_M15_H4, df1_filt_s_M15_H4 = df1_filt_b_M15_H4.reset_index(drop=True), df1_filt_s_M15_H4.reset_index(drop=True)\n",
    "df2_filt_b_M15_H4, df2_filt_s_M15_H4 = df2_filt_b_M15_H4.reset_index(drop=True), df2_filt_s_M15_H4.reset_index(drop=True)\n",
    "df3_filt_b_M15_H4, df3_filt_s_M15_H4 = df3_filt_b_M15_H4.reset_index(drop=True), df3_filt_s_M15_H4.reset_index(drop=True)\n",
    "\n",
    "# List of generic columns required to be drop:\n",
    "\n",
    "drop_gen_cols = ['index', 'Price_M15', 'MS_N_M15','MS_N_H1', 'MS_N_H4', 'MS_N_D1',\n",
    "                 #'Trend_M15','Trend_H1','Trend_H4','Trend_D1',\n",
    "                 'MS_Sit_M15','MS_Sit_H1', 'MS_Sit_H4', 'MS_Sit_D1',\n",
    "                 'Date_M15', 'Open_M15', 'High_M15', 'Low_M15', 'Close_M15', 'MS_L_M15', 'MS_H_M15',\n",
    "                 'Date_H1' , 'Open_H1' , 'High_H1' , 'Low_H1' , 'Close_H1' , 'MS_L_H1' , 'MS_H_H1',\n",
    "                 'Date_H4' , 'Open_H4' , 'High_H4' , 'Low_H4' , 'Close_H4' , 'MS_L_H4' , 'MS_H_H4',\n",
    "                 'Date_D1' , 'Open_D1' , 'High_D1' , 'Low_D1' , 'Close_D1' , 'MS_L_D1' , 'MS_H_D1',\n",
    "                 'B_Stop_Loss_M15_H1', 'B_Limit_M15_H1', 'S_Stop_Loss_M15_H1', 'S_Limit_M15_H1',\n",
    "                 'B_Stop_Loss_M15_H4', 'B_Limit_M15_H4', 'S_Stop_Loss_M15_H4', 'S_Limit_M15_H4']\n",
    "\n",
    "#Additonal columns to be dropped in each dataframe:\n",
    "\n",
    "drop_M15_H1_b_cols = ['Labels_M15_H1', 'Labelb_M15_H4', 'Labels_M15_H4'] # ['Labelb_M15_H1'] remains\n",
    "drop_M15_H1_s_cols = ['Labelb_M15_H1', 'Labelb_M15_H4', 'Labels_M15_H4'] # ['Labels_M15_H1'] remains\n",
    "\n",
    "drop_M15_H4_b_cols = ['Labelb_M15_H1', 'Labels_M15_H1', 'Labels_M15_H4'] # ['Labelb_M15_H4'] remains\n",
    "drop_M15_H4_s_cols = ['Labelb_M15_H1', 'Labels_M15_H1', 'Labelb_M15_H4'] # ['Labels_M15_H4'] remains\n",
    "\n",
    "# Drop the unnecessary columns\n",
    "\n",
    "df1_filt_b_M15_H1.drop(drop_gen_cols + drop_M15_H1_b_cols, axis=1, inplace =True)\n",
    "df2_filt_b_M15_H1.drop(drop_gen_cols + drop_M15_H1_b_cols, axis=1, inplace =True)\n",
    "df3_filt_b_M15_H1.drop(drop_gen_cols + drop_M15_H1_b_cols, axis=1, inplace =True)\n",
    "\n",
    "df1_filt_s_M15_H1.drop(drop_gen_cols + drop_M15_H1_s_cols, axis=1, inplace =True)\n",
    "df2_filt_s_M15_H1.drop(drop_gen_cols + drop_M15_H1_s_cols, axis=1, inplace =True)\n",
    "df3_filt_s_M15_H1.drop(drop_gen_cols + drop_M15_H1_s_cols, axis=1, inplace =True)\n",
    "\n",
    "df1_filt_b_M15_H4.drop(drop_gen_cols + drop_M15_H4_b_cols, axis=1, inplace =True)\n",
    "df2_filt_b_M15_H4.drop(drop_gen_cols + drop_M15_H4_b_cols, axis=1, inplace =True)\n",
    "df3_filt_b_M15_H4.drop(drop_gen_cols + drop_M15_H4_b_cols, axis=1, inplace =True)\n",
    "\n",
    "df1_filt_s_M15_H4.drop(drop_gen_cols + drop_M15_H4_s_cols, axis=1, inplace =True)\n",
    "df2_filt_s_M15_H4.drop(drop_gen_cols + drop_M15_H4_s_cols, axis=1, inplace =True)\n",
    "df3_filt_s_M15_H4.drop(drop_gen_cols + drop_M15_H4_s_cols, axis=1, inplace =True)\n",
    "\n",
    "# Columns left so far:\n",
    "\n",
    "#Trend_H1,\n",
    "#Labelb_H1, Labels_H1, Labelb_H1_H4, Labels_H1_H4,\n",
    "#Closing_X, MS_Sit_X, N_Breaks_X, MS_Pds_X, MS_range_X, MS_retracement_X\n",
    "#RSI_14_SMA_X, RSI_14_EMA, EMA_12_macd_X, EMA_26_macd,\n",
    "#MACD_12_26_X, EMA_MACD_12_26_9_X, Hist_MACD_12_26_9_X, MACD_signal_X\n",
    "#Boll_SMA_20_X, Boll_SMA_20_Var_-2_X, Boll_SMA_20_Var_+2_X,\n",
    "#Dist_BollB_SMA_20_Var_+2_X, Dist_BollB_SMA_20_Var_-2_XH1, BollB_Wideness_X\n",
    "\n",
    "print('H1 BUY  Shapes df1, df2, df3: ',df1_filt_b_M15_H1.shape,df2_filt_b_M15_H1.shape,df3_filt_b_M15_H1.shape)\n",
    "print('H1 SELL Shapes df1, df2, df3: ',df1_filt_s_M15_H1.shape,df2_filt_s_M15_H1.shape,df3_filt_s_M15_H1.shape)\n",
    "\n",
    "print('H4 BUY  Shapes df1, df2, df3: ',df1_filt_b_M15_H4.shape,df2_filt_b_M15_H4.shape,df3_filt_b_M15_H4.shape)\n",
    "print('H4 SELL Shapes df1, df2, df3: ',df1_filt_s_M15_H4.shape,df2_filt_s_M15_H4.shape,df3_filt_s_M15_H4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Modelling\n",
    "#### [M15_H1] BUY - Explores Best RFC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = 'Labelb_M15_H1'\n",
    "\n",
    "df_1  = df1_filt_b_M15_H1\n",
    "df_2  = df2_filt_b_M15_H1\n",
    "df_3  = df3_filt_b_M15_H1\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and splits the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Instantiates the model\n",
    "rfc_b1 = RandomForestClassifier()\n",
    "    \n",
    "# Fit the model of df_1 & df_2 split\n",
    "rfc_b1.fit(X_train,y_train)\n",
    "\n",
    "# Plots the ROC curve\n",
    "roc(rfc_b1, X2, y2)\n",
    "\n",
    "# Predicts the df_3 df using the rfc model created from df_1 & df_2\n",
    "predictions(rfc_b1, X_test = X2, y_test = y2, thres = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = 'Labelb_M15_H1'\n",
    "\n",
    "df_1  = df2_filt_b_M15_H1\n",
    "df_2  = df3_filt_b_M15_H1\n",
    "df_3  = df1_filt_b_M15_H1\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and splits the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Instantiates the model\n",
    "rfc_b2 = RandomForestClassifier()\n",
    "    \n",
    "# Fit the model of df_1 & df_2 split\n",
    "rfc_b2.fit(X_train,y_train)\n",
    "\n",
    "# Plots the ROC curve\n",
    "roc(rfc_b2, X2, y2)\n",
    "\n",
    "# Predicts the df_3 df using the rfc model created from df_1 & df_2\n",
    "predictions(rfc_b2, X_test = X2, y_test = y2, thres = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = 'Labelb_M15_H1'\n",
    "\n",
    "df_1  = df1_filt_b_M15_H1\n",
    "df_2  = df3_filt_b_M15_H1\n",
    "df_3  = df2_filt_b_M15_H1\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and splits the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Instantiates the model\n",
    "rfc_b3 = RandomForestClassifier()\n",
    "    \n",
    "# Fit the model of df_1 & df_2 split\n",
    "rfc_b3.fit(X_train,y_train)\n",
    "\n",
    "# Plots the ROC curve\n",
    "roc(rfc_b3, X2, y2)\n",
    "\n",
    "# Predicts the df_3 df using the rfc model created from df_1 & df_2\n",
    "predictions(rfc_b3, X_test = X2, y_test = y2, thres = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H1] BUY - Optimum RFC Model\n",
    "\n",
    "The chosen model is rcf_b2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rfc_opt_b_H1 model\n",
    "with open('rfc_opt_b_H1.pkl', 'rb') as f:\n",
    "    rfc_opt_b_H1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rfc_opt_b_H1 = rfc_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosen rcf is rcf_b2\n",
    "# Apply Random grid search to obtain optimum parameters on chosen model\n",
    "\n",
    "Label = 'Labelb_M15_H1'\n",
    "\n",
    "df_1  = df2_filt_b_M15_H1\n",
    "df_2  = df3_filt_b_M15_H1\n",
    "df_3  = df1_filt_b_M15_H1\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and splits the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Returns the optimum random search model\n",
    "rfc_opt_b_H1 = model_rfc_random(X_train,y_train)\n",
    "\n",
    "# Plots the ROC curve\n",
    "roc(rfc_opt_b_H1, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc_opt_b_H1, X_test = X2, y_test = y2, thres = 0.3)\n",
    "\n",
    "# save model to a file\n",
    "with open('rfc_opt_b_H1.pkl', 'wb') as f:\n",
    "    pickle.dump(rfc_opt_b_H1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H1] BUY - Optimum model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test on remaining dataframes\n",
    "Label = 'Labelb_M15_H1'\n",
    "\n",
    "X1 = df1_filt_b_M15_H1.drop([Label],axis=1)\n",
    "y1 = df1_filt_b_M15_H1[Label]\n",
    "\n",
    "X2 = df2_filt_b_M15_H1.drop([Label],axis=1)\n",
    "y2 = df2_filt_b_M15_H1[Label]\n",
    "\n",
    "X3 = df3_filt_b_M15_H1.drop([Label],axis=1)\n",
    "y3 = df3_filt_b_M15_H1[Label]\n",
    "\n",
    "thres = opt_thres(rfc_opt_b_H1, X1, y1, X2, y2, X3, y3, pr = 0.5)\n",
    "\n",
    "# Displays the ROC curve (Receiver Operating Characteristic)\n",
    "roc(rfc_opt_b_H1, X1, y1)\n",
    "\n",
    "# Predicts\n",
    "predictions(rfc_opt_b_H1, X_test = X1, y_test = y1, thres = thres)\n",
    "\n",
    "# Displays the ROC curve (Receiver Operating Characteristic)\n",
    "roc(rfc_opt_b_H1, X2, y2)\n",
    "\n",
    "# Predicts\n",
    "predictions(rfc_opt_b_H1, X_test = X2, y_test = y2, thres = thres)\n",
    "\n",
    "# Displays the ROC curve (Receiver Operating Characteristic)\n",
    "roc(rfc_opt_b_H1, X3, y3)\n",
    "\n",
    "# Predicts\n",
    "predictions(rfc_opt_b_H1, X_test = X3, y_test = y3, thres = thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the Precision - Recall curve\n",
    "Pre_rec_curve(rfc_opt_b_H1, X1, y1, thres)\n",
    "\n",
    "# Displays the Precision - Recall curve\n",
    "Pre_rec_curve(rfc_opt_b_H1, X2, y2, thres)\n",
    "\n",
    "# Displays the Precision - Recall curve\n",
    "Pre_rec_curve(rfc_opt_b_H1, X3, y3, thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = {} # a dict to hold feature_name: feature_importance\n",
    "for feature, importance in zip(X1.columns, rfc_opt_b_H1.feature_importances_):\n",
    "    feats[feature] = importance #add the name/value pair \n",
    "\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n",
    "importances = importances[importances['Gini-importance'] > 0.015]\n",
    "importances = importances.sort_values('Gini-importance',ascending=False)\n",
    "importances.plot(kind='bar', stacked=True, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H1] SELL - Explores Best RFC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = 'Labels_M15_H1'\n",
    "\n",
    "df_1  = df1_filt_s_M15_H1\n",
    "df_2  = df2_filt_s_M15_H1\n",
    "df_3  = df3_filt_s_M15_H1\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and splits the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Instantiates the model\n",
    "rfc_s1 = RandomForestClassifier()\n",
    "    \n",
    "# Fit the model of df_1 & df_2 split\n",
    "rfc_s1.fit(X_train,y_train)\n",
    "\n",
    "# Plots the ROC curve\n",
    "roc(rfc_s1, X2, y2)\n",
    "\n",
    "# Predicts the df_3 df using the rfc model created from df_1 & df_2\n",
    "predictions(rfc_s1, X_test = X2, y_test = y2, thres = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = 'Labels_M15_H1'\n",
    "\n",
    "df_1  = df2_filt_s_M15_H1\n",
    "df_2  = df3_filt_s_M15_H1\n",
    "df_3  = df1_filt_s_M15_H1\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and splits the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Instantiates the model\n",
    "rfc_s2 = RandomForestClassifier()\n",
    "    \n",
    "# Fit the model of df_1 & df_2 split\n",
    "rfc_s2.fit(X_train,y_train)\n",
    "\n",
    "# Plots the ROC curve\n",
    "roc(rfc_s2, X2, y2)\n",
    "\n",
    "# Predicts the df_3 df using the rfc model created from df_1 & df_2\n",
    "predictions(rfc_s2, X_test = X2, y_test = y2, thres = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = 'Labels_M15_H1'\n",
    "\n",
    "df_1  = df1_filt_s_M15_H1\n",
    "df_2  = df3_filt_s_M15_H1\n",
    "df_3  = df2_filt_s_M15_H1\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and splits the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Instantiates the model\n",
    "rfc_s3 = RandomForestClassifier()\n",
    "    \n",
    "# Fit the model of df_1 & df_2 split\n",
    "rfc_s3.fit(X_train,y_train)\n",
    "\n",
    "# Plots the ROC curve\n",
    "roc(rfc_s3, X2, y2)\n",
    "\n",
    "# Predicts the df_3 df using the rfc model created from df_1 & df_2\n",
    "predictions(rfc_s3, X_test = X2, y_test = y2, thres = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H1] SELL - Optimum RFC Model\n",
    "\n",
    "The chosen model is rcf_s3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rfc_opt_s_H1 model\n",
    "with open('rfc_opt_s_H1.pkl', 'rb') as f:\n",
    "    rfc_opt_s_H1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rfc_opt_s_H1 = rfc_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosen rcf is rcf_s3\n",
    "# Apply Random grid search to obtain optimum parameters on chosen model\n",
    "\n",
    "Label = 'Labels_M15_H1'\n",
    "\n",
    "df_1  = df1_filt_s_M15_H1\n",
    "df_2  = df3_filt_s_M15_H1\n",
    "df_3  = df2_filt_s_M15_H1\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and splits the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Returns the optimum random search model\n",
    "rfc_opt_s_H1 = model_rfc_random(X_train,y_train)\n",
    "\n",
    "# Plots the ROC curve\n",
    "roc(rfc_opt_s_H1, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc_opt_s_H1, X_test = X2, y_test = y2, thres = 0.3)\n",
    "\n",
    "# save model to a file\n",
    "with open('rfc_opt_s_H1.pkl', 'wb') as f:\n",
    "    pickle.dump(rfc_opt_s_H1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H1] SELL - Optimum model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test on remaining dataframes\n",
    "Label = 'Labels_M15_H1'\n",
    "\n",
    "X1 = df1_filt_s_M15_H1.drop([Label],axis=1)\n",
    "y1 = df1_filt_s_M15_H1[Label]\n",
    "\n",
    "X2 = df2_filt_s_M15_H1.drop([Label],axis=1)\n",
    "y2 = df2_filt_s_M15_H1[Label]\n",
    "\n",
    "X3 = df3_filt_s_M15_H1.drop([Label],axis=1)\n",
    "y3 = df3_filt_s_M15_H1[Label]\n",
    "\n",
    "thres = opt_thres(rfc_opt_s_H1, X1, y1, X2, y2, X3, y3, pr = 0.5)\n",
    "\n",
    "# Displays the ROC curve (Receiver Operating Characteristic)\n",
    "roc(rfc_opt_s_H1, X1, y1)\n",
    "\n",
    "# Predicts\n",
    "predictions(rfc_opt_s_H1, X_test = X1, y_test = y1, thres = thres)\n",
    "\n",
    "# Displays the ROC curve (Receiver Operating Characteristic)\n",
    "roc(rfc_opt_s_H1, X2, y2)\n",
    "\n",
    "# Predicts\n",
    "predictions(rfc_opt_s_H1, X_test = X2, y_test = y2, thres = thres)\n",
    "\n",
    "# Displays the ROC curve (Receiver Operating Characteristic)\n",
    "roc(rfc_opt_s_H1, X3, y3)\n",
    "\n",
    "# Predicts\n",
    "predictions(rfc_opt_s_H1, X_test = X3, y_test = y3, thres = thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the Precision - Recall curve\n",
    "Pre_rec_curve(rfc_opt_s_H1, X1, y1, thres)\n",
    "\n",
    "# Displays the Precision - Recall curve\n",
    "Pre_rec_curve(rfc_opt_s_H1, X2, y2, thres)\n",
    "\n",
    "# Displays the Precision - Recall curve\n",
    "Pre_rec_curve(rfc_opt_s_H1, X3, y3, thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = {} # a dict to hold feature_name: feature_importance\n",
    "for feature, importance in zip(X1.columns, rfc_opt_s_H1.feature_importances_):\n",
    "    feats[feature] = importance #add the name/value pair \n",
    "\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n",
    "importances = importances[importances['Gini-importance'] > 0.015]\n",
    "importances = importances.sort_values('Gini-importance',ascending=False)\n",
    "importances.plot(kind='bar', stacked=True, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H4] BUY - Explores Best RFC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = 'Labelb_M15_H4'\n",
    "\n",
    "df_1  = df1_filt_b_M15_H4\n",
    "df_2  = df2_filt_b_M15_H4\n",
    "df_3  = df3_filt_b_M15_H4\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and splits the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Instantiates the model\n",
    "rfc_b1 = RandomForestClassifier()\n",
    "    \n",
    "# Fit the model of df_1 & df_2 split\n",
    "rfc_b1.fit(X_train,y_train)\n",
    "\n",
    "# Plots the ROC curve\n",
    "roc(rfc_b1, X2, y2)\n",
    "\n",
    "# Predicts the df_3 df using the rfc model created from df_1 & df_2\n",
    "predictions(rfc_b1, X_test = X2, y_test = y2, thres = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = 'Labelb_M15_H4'\n",
    "\n",
    "df_1  = df2_filt_b_M15_H4\n",
    "df_2  = df3_filt_b_M15_H4\n",
    "df_3  = df1_filt_b_M15_H4\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and splits the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Instantiates the model\n",
    "rfc_b2 = RandomForestClassifier()\n",
    "    \n",
    "# Fit the model of df_1 & df_2 split\n",
    "rfc_b2.fit(X_train,y_train)\n",
    "\n",
    "# Plots the ROC curve\n",
    "roc(rfc_b2, X2, y2)\n",
    "\n",
    "# Predicts the df_3 df using the rfc model created from df_1 & df_2\n",
    "predictions(rfc_b2, X_test = X2, y_test = y2, thres = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = 'Labelb_M15_H4'\n",
    "\n",
    "df_1  = df1_filt_b_M15_H4\n",
    "df_2  = df3_filt_b_M15_H4\n",
    "df_3  = df2_filt_b_M15_H4\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and splits the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Instantiates the model\n",
    "rfc_b3 = RandomForestClassifier()\n",
    "    \n",
    "# Fit the model of df_1 & df_2 split\n",
    "rfc_b3.fit(X_train,y_train)\n",
    "\n",
    "# Plots the ROC curve\n",
    "roc(rfc_b3, X2, y2)\n",
    "\n",
    "# Predicts the df_3 df using the rfc model created from df_1 & df_2\n",
    "predictions(rfc_b3, X_test = X2, y_test = y2, thres = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H4] BUY - Optimum RFC Model\n",
    "\n",
    "The chosen model is rcf_b3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rfc_opt_b_H4 model\n",
    "with open('rfc_opt_b_H4.pkl', 'rb') as f:\n",
    "    rfc_opt_b_H4 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rfc_opt_b_H4 = rfc_b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosen rcf is rcf_b2\n",
    "# Apply Random grid search to obtain optimum parameters on chosen model\n",
    "\n",
    "Label = 'Labelb_M15_H4'\n",
    "\n",
    "df_1  = df1_filt_b_M15_H4\n",
    "df_2  = df3_filt_b_M15_H4\n",
    "df_3  = df2_filt_b_M15_H4\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and splits the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Returns the optimum random search model\n",
    "rfc_opt_b_H4 = model_rfc_random(X_train,y_train)\n",
    "\n",
    "# Plots the ROC curve\n",
    "roc(rfc_opt_b_H4, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc_opt_b_H4, X_test = X2, y_test = y2, thres = 0.3)\n",
    "\n",
    "# save model to a file\n",
    "with open('rfc_opt_b_H4.pkl', 'wb') as f:\n",
    "    pickle.dump(rfc_opt_b_H4, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H4] BUY - Optimum model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test on remaining dataframes\n",
    "Label = 'Labelb_M15_H4'\n",
    "\n",
    "X1 = df1_filt_b_M15_H4.drop([Label],axis=1)\n",
    "y1 = df1_filt_b_M15_H4[Label]\n",
    "\n",
    "X2 = df2_filt_b_M15_H4.drop([Label],axis=1)\n",
    "y2 = df2_filt_b_M15_H4[Label]\n",
    "\n",
    "X3 = df3_filt_b_M15_H4.drop([Label],axis=1)\n",
    "y3 = df3_filt_b_M15_H4[Label]\n",
    "\n",
    "thres = opt_thres(rfc_opt_b_H4, X1, y1, X2, y2, X3, y3, pr = 0.5)\n",
    "\n",
    "# Displays the ROC curve (Receiver Operating Characteristic)\n",
    "roc(rfc_opt_b_H4, X1, y1)\n",
    "\n",
    "# Predicts\n",
    "predictions(rfc_opt_b_H4, X_test = X1, y_test = y1, thres = thres)\n",
    "\n",
    "# Displays the ROC curve (Receiver Operating Characteristic)\n",
    "roc(rfc_opt_b_H4, X2, y2)\n",
    "\n",
    "# Predicts\n",
    "predictions(rfc_opt_b_H4, X_test = X2, y_test = y2, thres = thres)\n",
    "\n",
    "# Displays the ROC curve (Receiver Operating Characteristic)\n",
    "roc(rfc_opt_b_H4, X3, y3)\n",
    "\n",
    "# Predicts\n",
    "predictions(rfc_opt_b_H4, X_test = X3, y_test = y3, thres = thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the Precision - Recall curve\n",
    "Pre_rec_curve(rfc_opt_b_H4, X1, y1, thres)\n",
    "\n",
    "# Displays the Precision - Recall curve\n",
    "Pre_rec_curve(rfc_opt_b_H4, X2, y2, thres)\n",
    "\n",
    "# Displays the Precision - Recall curve\n",
    "Pre_rec_curve(rfc_opt_b_H4, X3, y3, thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = {} # a dict to hold feature_name: feature_importance\n",
    "for feature, importance in zip(X1.columns, rfc_opt_b_H4.feature_importances_):\n",
    "    feats[feature] = importance #add the name/value pair \n",
    "\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n",
    "importances = importances[importances['Gini-importance'] > 0.015]\n",
    "importances = importances.sort_values('Gini-importance',ascending=False)\n",
    "importances.plot(kind='bar', stacked=True, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H4] SELL - Explores Best RFC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = 'Labels_M15_H4'\n",
    "\n",
    "df_1  = df1_filt_s_M15_H4\n",
    "df_2  = df2_filt_s_M15_H4\n",
    "df_3  = df3_filt_s_M15_H4\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and splits the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Instantiates the model\n",
    "rfc_s1 = RandomForestClassifier()\n",
    "    \n",
    "# Fit the model of df_1 & df_2 split\n",
    "rfc_s1.fit(X_train,y_train)\n",
    "\n",
    "# Plots the ROC curve\n",
    "roc(rfc_s1, X2, y2)\n",
    "\n",
    "# Predicts the df_3 df using the rfc model created from df_1 & df_2\n",
    "predictions(rfc_s1, X_test = X2, y_test = y2, thres = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = 'Labels_M15_H4'\n",
    "\n",
    "df_1  = df2_filt_s_M15_H4\n",
    "df_2  = df3_filt_s_M15_H4\n",
    "df_3  = df1_filt_s_M15_H4\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and splits the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Instantiates the model\n",
    "rfc_s2 = RandomForestClassifier()\n",
    "    \n",
    "# Fit the model of df_1 & df_2 split\n",
    "rfc_s2.fit(X_train,y_train)\n",
    "\n",
    "# Plots the ROC curve\n",
    "roc(rfc_s2, X2, y2)\n",
    "\n",
    "# Predicts the df_3 df using the rfc model created from df_1 & df_2\n",
    "predictions(rfc_s2, X_test = X2, y_test = y2, thres = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = 'Labels_M15_H4'\n",
    "\n",
    "df_1  = df1_filt_s_M15_H4\n",
    "df_2  = df3_filt_s_M15_H4\n",
    "df_3  = df2_filt_s_M15_H4\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and splits the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Instantiates the model\n",
    "rfc_s3 = RandomForestClassifier()\n",
    "    \n",
    "# Fit the model of df_1 & df_2 split\n",
    "rfc_s3.fit(X_train,y_train)\n",
    "\n",
    "# Plots the ROC curve\n",
    "roc(rfc_s3, X2, y2)\n",
    "\n",
    "# Predicts the df_3 df using the rfc model created from df_1 & df_2\n",
    "predictions(rfc_s3, X_test = X2, y_test = y2, thres = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H4] SELL - Optimum RFC Model\n",
    "\n",
    "The chosen model is rcf_s3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rfc_opt_s_H4 model\n",
    "with open('rfc_opt_s_H4.pkl', 'rb') as f:\n",
    "    rfc_opt_s_H4 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rfc_opt_s_H1 = rfc_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosen rcf is rcf_s3\n",
    "# Apply Random grid search to obtain optimum parameters on chosen model\n",
    "\n",
    "Label = 'Labels_M15_H4'\n",
    "\n",
    "df_1  = df1_filt_s_M15_H4\n",
    "df_2  = df3_filt_s_M15_H4\n",
    "df_3  = df2_filt_s_M15_H4\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and splits the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Returns the optimum random search model\n",
    "rfc_opt_s_H4 = model_rfc_random(X_train,y_train)\n",
    "\n",
    "# Plots the ROC curve\n",
    "roc(rfc_opt_s_H4, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc_opt_s_H4, X_test = X2, y_test = y2, thres = 0.3)\n",
    "\n",
    "# save model to a file\n",
    "with open('rfc_opt_s_H4.pkl', 'wb') as f:\n",
    "    pickle.dump(rfc_opt_s_H4, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H4] SELL - Optimum model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test on remaining dataframes\n",
    "Label = 'Labels_M15_H4'\n",
    "\n",
    "X1 = df1_filt_s_M15_H4.drop([Label],axis=1)\n",
    "y1 = df1_filt_s_M15_H4[Label]\n",
    "\n",
    "X2 = df2_filt_s_M15_H4.drop([Label],axis=1)\n",
    "y2 = df2_filt_s_M15_H4[Label]\n",
    "\n",
    "X3 = df3_filt_s_M15_H4.drop([Label],axis=1)\n",
    "y3 = df3_filt_s_M15_H4[Label]\n",
    "\n",
    "thres = opt_thres(rfc_opt_s_H4, X1, y1, X2, y2, X3, y3, pr = 0.5)\n",
    "\n",
    "# Displays the ROC curve (Receiver Operating Characteristic)\n",
    "roc(rfc_opt_s_H4, X1, y1)\n",
    "\n",
    "# Predicts\n",
    "predictions(rfc_opt_s_H4, X_test = X1, y_test = y1, thres = thres)\n",
    "\n",
    "# Displays the ROC curve (Receiver Operating Characteristic)\n",
    "roc(rfc_opt_s_H4, X2, y2)\n",
    "\n",
    "# Predicts\n",
    "predictions(rfc_opt_s_H4, X_test = X2, y_test = y2, thres = thres)\n",
    "\n",
    "# Displays the ROC curve (Receiver Operating Characteristic)\n",
    "roc(rfc_opt_s_H4, X3, y3)\n",
    "\n",
    "# Predicts\n",
    "predictions(rfc_opt_s_H4, X_test = X3, y_test = y3, thres = thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the Precision - Recall curve\n",
    "Pre_rec_curve(rfc_opt_s_H4, X1, y1, thres)\n",
    "\n",
    "# Displays the Precision - Recall curve\n",
    "Pre_rec_curve(rfc_opt_s_H4, X2, y2, thres)\n",
    "\n",
    "# Displays the Precision - Recall curve\n",
    "Pre_rec_curve(rfc_opt_s_H4, X3, y3, thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = {} # a dict to hold feature_name: feature_importance\n",
    "for feature, importance in zip(X1.columns, rfc_opt_s_H4.feature_importances_):\n",
    "    feats[feature] = importance #add the name/value pair \n",
    "\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n",
    "importances = importances[importances['Gini-importance'] > 0.015]\n",
    "importances = importances.sort_values('Gini-importance',ascending=False)\n",
    "importances.plot(kind='bar', stacked=True, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing random columns for Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H1] BUY - Trains df_2 & df_3 ---> Tests df_1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_1  = df2_filt_b_M15_H1\n",
    "df_2  = df3_filt_b_M15_H1\n",
    "df_3  = df1_filt_b_M15_H1\n",
    "Label = 'Labelb_M15_H1'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Returns the optimum random serach model\n",
    "#rfc_random_b_231 = model_rfc_random(X_train,y_train)\n",
    "rfc_random       = rfc_random_b_231\n",
    "# Displays the ROC curve (Receiver Operating Characteristic) of df_3 based on the model trained on df_1 & df_2\n",
    "roc(rfc_random, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc_random, X_test = X2, y_test = y2, thres = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1  = df2_filt_b_M15_H1\n",
    "df_2  = df3_filt_b_M15_H1\n",
    "df_3  = df1_filt_b_M15_H1\n",
    "Label = 'Labelb_M15_H1'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "rfc2 = RandomForestClassifier()\n",
    "    \n",
    "# Fit the random search model\n",
    "rfc2.fit(X_train,y_train)\n",
    "\n",
    "roc(rfc2, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc2, X_test = X2, y_test = y2, thres = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = {} # a dict to hold feature_name: feature_importance\n",
    "for feature, importance in zip(X_train.columns, rfc2.feature_importances_):\n",
    "    feats[feature] = importance #add the name/value pair \n",
    "\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n",
    "#importances.sort_values(by='Gini-importance').plot(kind='bar', rot=45)\n",
    "importances.sort_values('Gini-importance',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rfc.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features_names = df1_filt_b_M15_H1.columns\n",
    "feature_names = [features_names[i] for i in indices]\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(features_names[f], importances[indices[f]])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H1] BUY - Trains df_1 & df_3 ---> Tests df_2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_1  = df1_filt_b_M15_H1\n",
    "df_2  = df3_filt_b_M15_H1\n",
    "df_3  = df2_filt_b_M15_H1\n",
    "Label = 'Labelb_M15_H1'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Returns the optimum random serach model\n",
    "#rfc_random_b_132 = model_rfc_random(X_train,y_train)\n",
    "rfc_random      = rfc_random_b_132\n",
    "# Displays the ROC curve (Receiver Operating Characteristic) of df_3 based on the model trained on df_1 & df_2\n",
    "roc(rfc_random, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc_random, X_test = X2, y_test = y2, thres = 0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"['MACD_signal_H1' 'Dist_BollB_SMA_20_Var_+2_H1'\\n 'Dist_BollB_SMA_20_Var_-2_H1' 'BollB_Wideness_H1' 'Closing_H4'\\n 'N_Breaks_H4' 'MS_Pds_H4' 'MS_range_H4' 'RSI_14_SMA_H4' 'RSI_14_EMA_H4'\\n 'MACD_12_26_H4' 'Hist_MACD_12_26_9_H4' 'MACD_signal_H4'\\n 'Dist_BollB_SMA_20_Var_+2_H4' 'Dist_BollB_SMA_20_Var_-2_H4'\\n 'BollB_Wideness_H4' 'Closing_D1' 'N_Breaks_D1' 'MS_Pds_D1' 'MS_range_D1'\\n 'RSI_14_SMA_D1' 'RSI_14_EMA_D1' 'MACD_12_26_D1' 'Hist_MACD_12_26_9_D1'\\n 'MACD_signal_D1' 'Dist_BollB_SMA_20_Var_+2_D1'\\n 'Dist_BollB_SMA_20_Var_-2_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1  = df1_filt_b_M15_H1#.drop(del_cols,axis=1)\n",
    "df_2  = df3_filt_b_M15_H1#.drop(del_cols,axis=1)\n",
    "df_3  = df2_filt_b_M15_H1#.drop(del_cols,axis=1)\n",
    "Label = 'Labelb_M15_H1'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "#rfc3 = RandomForestClassifier()\n",
    "    \n",
    "# Fit the random search model\n",
    "#rfc2.fit(X_train,y_train)\n",
    "\n",
    "roc(rfc2, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc2, X_test = X2, y_test = y2, thres = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = {} # a dict to hold feature_name: feature_importance\n",
    "for feature, importance in zip(X_train.columns, rfc3.feature_importances_):\n",
    "    feats[feature] = importance #add the name/value pair \n",
    "\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n",
    "#importances.sort_values(by='Gini-importance').plot(kind='bar', rot=45)\n",
    "importances.sort_values('Gini-importance',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rfc2.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features_names = df1_filt_b_M15_H1.columns\n",
    "feature_names = [features_names[i] for i in indices]\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(features_names[f], importances[indices[f]])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H1] SELL - Trains df_1 & df_2 ---> Tests df_3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_1  = df1_filt_s_M15_H1\n",
    "df_2  = df2_filt_s_M15_H1\n",
    "df_3  = df3_filt_s_M15_H1\n",
    "Label = 'Labels_M15_H1'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Returns the optimum random serach model\n",
    "#rfc_random_s_123 = model_rfc_random(X_train,y_train)\n",
    "rfc_random       = rfc_random_s_123\n",
    "\n",
    "# Displays the ROC curve (Receiver Operating Characteristic) of df_3 based on the model trained on df_1 & df_2\n",
    "roc(rfc_random, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc_random, X_test = X2, y_test = y2, thres = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1  = df1_filt_s_M15_H1\n",
    "df_2  = df2_filt_s_M15_H1\n",
    "df_3  = df3_filt_s_M15_H1\n",
    "Label = 'Labels_M15_H1'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "    \n",
    "# Fit the random search model\n",
    "rfc.fit(X_train,y_train)\n",
    "\n",
    "roc(rfc, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc, X_test = X2, y_test = y2, thres = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H1] SELL - Trains df_2 & df_3 ---> Tests df_1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_1  = df2_filt_s_M15_H1\n",
    "df_2  = df3_filt_s_M15_H1\n",
    "df_3  = df1_filt_s_M15_H1\n",
    "Label = 'Labels_M15_H1'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Returns the optimum random serach model\n",
    "#rfc_random_s_231 = model_rfc_random(X_train,y_train)\n",
    "rfc_random       = rfc_random_s_231\n",
    "# Displays the ROC curve (Receiver Operating Characteristic) of df_3 based on the model trained on df_1 & df_2\n",
    "roc(rfc_random, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc_random, X_test = X2, y_test = y2, thres = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1  = df2_filt_s_M15_H1\n",
    "df_2  = df3_filt_s_M15_H1\n",
    "df_3  = df1_filt_s_M15_H1\n",
    "Label = 'Labels_M15_H1'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "    \n",
    "# Fit the random search model\n",
    "rfc.fit(X_train,y_train)\n",
    "\n",
    "roc(rfc, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc, X_test = X2, y_test = y2, thres = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H1] SELL - Trains df_1 & df_3 ---> Tests df_2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_1  = df1_filt_s_M15_H1\n",
    "df_2  = df3_filt_s_M15_H1\n",
    "df_3  = df2_filt_s_M15_H1\n",
    "Label = 'Labels_M15_H1'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Returns the optimum random serach model\n",
    "#rfc_random_s_132 = model_rfc_random(X_train,y_train)\n",
    "rfc_random       = rfc_random_s_132\n",
    "# Displays the ROC curve (Receiver Operating Characteristic) of df_3 based on the model trained on df_1 & df_2\n",
    "roc(rfc_random, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc_random, X_test = X2, y_test = y2, thres = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1  = df1_filt_s_M15_H1\n",
    "df_2  = df3_filt_s_M15_H1\n",
    "df_3  = df2_filt_s_M15_H1\n",
    "Label = 'Labels_M15_H1'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "    \n",
    "# Fit the random search model\n",
    "rfc.fit(X_train,y_train)\n",
    "\n",
    "roc(rfc, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc, X_test = X2, y_test = y2, thres = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H4] BUY - Trains df_1 & df_2 ---> Tests df_3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_1  = df1_filt_b_M15_H4\n",
    "df_2  = df2_filt_b_M15_H4\n",
    "df_3  = df3_filt_b_M15_H4\n",
    "Label = 'Labelb_M15_H4'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Returns the optimum random serach model\n",
    "#rfc_random_b_123_H4 = model_rfc_random(X_train,y_train)\n",
    "rfc_random       = rfc_random_b_123_H4\n",
    "# Displays the ROC curve (Receiver Operating Characteristic) of df_3 based on the model trained on df_1 & df_2\n",
    "roc(rfc_random, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc_random, X_test = X2, y_test = y2, thres = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1  = df1_filt_b_M15_H4\n",
    "df_2  = df2_filt_b_M15_H4\n",
    "df_3  = df3_filt_b_M15_H4\n",
    "Label = 'Labelb_M15_H4'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "    \n",
    "# Fit the random search model\n",
    "rfc.fit(X_train,y_train)\n",
    "\n",
    "roc(rfc, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc, X_test = X2, y_test = y2, thres = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H4] BUY - Trains df_2 & df_3 ---> Tests df_1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_1  = df2_filt_b_M15_H4\n",
    "df_2  = df3_filt_b_M15_H4\n",
    "df_3  = df1_filt_b_M15_H4\n",
    "Label = 'Labelb_M15_H4'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Returns the optimum random serach model\n",
    "#rfc_random_b_231_H4 = model_rfc_random(X_train,y_train)\n",
    "rfc_random       = rfc_random_b_231_H4\n",
    "# Displays the ROC curve (Receiver Operating Characteristic) of df_3 based on the model trained on df_1 & df_2\n",
    "roc(rfc_random, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc_random, X_test = X2, y_test = y2, thres = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1  = df2_filt_b_M15_H4\n",
    "df_2  = df3_filt_b_M15_H4\n",
    "df_3  = df1_filt_b_M15_H4\n",
    "Label = 'Labelb_M15_H4'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "    \n",
    "# Fit the random search model\n",
    "rfc.fit(X_train,y_train)\n",
    "\n",
    "roc(rfc, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc, X_test = X2, y_test = y2, thres = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H4] BUY - Trains df_1 & df_3 ---> Tests df_2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_1  = df1_filt_b_M15_H4\n",
    "df_2  = df3_filt_b_M15_H4\n",
    "df_3  = df2_filt_b_M15_H4\n",
    "Label = 'Labelb_M15_H4'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Returns the optimum random serach model\n",
    "#rfc_random_b_132_H4 = model_rfc_random(X_train,y_train)\n",
    "rfc_random       = rfc_random_b_132_H4\n",
    "# Displays the ROC curve (Receiver Operating Characteristic) of df_3 based on the model trained on df_1 & df_2\n",
    "roc(rfc_random, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc_random, X_test = X2, y_test = y2, thres = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1  = df1_filt_b_M15_H4\n",
    "df_2  = df3_filt_b_M15_H4\n",
    "df_3  = df2_filt_b_M15_H4\n",
    "Label = 'Labelb_M15_H4'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "    \n",
    "# Fit the random search model\n",
    "rfc.fit(X_train,y_train)\n",
    "\n",
    "roc(rfc, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc, X_test = X2, y_test = y2, thres = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H4] SELL - Trains df_1 & df_2 ---> Tests df_3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_1  = df1_filt_s_M15_H4\n",
    "df_2  = df2_filt_s_M15_H4\n",
    "df_3  = df3_filt_s_M15_H4\n",
    "Label = 'Labels_M15_H4'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Returns the optimum random serach model\n",
    "#rfc_random_s_123_H4 = model_rfc_random(X_train,y_train)\n",
    "rfc_random       = rfc_random_s_123_H4\n",
    "# Displays the ROC curve (Receiver Operating Characteristic) of df_3 based on the model trained on df_1 & df_2\n",
    "roc(rfc_random, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc_random, X_test = X2, y_test = y2, thres = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1  = df1_filt_s_M15_H4\n",
    "df_2  = df2_filt_s_M15_H4\n",
    "df_3  = df3_filt_s_M15_H4\n",
    "Label = 'Labels_M15_H4'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "    \n",
    "# Fit the random search model\n",
    "rfc.fit(X_train,y_train)\n",
    "\n",
    "roc(rfc, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc, X_test = X2, y_test = y2, thres = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H4] SELL - Trains df_2 & df_3 ---> Tests df_1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_1  = df2_filt_s_M15_H4\n",
    "df_2  = df3_filt_s_M15_H4\n",
    "df_3  = df1_filt_s_M15_H4\n",
    "Label = 'Labels_M15_H4'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Returns the optimum random serach model\n",
    "#rfc_random_s_231_H4 = model_rfc_random(X_train,y_train)\n",
    "rfc_random       = rfc_random_s_231_H4\n",
    "# Displays the ROC curve (Receiver Operating Characteristic) of df_3 based on the model trained on df_1 & df_2\n",
    "roc(rfc_random, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc_random, X_test = X2, y_test = y2, thres = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1  = df2_filt_s_M15_H4\n",
    "df_2  = df3_filt_s_M15_H4\n",
    "df_3  = df1_filt_s_M15_H4\n",
    "Label = 'Labels_M15_H4'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "    \n",
    "# Fit the random search model\n",
    "rfc.fit(X_train,y_train)\n",
    "\n",
    "roc(rfc, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc, X_test = X2, y_test = y2, thres = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [M15_H4] SELL - Trains df_1 & df_3 ---> Tests df_2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_1  = df1_filt_s_M15_H4\n",
    "df_2  = df3_filt_s_M15_H4\n",
    "df_3  = df2_filt_s_M15_H4\n",
    "Label = 'Labels_M15_H4'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Returns the optimum random serach model\n",
    "#rfc_random_s_132_H4 = model_rfc_random(X_train,y_train)\n",
    "rfc_random       = rfc_random_s_132_H4\n",
    "# Displays the ROC curve (Receiver Operating Characteristic) of df_3 based on the model trained on df_1 & df_2\n",
    "roc(rfc_random, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc_random, X_test = X2, y_test = y2, thres = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1  = df1_filt_s_M15_H4\n",
    "df_2  = df3_filt_s_M15_H4\n",
    "df_3  = df2_filt_s_M15_H4\n",
    "Label = 'Labels_M15_H4'\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "    \n",
    "# Fit the random search model\n",
    "rfc.fit(X_train,y_train)\n",
    "\n",
    "roc(rfc, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "predictions(rfc, X_test = X2, y_test = y2, thres = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,5000):\n",
    "\n",
    "    random_cols = list(random_cols_drop(df1_filt_b_M15_H1, random.randrange(1,11,2)))\n",
    "    \n",
    "    if Label in random_cols:\n",
    "        random_cols.remove(Label)\n",
    "        random_cols = random_cols + ['MS_retracement_H1', 'MS_retracement_D1', 'MS_Pds_D1', 'MS_Pds_H1', 'N_Breaks_H1', 'EMA_MACD_12_26_9_H1', 'Dist_BollB_SMA_20_Var_+2_M15', 'BollB_Wideness_H1']\n",
    "    \n",
    "    lr_auc1, random_cols1 = roc_ass(random_cols, df1_filt_b_M15_H1, df2_filt_b_M15_H1, df3_filt_b_M15_H1, Label = 'Labelb_M15_H1')\n",
    "    lr_auc2, random_cols2 = roc_ass(random_cols, df1_filt_b_M15_H1, df3_filt_b_M15_H1, df2_filt_b_M15_H1, Label = 'Labelb_M15_H1')\n",
    "    lr_auc3, random_cols3 = roc_ass(random_cols, df2_filt_b_M15_H1, df3_filt_b_M15_H1, df1_filt_b_M15_H1, Label = 'Labelb_M15_H1')\n",
    "    lr_auc4, random_cols4 = roc_ass(random_cols, df1_filt_s_M15_H1, df2_filt_s_M15_H1, df3_filt_s_M15_H1, Label = 'Labels_M15_H1')\n",
    "    lr_auc5, random_cols5 = roc_ass(random_cols, df1_filt_s_M15_H1, df3_filt_s_M15_H1, df2_filt_s_M15_H1, Label = 'Labels_M15_H1')\n",
    "    lr_auc6, random_cols6 = roc_ass(random_cols, df2_filt_s_M15_H1, df3_filt_s_M15_H1, df1_filt_s_M15_H1, Label = 'Labels_M15_H1')\n",
    "    \n",
    "    par = 0.5\n",
    "    if (i%20 == 0):\n",
    "        print(i)\n",
    "    if (lr_auc1 > par) & (lr_auc2 > par) & (lr_auc3 > par) & (lr_auc4 > par) & (lr_auc5 > par) & (lr_auc6 > par):\n",
    "        print(lr_auc1,lr_auc2,lr_auc3,lr_auc4,lr_auc5,lr_auc6, random_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = ['MS_range_M15', 'RSI_14_SMA_M15', 'Hist_MACD_12_26_9_M15', 'Boll_SMA_20_M15', 'Dist_BollB_SMA_20_Var_+2_M15', 'Dist_BollB_SMA_20_Var_-2_M15', 'BollB_Wideness_M15', 'N_Breaks_H1', 'MS_Pds_H1', 'MS_range_H1', 'EMA_MACD_12_26_9_H1', 'MACD_signal_H1', 'Dist_BollB_SMA_20_Var_+2_H1', 'BollB_Wideness_H1', 'Closing_H4', 'MS_Pds_H4', 'MS_range_H4', 'RSI_14_EMA_H4', 'EMA_MACD_12_26_9_H4', 'MACD_signal_H4', 'Dist_BollB_SMA_20_Var_-2_H4', 'BollB_Wideness_H4', 'N_Breaks_D1', 'MS_Pds_D1', 'Hist_MACD_12_26_9_D1', 'MACD_signal_D1', 'Dist_BollB_SMA_20_Var_-2_D1', 'BollB_Wideness_D1', 'MS_retracement_D1', 'MS_retracement_H1']\n",
    "l2 = ['Closing_M15', 'RSI_14_EMA_M15', 'Hist_MACD_12_26_9_M15', 'MACD_signal_M15', 'Boll_SMA_20_M15', 'Dist_BollB_SMA_20_Var_+2_M15', 'N_Breaks_H1', 'MS_Pds_H1', 'MS_range_H1', 'EMA_MACD_12_26_9_H1', 'MACD_signal_H1', 'Dist_BollB_SMA_20_Var_+2_H1', 'BollB_Wideness_H1', 'Closing_H4', 'N_Breaks_H4', 'RSI_14_SMA_H4', 'EMA_MACD_12_26_9_H4', 'Hist_MACD_12_26_9_H4', 'Dist_BollB_SMA_20_Var_+2_H4', 'Dist_BollB_SMA_20_Var_-2_H4', 'BollB_Wideness_H4', 'Closing_D1', 'MS_Pds_D1', 'RSI_14_EMA_D1', 'EMA_MACD_12_26_9_D1', 'MACD_signal_D1', 'Dist_BollB_SMA_20_Var_+2_D1', 'Dist_BollB_SMA_20_Var_-2_D1', 'MS_retracement_D1', 'MS_retracement_H1', 'MS_retracement_M15']\n",
    "l3 = ['MS_Pds_M15', 'MS_range_M15', 'RSI_14_EMA_M15', 'EMA_MACD_12_26_9_M15', 'Hist_MACD_12_26_9_M15', 'MACD_signal_M15', 'Dist_BollB_SMA_20_Var_+2_M15', 'Closing_H1', 'N_Breaks_H1', 'MS_Pds_H1', 'RSI_14_SMA_H1', 'EMA_MACD_12_26_9_H1', 'MACD_signal_H1', 'Dist_BollB_SMA_20_Var_-2_H1', 'BollB_Wideness_H1', 'MS_range_H4', 'RSI_14_SMA_H4', 'RSI_14_EMA_H4', 'Hist_MACD_12_26_9_H4', 'MACD_signal_H4', 'Dist_BollB_SMA_20_Var_+2_H4', 'Closing_D1', 'MS_Pds_D1', 'RSI_14_SMA_D1', 'RSI_14_EMA_D1', 'EMA_MACD_12_26_9_D1', 'Hist_MACD_12_26_9_D1', 'Dist_BollB_SMA_20_Var_+2_D1', 'BollB_Wideness_D1', 'MS_retracement_D1', 'MS_retracement_H1']\n",
    "l4 = ['MS_Pds_M15', 'MS_range_M15', 'RSI_14_SMA_M15', 'RSI_14_EMA_M15', 'Boll_SMA_20_M15', 'Dist_BollB_SMA_20_Var_+2_M15', 'Closing_H1', 'N_Breaks_H1', 'MS_Pds_H1', 'MS_range_H1', 'RSI_14_SMA_H1', 'EMA_MACD_12_26_9_H1', 'Hist_MACD_12_26_9_H1', 'BollB_Wideness_H1', 'Closing_H4', 'N_Breaks_H4', 'MS_Pds_H4', 'RSI_14_SMA_H4', 'RSI_14_EMA_H4', 'MACD_signal_H4', 'BollB_Wideness_H4', 'MS_Pds_D1', 'RSI_14_EMA_D1', 'Dist_BollB_SMA_20_Var_+2_D1', 'Dist_BollB_SMA_20_Var_-2_D1', 'BollB_Wideness_D1', 'MS_retracement_D1', 'MS_retracement_H4', 'MS_retracement_H1', 'MS_retracement_M15']\n",
    "print(list(set(l1).intersection(l2)))\n",
    "print(list(set(l1).intersection(l3)))\n",
    "print(list(set(l1).intersection(l4)))\n",
    "print(list(set(l2).intersection(l3)))\n",
    "print(list(set(l2).intersection(l4)))\n",
    "print(list(set(l3).intersection(l4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "print('Predictions df2 - Threshold >= 0.3')\n",
    "predicted_proba = rfc_random.predict_proba(X_test)\n",
    "predictions = (predicted_proba [:,1] >= 0.5).astype('int')\n",
    "print(classification_report(y_test,predictions))\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "print('Predictions df3 - Threshold >= 0.3')\n",
    "predicted_proba = rfc_random.predict_proba(X2)\n",
    "predictions = (predicted_proba [:,1] >= 0.3).astype('int')\n",
    "print(classification_report(y2,predictions))\n",
    "print(confusion_matrix(y2,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "disp = plot_precision_recall_curve(rfc_random, X2, y2)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                   'AP={0:0.2f}'.format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "disp = plot_precision_recall_curve(rfc_random, X_test, y_test)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                   'AP={0:0.2f}'.format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "print('Predictions df2')\n",
    "predictions = rfc_random.predict(X_test)\n",
    "print(classification_report(y_test,predictions))\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "\n",
    "print('Predictions df2 - Threshold >=0.3')\n",
    "predicted_proba = rfc_random.predict_proba(X_test)\n",
    "predictions = (predicted_proba [:,1] >= 0.3).astype('int')\n",
    "print(classification_report(y_test,predictions))\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "\n",
    "print('Predictions df3')\n",
    "predictions = rfc_random.predict(X2)\n",
    "print(classification_report(y2,predictions2))\n",
    "print(confusion_matrix(y2,predictions2))\n",
    "\n",
    "print('Predictions df3 - Threshold >=0.3')\n",
    "predicted_proba = rfc_random.predict_proba(X2)\n",
    "predictions = (predicted_proba [:,1] >= 0.3).astype('int')\n",
    "print(classification_report(y2,predictions))\n",
    "print(confusion_matrix(y2,predictions))\n",
    "\n",
    "\n",
    "#predicted_proba = rfc_random.predict_proba(X2)\n",
    "#predicted = (predicted_proba [:,1] >= threshold).astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[int(x) for x in np.linspace(start = 0.1, stop = 0.5, num = 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, average_precision_score\n",
    "\n",
    "for i in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "\n",
    "    threshold = i\n",
    "\n",
    "    predicted_proba = rfc_random.predict_proba(X2)\n",
    "    predicted = (predicted_proba [:,1] >= threshold).astype('int')\n",
    "    \n",
    "    print(i)\n",
    "    print(classification_report(y2,predicted))\n",
    "    print(confusion_matrix(y2,predicted))\n",
    "\n",
    "    #accuracy  = accuracy_score(y_test, predicted)\n",
    "    #precision = precision_score(y_test, predicted)\n",
    "\n",
    "    #print(precision,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "X, y = make_classification(random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "class CustomThreshold(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\" Custom threshold wrapper for binary classification\"\"\"\n",
    "    def __init__(self, base, threshold=0.5):\n",
    "        self.base = base\n",
    "        self.threshold = threshold\n",
    "    def fit(self, *args, **kwargs):\n",
    "        self.base.fit(*args, **kwargs)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return (self.base.predict_proba(X)[:, 1] > self.threshold).astype(int)\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=1).fit(X_train, y_train)\n",
    "clf = [CustomThreshold(rf, threshold) for threshold in [0.3, 0.5, 0.7]]\n",
    "\n",
    "for model in clf:\n",
    "    print(confusion_matrix(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model\n",
    "#### Instantiating and Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "predictions = dtree.predict(X_test)\n",
    "print(classification_report(y_test,predictions))\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "\n",
    "predictions2 = dtree.predict(X2)\n",
    "print(classification_report(y2,predictions2))\n",
    "print(confusion_matrix(y2,predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dtree.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,predictions))\n",
    "\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ver = dtree.predict(df3_final.drop('Label_B',axis=1))\n",
    "\n",
    "print(classification_report(df3_final['Label_B'],predictions_ver))\n",
    "\n",
    "print(confusion_matrix(df3_final['Label_B'],predictions_ver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(predictions,columns=['Predictions'])\n",
    "pred_df = pd.DataFrame(y_test,columns=['Label_B'])\n",
    "pred_df = pred_df.reset_index(drop=True)\n",
    "pred_df = pd.concat([pred_df,predictions],axis=1)\n",
    "#pred_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_df.to_excel('T1213.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiating and Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "X = df1_final.drop('Labelb_H1',axis=1)\n",
    "y = df1_final['Labelb_H1']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,shuffle=True,random_state = 101)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=200)\n",
    "rfc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=200)\n",
    "rfc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "predictions = rfc.predict(X_test)\n",
    "print(classification_report(y_test,predictions))\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "\n",
    "predictions2 = rfc.predict(X2)\n",
    "print(classification_report(y2,predictions2))\n",
    "print(confusion_matrix(y2,predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ver = rfc.predict(df3_final.drop('Label_b_H1',axis=1))\n",
    "\n",
    "print(classification_report(df3_final['Label_B'],predictions_ver))\n",
    "\n",
    "print(confusion_matrix(df3_final['Label_B'],predictions_ver))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "model = Sequential()\n",
    "\n",
    "# input layer\n",
    "model.add(Dense(78,  activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# hidden layer\n",
    "model.add(Dense(39, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# hidden layer\n",
    "model.add(Dense(19, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 25)\n",
    "    # monitor = 'Val_loss' & mode = 'min' ==> we want to minimise loss\n",
    "    # patience - # of iter. extra after early stop has been 'triggered' to ensure it wasn't only noise\n",
    "\n",
    "Label = 'Labelb_M15_H1'\n",
    "\n",
    "df_1  = df1_filt_b_M15_H1\n",
    "df_2  = df2_filt_b_M15_H1\n",
    "df_3  = df3_filt_b_M15_H1\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "# Fit the NN model\n",
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=150,\n",
    "          batch_size=256,\n",
    "          validation_data=(X_test, y_test), \n",
    "          )\n",
    "\n",
    "\n",
    "#roc(model, X2, y2)\n",
    "\n",
    "# Predicts the 3rd df using the random search model created from df1 & df2\n",
    "#predictions(rfc, X_test = X2, y_test = y2, thres = 0.4)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#model.save('my_Lendingclub_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(model.history.history) \n",
    "loss_df.plot()\n",
    "plt.xlim(0, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(X2)\n",
    "\n",
    "print(classification_report(y2,predictions))\n",
    "\n",
    "print(confusion_matrix(y2,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ver = model.predict_classes(df3_final.drop('Label_B',axis=1))\n",
    "\n",
    "print(classification_report(df3_final['Label_B'],predictions_ver))\n",
    "\n",
    "print(confusion_matrix(df3_final['Label_B'],predictions_ver))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "Label = 'Labelb_M15_H1'\n",
    "\n",
    "df_1  = df1_filt_b_M15_H1\n",
    "df_2  = df2_filt_b_M15_H1\n",
    "df_3  = df3_filt_b_M15_H1\n",
    "\n",
    "# Concatenates df_1 & df_2 as X and y, and split the data in train - test. We validate afterwards with df_3\n",
    "X_train, X_test, y_train, y_test, X2, y2 = split(df_1,df_2,df_3,Label)\n",
    "\n",
    "#encoder = LabelEncoder()\n",
    "#encoder.fit(y)\n",
    "#y = encoder.transform(y)\n",
    "#y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "Scaler = MinMaxScaler()\n",
    "\n",
    "X_train = Scaler.fit_transform(X_train)\n",
    "X_test = Scaler.transform(X_test)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(len(df_1.columns)-1,input_dim = len(df_1.columns)-1,activation='relu'))\n",
    "model.add(Dense(2*(len(df_1.columns)-1),activation='relu'))\n",
    "model.add(Dense(2*(len(df_1.columns)-1),activation='relu'))\n",
    "model.add(Dense(len(df_1.columns)-1,activation='relu'))\n",
    "\n",
    "# BINARY CLASSIFICATION\n",
    "#model.add(Dense(3, activation = 'softmax')) # activation='sigmoid'\n",
    "\n",
    "#model.compile(loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'] )\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=25, verbose=1, mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x = X_train, y = y_train, validation_data = (X_test,y_test),epochs=350, callbacks=[early_stop])\n",
    "\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_proba = model.predict_proba(X2)\n",
    "predictions = (predicted_proba [:,1] >= 0.3).astype('int')\n",
    "\n",
    "print(classification_report(y2,predictions))\n",
    "\n",
    "print(confusion_matrix(y2,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ver = model.predict_classes(df3_filt_b_H1_f.drop('Labelb_H1',axis=1))\n",
    "\n",
    "print(classification_report(df3_filt_b_H1_f['Labelb_H1'],predictions_ver))\n",
    "\n",
    "print(confusion_matrix(df3_filt_b_H1_f['Labelb_H1'],predictions_ver))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_merge_D1_H4_H1(df_file_Path_H1,df_file_Path_H4,df_file_Path_D1, pickle_name):\n",
    "    \n",
    "    if ((df_file_Path_H1 != None) & (df_file_Path_H4 != None) & (df_file_Path_D1 != None)):\n",
    "        \n",
    "        df_H1, df_H4, df_D1 = Read_dfs(df_file_Path_H1,df_file_Path_H4,df_file_Path_D1)\n",
    "        \n",
    "        df_4H_1D = Merge_shift_df_4H_1D(Rename_df(df_H4,'H4'),Rename_df(df_D1,'D1'))\n",
    "        df = Merge_shift_df_1H_4H(Rename_df(df_H1,'H1'),df_4H_1D)\n",
    "        \n",
    "        df = Price_H1(df,3)\n",
    "        df = Price_H1_H4(df,3)\n",
    "        \n",
    "        df.to_pickle(pickle_name)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        df = pd.read_pickle(pickle_name)\n",
    "        \n",
    "        return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = read_and_merge_D1_H4_H1(df_file_Path_H1 = None,df_file_Path_H4 = None, df_file_Path_D1 = None, pickle_name = 'df1.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "841.111px",
    "left": "91px",
    "top": "110.051px",
    "width": "303.542px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
